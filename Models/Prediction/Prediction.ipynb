{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e826cdb2-9525-4d74-b93f-aaf5def3b151",
   "metadata": {},
   "source": [
    "### Notebook para realizar a predição em documentos novos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea3319b2-5a72-40ea-a921-e2d49314ad60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Senha:  ··········\n"
     ]
    }
   ],
   "source": [
    "# Configurando Proxy\n",
    "\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "chave  = os.getenv('USER')\n",
    "senha  = getpass('Senha: ')\n",
    "\n",
    "os.environ['HTTP_PROXY']  = f'http://{chave}:{senha}@inet-sys.petrobras.com.br:804'\n",
    "os.environ['HTTPS_PROXY'] = f'http://{chave}:{senha}@inet-sys.petrobras.com.br:804'\n",
    "os.environ['NO_PROXY']    = '127.0.0.1, localhost, petrobras.com.br, petrobras.biz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27abee1b-3959-474a-8e2b-34b843ab63b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from conllu import parse_incr, parse\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, TFBertModel\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdb1861-5125-4cb7-b402-609e0d5ac3ba",
   "metadata": {},
   "source": [
    "### (Fazer a predição para um documento e depois ajustar o script para que ele itere por todos os documentos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0845d52-362e-4efb-b542-9a6ab62346b7",
   "metadata": {},
   "source": [
    "Carregando documentos (ajustar para iterar por todos os documentos da pasta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a9133c4-1cd7-4490-a73a-2ec936378701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicao_teste1_stanza.conllu\n",
      "Predicao_teste2_stanza.conllu\n"
     ]
    }
   ],
   "source": [
    "path = \"../../Corpora/Predicao/Documentos_conllu/\"\n",
    "for file in os.listdir(path):\n",
    "    filename = os.fsdecode(file)\n",
    "    print (filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f01683cf-bada-475a-bd4b-63bf25ac3867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corpora completo\n",
    "\n",
    "\n",
    "path = \"../../Corpora/Predicao/Documentos_conllu/\"\n",
    "file = \"Predicao_teste1_stanza.conllu\"\n",
    "pred = open(path + file, \"r\", encoding=\"utf-8\")\n",
    "pred_sentences = parse_incr(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37303186-7dd6-4e59-a50b-c768d09213c3",
   "metadata": {},
   "source": [
    "### Ajustando o dataset para predição do NER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8f3ffa-b348-49c3-b75f-f77c843ee42e",
   "metadata": {},
   "source": [
    "Usar as mesmos nome de classes e encoder usados no treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2e55762-4612-4b43-8a68-4c833f45192d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B=BACIA' 'B=CAMPO' 'B=ELEMENTO_PETRO' 'B=ESTRUTURA_FÍSICA'\n",
      " 'B=EVENTO_PETRO' 'B=FLUIDO' 'B=FLUIDODATERRA_i' 'B=FLUIDODATERRA_o'\n",
      " 'B=NÃOCONSOLID' 'B=POÇO' 'B=ROCHA' 'B=TEXTURA' 'B=UNIDADE_CRONO'\n",
      " 'B=UNIDADE_LITO' 'I=BACIA' 'I=CAMPO' 'I=ELEMENTO_PETRO'\n",
      " 'I=ESTRUTURA_FÍSICA' 'I=EVENTO_PETRO' 'I=FLUIDO' 'I=FLUIDODATERRA_i'\n",
      " 'I=FLUIDODATERRA_o' 'I=NÃOCONSOLID' 'I=POÇO' 'I=ROCHA' 'I=TEXTURA'\n",
      " 'I=UNIDADE_CRONO' 'I=UNIDADE_LITO' 'O' 'Z=Ignorar']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Label Encoder\n",
    "NER_classes =  ['O', \n",
    "                'B=BACIA','I=BACIA',\n",
    "                'B=UNIDADE_CRONO', 'I=UNIDADE_CRONO', \n",
    "                'B=UNIDADE_LITO', 'I=UNIDADE_LITO',\n",
    "                'B=ROCHA', 'I=ROCHA',\n",
    "                'B=CAMPO', 'I=CAMPO',\n",
    "                'B=FLUIDODATERRA_i', 'I=FLUIDODATERRA_i',\n",
    "                'B=POÇO', 'I=POÇO',\n",
    "                'B=FLUIDO', 'I=FLUIDO',\n",
    "                'B=TEXTURA', 'I=TEXTURA', \n",
    "                'B=ESTRUTURA_FÍSICA', 'I=ESTRUTURA_FÍSICA', \n",
    "                'B=NÃOCONSOLID', 'I=NÃOCONSOLID',\n",
    "                'B=EVENTO_PETRO', 'I=EVENTO_PETRO',\n",
    "                'B=FLUIDODATERRA_o', 'I=FLUIDODATERRA_o',\n",
    "                'B=ELEMENTO_PETRO', 'I=ELEMENTO_PETRO',\n",
    "                'Z=Ignorar'] #,\n",
    "                #'B=TIPO_POROSIDADE', 'I=TIPO_POROSIDADE',\n",
    "                #'B=POÇO_R', 'I=POÇO_R',\n",
    "                #'B=POÇO_T', 'I=POÇO_T',\n",
    "                #'B=POÇO_Q', 'I=POÇO_Q']\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "le.fit(NER_classes)\n",
    "print(le.classes_)\n",
    "le.transform(le.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "458ea278-8635-4136-b263-6455fa7d6311",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sentences(PetroNER_sentences):\n",
    "    \n",
    "    dataset_dict = {}\n",
    "    \n",
    "    for tokenlist in PetroNER_sentences:\n",
    "\n",
    "        ID = []\n",
    "        deps = []\n",
    "        deps_encod = []\n",
    "        upos = []\n",
    "        form = []\n",
    "        grafo = []\n",
    "    \n",
    "        for tok in tokenlist:\n",
    "            if tok['upos'] != '_':\n",
    "                # Verificando se tem a anotação 'grafo'\n",
    "                try:\n",
    "                    grafo.append(tok['misc']['grafo'])\n",
    "                except:\n",
    "                    grafo.append(None)\n",
    "                    \n",
    "                # Verificando se a classe de entidade está na lista de interesse\n",
    "                try:\n",
    "                    deps_encod.append(le.transform([tok['deps']])[0])\n",
    "                    deps.append(tok['deps'])\n",
    "                except:\n",
    "                    deps_encod.append(le.transform(['O'])[0])\n",
    "                    deps.append('O')\n",
    "                \n",
    "                #deps_encod.append(le.transform([tok['deps']])[0])\n",
    "                #deps.append(tok['deps'])    \n",
    "                ID.append(tok['id'])\n",
    "                upos.append(tok['upos'])\n",
    "                form.append(tok['form'])\n",
    "        \n",
    "        dataset_dict[tokenlist.metadata['sent_id']] = {'id':ID, \n",
    "                                                       'deps': deps,\n",
    "                                                       'deps_encod':deps_encod, \n",
    "                                                       'upos': upos, \n",
    "                                                       'form': form,\n",
    "                                                       'grafo': grafo}\n",
    "\n",
    "    return pd.DataFrame(dataset_dict).T.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1942c97-8522-45d5-b623-15e5d91ff4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processando os datasets\n",
    "dataset_pred = process_sentences(pred_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94234888-60da-4ffc-8fce-5e0ccc6c5c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dicionário para receber as predições\n",
    "pred_dic = {}\n",
    "\n",
    "for n in range(len(dataset_pred)):\n",
    "    pred_dic[dataset_pred['index'][n]] = {'NER': {'Entities':None,\n",
    "                                                  'Classes': None,\n",
    "                                                  'Sent_original':None,\n",
    "                                                  'Sent_processadas':None},\n",
    "                                          'RE': None}                      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8980cbf-4ee4-48ca-b935-d4b24bd52bd0",
   "metadata": {},
   "source": [
    "Carregar o mesmo tokenizador usado no modelo NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "069e6bd0-cc02-4e21-91bf-9e7d8f97ca24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir o modelo pretreinado a ser usado\n",
    "model_checkpoint = \"neuralmind/bert-large-portuguese-cased\" #'bert-base-multilingual-cased' #'monilouise/ner_news_portuguese'\n",
    "# Tamano máximo da sentença\n",
    "max_length=512\n",
    "\n",
    "# Carregar o tokenizador\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9585da5b-d7e6-4ee2-8291-007fdd4a669f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred = tokenizer(list(dataset_pred['form'].values),\n",
    "                     truncation=True,\n",
    "                     is_split_into_words=True,\n",
    "                     padding=\"max_length\",\n",
    "                     max_length=max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee807c8-904d-4351-b11a-ad9340c7dbfe",
   "metadata": {},
   "source": [
    "Carregando modelo NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b81f1e0e-e14a-48c4-9da5-9c6f1ee02a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "attention_masks (InputLayer)    [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_bert_model (TFBertModel)     TFBaseModelOutputWit 334396416   input_ids[0][0]                  \n",
      "                                                                 attention_masks[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_73 (Dropout)            (None, 512, 1024)    0           tf_bert_model[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 512, 30)      30750       dropout_73[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 334,427,166\n",
      "Trainable params: 334,427,166\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "NER_model = tf.keras.models.load_model(\"../Named Entity Recognition/Model_NER.h5\",\n",
    "                                       compile=False, \n",
    "                                       custom_objects={\"TFBertModel\": TFBertModel})\n",
    "\n",
    "NER_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4511b401-357a-42f0-b82a-a7c65836c7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predizendo NER\n",
    "pred = NER_model.predict([tf.convert_to_tensor(X_pred['input_ids']),\n",
    "                          #tf.convert_to_tensor(X_pred['token_type_ids']),\n",
    "                          tf.convert_to_tensor(X_pred['attention_mask'])]) \n",
    "\n",
    "# Convertendo o vetor de saída em labels\n",
    "pred_labels = np.argmax(pred, axis=2)\n",
    "labels = le.inverse_transform(pred_labels.reshape(-1)).reshape(-1, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "183d7132-2535-4fed-b6e5-b33b11632ed4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "def process_sentences(X_pred_input_ids, labels):\n",
    "    NER = [] # lista com entidades\n",
    "    classe = []\n",
    "    new_NER = []  # lista para receber tok de entidades durante o parser \n",
    "    sentence_orig = [] # lista para receber tokens da sentença\n",
    "    sentence_NER = [] # lista para receber sentenças anotadas com a entidade\n",
    "    NER_on = False # variável para indicar se está fazendo o parser por uma entidade\n",
    "    \n",
    "    #Sentença tokenizada\n",
    "    toks = tokenizer.tokenize(tokenizer.decode(X_pred_input_ids))\n",
    "\n",
    "    #iterando por cada token\n",
    "    for n in range(max_length):\n",
    "        # Pra o loop quando encontrar '[SEP]'\n",
    "        if toks[n] == '[SEP]':\n",
    "            break\n",
    "            \n",
    "        else:\n",
    "            #Ignorar o token '[CLS]' \n",
    "            if toks[n] != '[CLS]':\n",
    "                # Ao longo o parser vai gravando a sentença original e nas sentenças marcadas com o NER\n",
    "                sentence_orig.append(X_pred_input_ids[n])\n",
    "                \n",
    "                for sent in range(len(sentence_NER)):\n",
    "                    sentence_NER[sent].append(X_pred_input_ids[n])\n",
    "                \n",
    "                # Verificando se uma entidade está iniciando\n",
    "                if labels[n][0] == 'B':\n",
    "                    # Se uma entidade já está sendo processada, a entidade anterior deve ser finalizada e iniciar a nova\n",
    "                    if NER_on:\n",
    "                        NER.append(tokenizer.decode(new_NER))\n",
    "                        sentence_NER[-1] = tokenizer('[' + classe[-1] + '] ' +  NER[-1] + ' | ')['input_ids'][1:-1] + sentence_NER[-1] + tokenizer('[ / E ]')['input_ids'][1:-1]\n",
    "                        new_NER = []\n",
    "                        new_NER.append(X_pred_input_ids[n])\n",
    "                        sentence_NER.append(sentence_orig)\n",
    "                        sentence_NER[-1] = sentence_NER[-1][:-1] + tokenizer('[E]')['input_ids'][1:-1] + [sentence_NER[-1][-1]]\n",
    "                    else:\n",
    "                        NER_on = True\n",
    "                        classe.append(labels[n][2:])\n",
    "                        sentence_NER.append(sentence_orig)\n",
    "                        sentence_NER[-1] = sentence_NER[-1][:-1] + tokenizer('[E]')['input_ids'][1:-1] + [sentence_NER[-1][-1]]\n",
    "                        #sententence.append(last_tokking)\n",
    "                        new_NER.append(X_pred_input_ids[n])\n",
    "                \n",
    "                # Verificando se uma entidade está no meio e adicionar na lista 'new_NER'       \n",
    "                if labels[n][0] == 'I':\n",
    "                    if NER_on:\n",
    "                        new_NER.append(X_pred_input_ids[n])\n",
    "                            \n",
    "                # Verificar ser o token não é entidade e caso anteriormente uma entidade estivesse sendo processada, adicionar na lista 'new_NER'  \n",
    "\n",
    "                if labels[n][0] == 'O':\n",
    "                    if NER_on:\n",
    "                        NER.append(tokenizer.decode(new_NER))\n",
    "                        sentence_NER[-1] = tokenizer('[' + classe[-1] + '] ' +  NER[-1] + ' | ')['input_ids'][1:-1] + sentence_NER[-1] + tokenizer('[ / E ]')['input_ids'][1:-1] + [sentence_NER[-1][-1]]\n",
    "                        new_NER = []\n",
    "                        NER_on = False\n",
    "\n",
    "    if NER_on:\n",
    "        NER.append(tokenizer.decode(new_NER))\n",
    "        sentence_NER[-1] = tokenizer('[' + classe[-1] + '] ' +  NER[-1] + ' | ')['input_ids'][1:-1] + sentence_NER[-1] + tokenizer('[ / E ]')['input_ids'][1:-1]\n",
    "        new_NER = []\n",
    "        NER_on = False\n",
    "        \n",
    "    if NER == []:\n",
    "        return ([[],[],[],[]])\n",
    "    \n",
    "    else:\n",
    "\n",
    "        sentence_orig = tokenizer.decode(sentence_orig)\n",
    "        new_sentence_NER = []\n",
    "        for new_sent in sentence_NER:\n",
    "            new_sentence_NER.append(tokenizer.decode(new_sent))\n",
    "        sentence_NER = new_sentence_NER\n",
    "\n",
    "        return ([NER, classe, sentence_orig, sentence_NER])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "83f58b61-a319-4af0-8e52-6b357e9f589a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(len(labels)):\n",
    "    sent_dic = process_sentences(X_pred['input_ids'][n], labels[n])\n",
    "\n",
    "    pred_dic[dataset_pred['index'][n]]['NER']['Entities'] = sent_dic[0]\n",
    "    pred_dic[dataset_pred['index'][n]]['NER']['Classes'] = sent_dic[1]\n",
    "    pred_dic[dataset_pred['index'][n]]['NER']['Sent_original'] = sent_dic[2]\n",
    "    pred_dic[dataset_pred['index'][n]]['NER']['Sent_processadas'] = sent_dic[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c592e45e-6584-4657-a151-7350fb7e6ae9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NER': {'Entities': [],\n",
       "  'Classes': [],\n",
       "  'Sent_original': [],\n",
       "  'Sent_processadas': []},\n",
       " 'RE': None}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_dic['Predicao_teste1-18']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e8227a-9ac4-4c65-ab83-e0ee37f3e7f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
