{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5392406-540c-4768-82d6-8c4e96d12a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import gensim\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c0a49d-85b6-4ec4-9829-2c9995fe578f",
   "metadata": {},
   "source": [
    "### Carregando dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af296cc0-bd22-412a-b5c5-3123f0e24343",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = np.load('../../Corpora/PetroNER-LinkedEntity/sentences.npy')\n",
    "entities = np.load('../../Corpora/PetroNER-LinkedEntity/entities.npy')\n",
    "classes = np.load('../../Corpora/PetroNER-LinkedEntity/classes.npy')\n",
    "URI =  np.load('../../Corpora/PetroNER-LinkedEntity/URI.npy')\n",
    "URIvec =  np.load('../../Corpora/PetroNER-LinkedEntity/URI_vectors.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cee3300-32af-48d5-8cdf-f59607200d7a",
   "metadata": {},
   "source": [
    "### **A separação entre treino e teste deverá ser refeita depois que a PUC entregar o PetroNER dividido entre trieno e teste**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c7f519-509d-4382-a5d3-3435cb8baadf",
   "metadata": {},
   "source": [
    "Iremos separar o conjunto de treino, validação e teste baseado nas URI. Ou seja, as URI observadas na fase de treinamento do modelo não será usada para a validação e teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06ae2ca0-8b6d-40ec-95c8-3d84513e7ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listando as URI referente às classes que aparecerão nos datasets de treino, validação e teste\n",
    "\n",
    "# Lista de URI que não serão separadas entre treino, validação e teste \n",
    "# Incluímos manualmente as URI das principais classes\n",
    "URI_not_split = ['http://www.semanticweb.org/bg40/ontologies/2022/5/untitled-ontology-2#basin',\n",
    "                 'http://www.semanticweb.org/bg40/ontologies/2022/5/untitled-ontology-2#field',\n",
    "                 'http://www.semanticweb.org/bg40/ontologies/2022/5/untitled-ontology-2#well',\n",
    "                 'http://www.semanticweb.org/bg40/ontologies/2022/5/untitled-ontology-2#rock',\n",
    "                 None]\n",
    "\n",
    "# Fração das URI que não serão separadas entre treino, validação e teste \n",
    "fra_URI_not_split = 0.6\n",
    "\n",
    "for U in set(URI):   \n",
    "    if random.random() < fra_URI_not_split:\n",
    "        URI_not_split.append(U)\n",
    "\n",
    "URItre = []\n",
    "URIval = []\n",
    "URItes = []\n",
    "\n",
    "# Separando as URI para treino (70%), validação (15%) e teste (15%)\n",
    "for U in set(URI):   \n",
    "    if U in URI_not_split:\n",
    "        pass\n",
    "    else:\n",
    "        aleat = random.random()\n",
    "        if aleat < 0.70:\n",
    "            URItre.append(U)\n",
    "        else:\n",
    "            if aleat < 0.85:\n",
    "                URIval.append(U)\n",
    "            else:\n",
    "                URItes.append(U)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7aa468e-66ab-4d86-b72c-aaf440f42b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separando os dataset de treino, validação e teste\n",
    "\n",
    "# Listas de treino\n",
    "text_treino = []\n",
    "entities_treino = []\n",
    "classes_treino = []\n",
    "URI_treino = []\n",
    "URIvec_treino = []\n",
    "\n",
    "# Listas de validação\n",
    "text_valid = []\n",
    "entities_valid = []\n",
    "classes_valid = []\n",
    "URI_valid = []\n",
    "URIvec_valid = []\n",
    "\n",
    "# Listas de teste\n",
    "text_teste = []\n",
    "entities_teste = []\n",
    "classes_teste = []\n",
    "URI_teste = []\n",
    "URIvec_teste = []\n",
    "\n",
    "for n in range(len(URI)):\n",
    "    # Verificando se a URI pertence a lista 'URI_not_split' distribuir usando a mesma proporção entre treino, validação e teste.\n",
    "    if URI[n] in URI_not_split:\n",
    "        aleat = random.random()\n",
    "        if aleat < 0.70:\n",
    "            text_treino.append(text[n])\n",
    "            entities_treino.append(entities[n])\n",
    "            classes_treino.append(classes[n])\n",
    "            URI_treino.append(URI[n])\n",
    "            URIvec_treino.append(URIvec[n])\n",
    "            \n",
    "        else:\n",
    "            if aleat < 0.85:\n",
    "                text_valid.append(text[n])\n",
    "                entities_valid.append(entities[n])\n",
    "                classes_valid.append(classes[n])\n",
    "                URI_valid.append(URI[n])\n",
    "                URIvec_valid.append(URIvec[n])\n",
    "                \n",
    "            else:\n",
    "                text_teste.append(text[n])\n",
    "                entities_teste.append(entities[n])\n",
    "                classes_teste.append(classes[n])\n",
    "                URI_teste.append(URI[n])\n",
    "                URIvec_teste.append(URIvec[n])\n",
    "                \n",
    "    # Verificando se URI está na lista de treino\n",
    "    if URI[n] in URItre:\n",
    "        text_treino.append(text[n])\n",
    "        entities_treino.append(entities[n])\n",
    "        classes_treino.append(classes[n])\n",
    "        URI_treino.append(URI[n])\n",
    "        URIvec_treino.append(URIvec[n])\n",
    "        \n",
    "    # Verificando se URI está na lista de validação\n",
    "    if URI[n] in URIval:\n",
    "        text_valid.append(text[n])\n",
    "        entities_valid.append(entities[n])\n",
    "        classes_valid.append(classes[n])\n",
    "        URI_valid.append(URI[n])\n",
    "        URIvec_valid.append(URIvec[n])\n",
    "        \n",
    "    # Verificando se URI está na lista de teste\n",
    "    if URI[n] in URItes:\n",
    "        text_teste.append(text[n])\n",
    "        entities_teste.append(entities[n])\n",
    "        classes_teste.append(classes[n])\n",
    "        URI_teste.append(URI[n])\n",
    "        URIvec_teste.append(URIvec[n])\n",
    "        \n",
    "# Listas de treino\n",
    "text_treino = np.array(text_treino)\n",
    "entities_treino = np.array(entities_treino)\n",
    "classes_treino = np.array(classes_treino)\n",
    "URI_treino = np.array(URI_treino)\n",
    "URIvec_treino = np.array(URIvec_treino)\n",
    "\n",
    "# Listas de validação\n",
    "text_valid = np.array(text_valid)\n",
    "entities_valid = np.array(entities_valid)\n",
    "classes_valid = np.array(classes_valid)\n",
    "URI_valid = np.array(URI_valid)\n",
    "URIvec_valid = np.array(URIvec_valid)\n",
    "\n",
    "# Listas de teste\n",
    "text_teste = np.array(text_teste)\n",
    "entities_teste = np.array(entities_teste)\n",
    "classes_teste = np.array(classes_teste)\n",
    "URI_teste = np.array(URI_teste)\n",
    "URIvec_teste = np.array(URIvec_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a87f85bf-ef71-40be-8e8f-c8cb0744cb4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino\n",
      "Texto:  7254\n",
      "Entidades:  7254\n",
      "Classes:  7254\n",
      "URI:  7254\n",
      "URIvec: : 7254\n",
      "\n",
      " Validação\n",
      "Texto:  1199\n",
      "Entidades:  1199\n",
      "Classes:  1199\n",
      "URI:  1199\n",
      "URIvec: : 1199\n",
      "\n",
      " Teste\n",
      "Texto:  1391\n",
      "Entidades:  1391\n",
      "Classes:  1391\n",
      "URI:  1391\n",
      "URIvec: : 1391\n"
     ]
    }
   ],
   "source": [
    "print ('Treino')\n",
    "print('Texto: ', len(text_treino))\n",
    "print('Entidades: ', len(entities_treino))\n",
    "print('Classes: ', len(classes_treino))\n",
    "print('URI: ', len(URI_treino))\n",
    "print('URIvec: :', len(URIvec_treino))\n",
    "\n",
    "print ('\\n Validação')\n",
    "print('Texto: ', len(text_valid))\n",
    "print('Entidades: ', len(entities_valid))\n",
    "print('Classes: ', len(classes_valid))\n",
    "print('URI: ', len(URI_valid))\n",
    "print('URIvec: :', len(URIvec_valid))\n",
    "\n",
    "print ('\\n Teste')\n",
    "print('Texto: ', len(text_teste))\n",
    "print('Entidades: ', len(entities_teste))\n",
    "print('Classes: ', len(classes_teste))\n",
    "print('URI: ', len(URI_teste))\n",
    "print('URIvec: :', len(URIvec_teste))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9221b7ef-55fd-4a7e-8c9f-98346bceb0d4",
   "metadata": {},
   "source": [
    "### Preparando dataset para treinar rede Siamesa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba642040-def9-43b9-b05f-a87dda6c64d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataframe com as URI, sentenças e classes\n",
    "\n",
    "def dataset_siamesa(classes, URI, URIvec, text, x_easy, x_hard):\n",
    "       \n",
    "    df = pd.DataFrame({'Classe': classes, 'URI': URI, 'Sentença': text}) #, 'URIvec': URIvec})\n",
    "    \n",
    "    # Arrays para armazenar as sentenças âncoras, positivas e negativas \n",
    "    URI_anchor = np.array([])\n",
    "    URIvec_anchor = np.array([])\n",
    "    dataset_anchor = np.array([])\n",
    "    dataset_positive = np.array([])\n",
    "    dataset_negative = np.array([])\n",
    "\n",
    "    #Quantidade de exemplos negativos fáceis e difíceis\n",
    "    #x_easy = 20\n",
    "    #x_hard = 30\n",
    "\n",
    "    # iterando por todas as URI\n",
    "    for i in set(URI):\n",
    "\n",
    "        try:\n",
    "            \n",
    "            # Subgrupo apenas com a URI selecionada\n",
    "            df_URI = df[df['URI'] == i]\n",
    "\n",
    "            # Classe da URI selecionada\n",
    "            classe_da_URI = df_URI['Classe'].values[0]\n",
    "            # Vetor da URI selecionada\n",
    "            vetor_da_URI = URIvec[df_URI.index[0]]\n",
    "            # Subgrupo com sentenças negativa 'fácil' (URI diferente e classe diferente).\n",
    "            df_negative_easy = df[(df['URI'] != i) & (df['Classe'] != classe_da_URI)]\n",
    "            # Subgrupo com sentenças negativa 'difícil' (URI diferente mas da mesma classe).\n",
    "            df_negative_hard = df[(df['URI'] != i) & (df['Classe'] == classe_da_URI)]\n",
    "\n",
    "            # Pular os casos onde só existe uma sentença para a URI selecionada\n",
    "            if (len(df_URI) > 1) & (len(df_negative_easy) > 1) & (len(df_negative_hard) > 1):\n",
    "\n",
    "                ### Construindo triplets com exemplos negativos fáceis\n",
    "\n",
    "                # Selecionando as sentenças âncoras \n",
    "                # (verificamos o número de combinações possíveis entre sentenças com mesma URI \"len(df_URI)*(len(df_URI) - 1)\".\n",
    "                #  Se X_easy for maior que esse valor, amostramos apenas essa quantidade de amostras.\n",
    "                df_ancora = df_URI.sample(min(len(df_URI)*(len(df_URI) - 1) , x_easy), replace=True)\n",
    "\n",
    "                # itera pelas senteças âncoras\n",
    "                for row in df_ancora.iterrows():\n",
    "\n",
    "                    anchor = row[1]['Sentença']\n",
    "                    # Seleciona uma exemplo positivo diferente da sentença âncora\n",
    "                    positive = np.str_(df_URI[df_URI['Sentença'] != anchor].sample(1)['Sentença'].values)\n",
    "\n",
    "                    # Seleciona uma sentença negativa\n",
    "                    negative_easy = np.str_(df_negative_easy.sample(1)['Sentença'].values)\n",
    "\n",
    "                    # Concatena com as arrays que armazenam as senteças e URI\n",
    "                    URI_anchor = np.concatenate((URI_anchor, np.array([np.str_(i)])), axis=0)\n",
    "                    URIvec_anchor = np.concatenate((URIvec_anchor, vetor_da_URI), axis=0)\n",
    "                    dataset_anchor = np.concatenate((dataset_anchor, np.array([anchor])), axis=0)\n",
    "                    #dataset_anchor  = np.concatenate((dataset_anchor, anchor), axis=0)\n",
    "                    dataset_positive = np.concatenate((dataset_positive, np.array([positive])), axis=0)\n",
    "                    dataset_negative = np.concatenate((dataset_negative, np.array([negative_easy])), axis=0)\n",
    "\n",
    "                ### Construindo triplets com exemplos negativos difíceis\n",
    "\n",
    "                # Selecionando as sentenças âncoras \n",
    "                # (verificamos o número de combinações possíveis entre sentenças com mesma URI \"len(df_URI)*(len(df_URI) - 1)\".\n",
    "                #  Se X_hard for maior que esse valor, amostramos apenas essa quantidade de amostras.\n",
    "                \n",
    "                df_ancora = df_URI.sample(min(len(df_URI)*(len(df_URI) - 1), x_hard), replace=True)\n",
    "\n",
    "                # itera pelas senteças âncoras\n",
    "                for row in df_ancora.iterrows():\n",
    "                    anchor = row[1]['Sentença']\n",
    "\n",
    "                    # Seleciona uma exemplo positivo diferente da sentença âncora\n",
    "                    positive = np.str_(df_URI[df_URI['Sentença'] != anchor].sample(1)['Sentença'].values)\n",
    "\n",
    "                    # Seleciona uma sentença negativa\n",
    "                    negative_hard = np.str_(df_negative_hard.sample(1)['Sentença'].values)\n",
    "\n",
    "                    # Concatena com as arrays que armazenam as senteças\n",
    "                    URI_anchor = np.concatenate((URI_anchor, np.array([np.str_(i)])), axis=1)\n",
    "                    URIvec_anchor = np.concatenate((URIvec_anchor, vetor_da_URI), axis=0)\n",
    "                    dataset_anchor  = np.concatenate((dataset_anchor, np.array([anchor])), axis=0)\n",
    "                    #dataset_anchor  = np.concatenate((dataset_anchor, anchor), axis=0)\n",
    "                    dataset_positive  = np.concatenate((dataset_positive, np.array([positive])), axis=0)\n",
    "                    dataset_negative  = np.concatenate((dataset_negative, np.array([negative_hard])), axis=0)\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    #Ajustando o formato da array URIvec_anchor\n",
    "    URIvec_anchor = np.reshape(URIvec_anchor, (-1, len(URIvec[0])))\n",
    "    \n",
    "    return(URI_anchor, URIvec_anchor, dataset_anchor, dataset_positive, dataset_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e57fee6-b02e-4af5-b7c1-ac17f3cff98b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "URI_anchor_treino, URIvec_anchor_treino, dataset_anchor_treino, dataset_positive_treino, dataset_negative_treino = dataset_siamesa(classes_treino,\n",
    "                                                                                                                                   URI_treino,\n",
    "                                                                                                                                   URIvec_treino,\n",
    "                                                                                                                                   text_treino, 50, 50)\n",
    "URI_anchor_valid, URIvec_anchor_valid, dataset_anchor_valid, dataset_positive_valid, dataset_negative_valid = dataset_siamesa(classes_valid,\n",
    "                                                                                                                              URI_valid,\n",
    "                                                                                                                              URIvec_valid,\n",
    "                                                                                                                              text_valid, 50, 50)\n",
    "URI_anchor_teste, URIvec_anchor_teste, dataset_anchor_teste, dataset_positive_teste, dataset_negative_teste = dataset_siamesa(classes_teste,\n",
    "                                                                                                                              URI_teste,\n",
    "                                                                                                                              URIvec_teste,\n",
    "                                                                                                                              text_teste, 50, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d04755-6eb6-46d9-ac39-d1374864e15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Dataset treino: ', len(URI_anchor_treino))\n",
    "print('Dataset validação: ', len(dataset_anchor_valid))\n",
    "print('Dataset teste: ', len(dataset_anchor_teste))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2b1136-76a6-46ea-934c-4ab5b85d46ea",
   "metadata": {},
   "source": [
    "### Salvando dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa474a98-d03a-40c6-be0b-1e6f4694c047",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Treino\n",
    "with open('URI_anchor_treino.npy', 'wb') as f:\n",
    "    np.save(f, URI_anchor_treino)\n",
    "with open('URIvec_anchor_treino.npy', 'wb') as f:\n",
    "    np.save(f, URIvec_anchor_treino)\n",
    "with open('anchor_treino.npy', 'wb') as f:\n",
    "    np.save(f, dataset_anchor_treino)\n",
    "with open('positive_treino.npy', 'wb') as f:\n",
    "    np.save(f, dataset_positive_treino)\n",
    "with open('negative_treino.npy', 'wb') as f:\n",
    "    np.save(f, dataset_negative_treino)\n",
    "\n",
    "    \n",
    "#Validação\n",
    "with open('URI_anchor_valid.npy', 'wb') as f:\n",
    "    np.save(f, URI_anchor_valid)\n",
    "with open('URIvec_anchor_valid.npy', 'wb') as f:\n",
    "    np.save(f, URIvec_anchor_valid)\n",
    "with open('anchor_valid.npy', 'wb') as f:\n",
    "    np.save(f, dataset_anchor_valid)\n",
    "with open('positive_valid.npy', 'wb') as f:\n",
    "    np.save(f, dataset_positive_valid)\n",
    "with open('negative_valid.npy', 'wb') as f:\n",
    "    np.save(f, dataset_negative_valid)\n",
    "    \n",
    "#Teste\n",
    "with open('URI_anchor_teste.npy', 'wb') as f:\n",
    "    np.save(f, URI_anchor_teste)\n",
    "with open('URIvec_anchor_teste.npy', 'wb') as f:\n",
    "    np.save(f, URIvec_anchor_teste)\n",
    "with open('anchor_teste.npy', 'wb') as f:\n",
    "    np.save(f, dataset_anchor_teste)\n",
    "with open('positive_teste.npy', 'wb') as f:\n",
    "    np.save(f, dataset_positive_teste)\n",
    "with open('negative_teste.npy', 'wb') as f:\n",
    "    np.save(f, dataset_negative_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e8337c-dbe9-4d6b-a2c6-3b4094dc4d20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
