{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "* Owlready2 * Warning: optimized Cython parser module 'owlready2_optimized' is not available, defaulting to slower Python implementation\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import operator as op\n",
    "import warnings \n",
    "from owlready2 import * #\n",
    "import random\n",
    "import unicodedata\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregar ontologia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "get_ontology(\"http://www.semanticweb.org/bg40/ontologies/2022/5/untitled-ontology-2#\")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onto_name = \"OntoGeoLogicaInstanciasRelacoes\"\n",
    "onto = get_ontology(\"../../KnowledgeGraph/OntoGeoLogicaInstanciasRelacoes.owl\")\n",
    "onto.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções para procurar na ontologia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relations_between_uris(uri_1, uri_2): \n",
    "    #funcao que acessa a ontologia e procura relacao entre URIs\n",
    "    dict_relation_uris = {}\n",
    "    #Pega as relacoes que a URI1 tem\n",
    "    relation_query_results = list(default_world.sparql(\"\"\"\n",
    "            SELECT DISTINCT ?rel\n",
    "            WHERE{?uri ?rel ?obj\n",
    "                 FILTER(contains(str(?rel), \"http://www.semanticweb.org/bg40/ontologies/2022/5/untitled-ontology-2#\"))\n",
    "                 FILTER (contains(str(?uri), \"\"\" + '\"' + uri_1 + '\"' + \"\"\"))\n",
    "                 }\n",
    "            \"\"\"))\n",
    "    \n",
    "    relations_str = []\n",
    "    for relation_uris in relation_query_results:\n",
    "        relations_str.append(str(relation_uris[0]).rsplit(\".\",1)[-1])\n",
    "        \n",
    "    # Para cada tipo de relação procura se existe match entre URI1 e URI2\n",
    "    for relation in relations_str:\n",
    "        relation_between_words = list(default_world.sparql(\"\"\"\n",
    "                PREFIX prefix: <http://www.semanticweb.org/bg40/ontologies/2022/5/untitled-ontology-2#>\n",
    "                SELECT distinct ?y ?x2\n",
    "                WHERE{?y prefix:\"\"\" +  relation  +  \"\"\" ?x1\n",
    "\n",
    "                      FILTER (contains(str(?y), \"\"\" + '\"' + uri_1  + '\"' + \"\"\"))        \n",
    "\n",
    "                      ?x2 rdf:type ?j                                   \n",
    "                      FILTER (contains(str(?x2), \"\"\" + '\"' + uri_2  + '\"' + \"\"\"))\n",
    "\n",
    "                      FILTER ( ?x2 = ?x1 )\n",
    "                    }\n",
    "                \"\"\"))\n",
    "        dict_relation_uris[relation] = relation_between_words\n",
    "    return dict_relation_uris\n",
    "\n",
    "def go_through_relations(uri1,uri2):\n",
    "    relation_uris = get_relations_between_uris(uri1, uri2)            \n",
    "    if relation_uris != {}: #talvez exista relacao entre URIs, dicionario pode vir vazio -> []\n",
    "        for x, y in relation_uris.items():#procurar por relacao\n",
    "            if y != []: #existe alguma relacao\n",
    "#                 print(x)\n",
    "                return x\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funções para printar informações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sentence_text(sentence):\n",
    "    #printa e retorna o texto original da sentenca\n",
    "    size_sentence = int(sentence.iloc[-1][\"end\"])\n",
    "    text = \" \"*size_sentence\n",
    "    for index, row in sentence.iterrows():\n",
    "        text = text[:int(row[\"start\"])] + row[\"form\"] +text[int(row[\"end\"]):]\n",
    "    print(text)\n",
    "    print(\"-------------\")\n",
    "    return text\n",
    "\n",
    "def print_relation_entities(word1,word2,ent1,ent2,URI_1,URI_2,relation_type,text):\n",
    "    #printa as entidades e relacao entre elas\n",
    "    print('Token 1 = ', word1, '--- Class 1 = ', ent1, '--- URI 1 = ', URI_1)\n",
    "    print('Token 2 = ', word2, '--- Class 2 = ', ent2,'--- URI 2 = ',URI_2)\n",
    "    print('Relation Type = ', relation_type)\n",
    "    print(text)\n",
    "    print(\"-------------\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções para gerar Jsons a serem lidos no labelstudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResultRelationJson(object):\n",
    "    def __init__(self, from_id, to_id, relations, direction = \"right\"):\n",
    "        self.dict = {\n",
    "            \"from_id\": str(from_id),\n",
    "            \"to_id\": str(to_id),\n",
    "            \"type\": \"relation\",\n",
    "            \"direction\": direction,\n",
    "            \"labels\": relations\n",
    "        }\n",
    "    def get_dict(self):\n",
    "        return self.dict\n",
    "class ResultNERJson(object):\n",
    "    def __init__(self, row):     \n",
    "        self.result_dict = {\n",
    "            \"value\": {\n",
    "            \"start\": row[\"start_word\"],\n",
    "            \"end\": row[\"end_word\"],\n",
    "            \"text\": row[\"word_join\"],\n",
    "            \"labels\": [\n",
    "              row[\"label_word\"]\n",
    "            ],\n",
    "            \"URI\": row[\"URI\"]\n",
    "            },\n",
    "            \n",
    "            \"id\": row[\"index_e\"],\n",
    "            \"from_name\": \"label\",\n",
    "            \"to_name\": \"text\",\n",
    "            \"type\": \"labels\",\n",
    "            \"origin\": \"prediction\"\n",
    "        }\n",
    "    def get_dict(self):\n",
    "        return self.result_dict  \n",
    "class CreateOutput(object):\n",
    "    def __init__(self, text, filtred_sentence, entity_name_new):\n",
    "        self.filtred_sentence = filtred_sentence\n",
    "        self.entity_name_new = entity_name_new\n",
    "        self.main_dict = {\n",
    "            \"id\": 1,\n",
    "            \"data\": {\n",
    "              \"text\": text #sentenca inteira\n",
    "            },\n",
    "            \"annotations\": []\n",
    "        }\n",
    "        self._add_annotations()      \n",
    "    def _add_annotations(self):\n",
    "        results = []\n",
    "        count = 0        \n",
    "        for index, row in self.entity_name_new.iterrows(): \n",
    "            results.append(ResultNERJson(row).get_dict())        \n",
    "        item = [{\n",
    "              \"id\": 1,\n",
    "              \"created_username\": \" null, 0\",\n",
    "              \"created_ago\": \"\",\n",
    "              \"result\": results\n",
    "            }]\n",
    "        self.main_dict[\"annotations\"] = item\n",
    "    def get_output(self):\n",
    "        return self.main_dict\n",
    "    def add_relationship(self, from_id, to_id, relations, direction):\n",
    "        results = self.main_dict.get(\"annotations\")[0].get(\"result\")\n",
    "        relation = ResultRelationJson(from_id, to_id, [relations], direction).get_dict()\n",
    "        results.append(relation)\n",
    "        self.main_dict[\"annotations\"][0][\"result\"] = results   \n",
    "        \n",
    "def combine_itens_from_lists_add_in_json(from_id_vec, to_id_vec, relation_from_vec, output):\n",
    "    for idxRelation in range(0,len(from_id_vec)):\n",
    "        direction = \"right\"\n",
    "        output.add_relationship(from_id=from_id_vec[idxRelation], to_id=to_id_vec[idxRelation], relations = relation_from_vec[idxRelation], direction=direction)\n",
    "    return output\n",
    "def saveJsonFiles(df,from_id,to_id, lista_relaoces_sentence,sentence,SentenceNum,path):\n",
    "    #cria e salva o arquivo Json para labelstudio\n",
    "    text = sentence.iloc[0]['text']\n",
    "    print('Saved Json ->', True)\n",
    "    output = CreateOutput(text,sentence, df)\n",
    "    combine_itens_from_lists_add_in_json(from_id, to_id, lista_relacoes_sentence, output)\n",
    "    print(\"-------------\")\n",
    "    with open(os.path.join(path,f\"{SentenceNum}.json\"), \"w\") as outfile: \n",
    "        json.dump(output.get_output(), outfile) \n",
    "        \n",
    "def get_df_forJsons(sentence,idxTokens):\n",
    "    #retorna um dataframe com as informações das entidades e uma string contendo o nome completo da entidade\n",
    "    df_save_words = pd.DataFrame(columns=['index_e', \"LABEL\", \"START\", \"END\",\\\n",
    "                                      \"TEXT\", \"word_join\", \"start_word\", \"end_word\", \"label_word\",\"URI\"])\n",
    "\n",
    "    index_e = sentence.iloc[idxTokens]['index_e']\n",
    "    label = sentence.iloc[idxTokens]['deps']\n",
    "    start = sentence.iloc[idxTokens]['word_join_start']\n",
    "    end = start + len(sentence.iloc[idxTokens]['form'])\n",
    "    text_ent = sentence.iloc[idxTokens]['form']\n",
    "    word_join = sentence.iloc[idxTokens]['word_join']\n",
    "    start_word = sentence.iloc[idxTokens]['word_join_start']\n",
    "    end_word = sentence.iloc[idxTokens]['word_join_end']\n",
    "    label_word = label.replace(\"B=\",\"\")\n",
    "    URI = sentence.iloc[idxTokens]['grafo']\n",
    "\n",
    "    df_save_words.loc[len(df_save_words.index)] = [index_e, label, start, end, text_ent, word_join,\n",
    "                                                   start_word,\n",
    "                                                   end_word,\n",
    "                                                   label_word,\n",
    "                                                   URI]\n",
    "\n",
    "    return df_save_words, word_join\n",
    "        \n",
    "def create_df_JsonFiles(df_entity,x,token,token2,URI_1,URI_2,idxTokens,idxTokens2,from_id,to_id,sentence):\n",
    "    #retorna o dataframe utilizado para criacao dos arquivos Json para labelstudio\n",
    "    entity_name_new_token1,wordjoin_1 = get_df_forJsons(sentence,idxTokens)\n",
    "    entity_name_new_token2,wordjoin_2 = get_df_forJsons(sentence,idxTokens2)\n",
    "    if idxTokens not in from_id and idxTokens not in to_id:\n",
    "        df_entity = pd.concat([df_entity, entity_name_new_token1])\n",
    "    if idxTokens2 not in from_id and idxTokens2 not in to_id:\n",
    "        df_entity = pd.concat([df_entity, entity_name_new_token2])\n",
    "\n",
    "#     print_relation_entities(wordjoin_1,wordjoin_2,token.replace('B=',''),token2.replace('B=',''),URI_1,URI_2,x)\n",
    "    return df_entity  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funções para criar dataframe para modelo BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_moddedText_BERT(text,start1,end1,start2,end2,Ent1_inic,Ent1_end,Ent2_inic,Ent2_end):\n",
    "    new_end_ent1 = end1 + len(Ent1_inic)\n",
    "    new_start_ent2 = start2 + len(Ent1_inic) + len(Ent1_end)\n",
    "    new_end_ent2 = end2 + len(Ent1_inic) + len(Ent1_end) + len(Ent2_inic)      \n",
    "    #adicionando [E1] e [/E1]\n",
    "    text_new = text[:start1] + Ent1_inic + text[start1:]\n",
    "    text_new = text_new[:new_end_ent1] + Ent1_end + text_new[new_end_ent1:]\n",
    "    #adicionando [E2] e [/E2]\n",
    "    text_new2 = text_new[:new_start_ent2] + Ent2_inic + text_new[new_start_ent2:]\n",
    "    text_new2 = text_new2[:new_end_ent2] + Ent2_end + text_new2[new_end_ent2:]\n",
    "    \n",
    "    return text_new2\n",
    "\n",
    "def createText_sentence_BERT(text,start_1,end_1,start_2,end_2):\n",
    "    #funcao que retorna um novo texto para sentenca com [E1] e [E2] adicionados junto de cada entidade\n",
    "    start_ent1, start_ent2 = start_1, start_2\n",
    "    end_ent1, end_ent2 = end_1, end_2\n",
    "    Ent1_inic, Ent1_end = '[E1] ', ' [/E1]'\n",
    "    Ent2_inic, Ent2_end = '[E2] ', ' [/E2]'\n",
    "    \n",
    "    if start_ent1 < start_ent2: #[E1] vem antes de [E2]\n",
    "        text_new = create_moddedText_BERT(text,start_ent1,end_ent1,start_ent2,end_ent2,\\\n",
    "                                  Ent1_inic,Ent1_end,Ent2_inic,Ent2_end)\n",
    "    else: #[E2] vem antes de [E1]      \n",
    "        text_new = create_moddedText_BERT(text,start_ent2,end_ent2,start_ent1,end_ent1,\\\n",
    "                                      Ent2_inic,Ent2_end,Ent1_inic,Ent1_end)  \n",
    "    return text_new\n",
    "\n",
    "def create_bert_dataframe(df_bert,idxTokens,idxTokens2,sentence,URI_1,URI_2,has_relation,relation_type,SentenceNumber):\n",
    "    #retorna o dataframe com as informacoes de cada sentenca para utilizar no modelo BERT\n",
    "    df_bert_temp = pd.DataFrame(columns=['#Sentence','sentence','Ent1','Ent2','URI_1','URI_2','has_relation','relation'])\n",
    "    text = sentence.iloc[0]['text']\n",
    "    wordjoin_1, wordjoin_2 = sentence.iloc[idxTokens]['word_join'], sentence.iloc[idxTokens2]['word_join']\n",
    "    ent1, ent2 = sentence.iloc[idxTokens]['deps'], sentence.iloc[idxTokens2]['deps']\n",
    "    ent1, ent2 = ent1.replace(\"B=\",\"\"), ent2.replace(\"B=\",\"\")\n",
    "    start_1, start_2 = sentence.iloc[idxTokens]['word_join_start'], sentence.iloc[idxTokens2]['word_join_start']\n",
    "    end_1, end_2 = sentence.iloc[idxTokens]['word_join_end'], sentence.iloc[idxTokens2]['word_join_end']\n",
    "    text_bert_ents = createText_sentence_BERT(text,start_1,end_1,start_2,end_2)\n",
    "    df_bert_temp.loc[0] = [SentenceNumber,\n",
    "                           text_bert_ents,\n",
    "                           ent1,\n",
    "                           ent2,\n",
    "                           URI_1,\n",
    "                           URI_2,\n",
    "                           has_relation,\n",
    "                           relation_type]\n",
    "    df_bert = pd.concat([df_bert, df_bert_temp])\n",
    "    if relation_type!='no_relation':\n",
    "        print_relation_entities(wordjoin_1,wordjoin_2,ent1,ent2,URI_1,URI_2,relation_type,text_bert_ents)\n",
    "    return df_bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções para processar as sentenças"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utils\n",
    "def create_relations_dataframe(df_relation,token,token2,URI_1,URI_2,x,originalSentenceNumber):\n",
    "    #retorna dataframe das entidades e suas relações em cada linha\n",
    "    #importante para contabilizar os tipos de relação\n",
    "    df_relation_new = pd.DataFrame(columns=['Relation','Ent1','Ent2','URI_1','URI_2','#Sentence'])\n",
    "    df_relation_new.loc[0] = [x,\n",
    "                            token.replace('B=',''),\n",
    "                            token2.replace('B=',''),\n",
    "                            URI_1,\n",
    "                            URI_2,\n",
    "                            originalSentenceNumber]\n",
    "    df_relation = pd.concat([df_relation, df_relation_new])\n",
    "    return df_relation\n",
    "\n",
    "def verifica_pares_entidade_interesse(ENT_1, ENT_2,relation_type):\n",
    "    #verifica se a relacao encontrada vai ser do tipo temporal_relation CRONO->CRONO\n",
    "    #funcao talvez precise ser atualizada no futuro conforme a ontologia for povoada\n",
    "    lista_from = ['POÇO','UNIDADE_LITO','UNIDADE_LITO','CAMPO','POÇO','POÇO','UNIDADE_LITO','UNIDADE_LITO']\n",
    "    lista_to = ['UNIDADE_LITO','NÂOCONSOLID','ROCHA','BACIA','BACIA','CAMPO','BACIA','UNIDADE_CRONO']        \n",
    "    for idx in range(0,len(lista_to)):\n",
    "        if lista_from[idx] == ENT_1 and lista_to[idx] == ENT_2:\n",
    "            return relation_type\n",
    "    return 'temporal_relation'\n",
    "\n",
    "def go_through_sentence(sentence_df,df_relation,df_bert,sent_numb):\n",
    "    #percorre a sentenca em busca de relacoes entre entidades anotadas com URIs\n",
    "    df_entity = pd.DataFrame()\n",
    "    from_id, to_id = [], []\n",
    "    relation_from, relation_to = [], []\n",
    "    lista_relacoes_sentence = []\n",
    "    is_to_save = False\n",
    "#     df_bert.to_csv(save_csv_name, encoding='utf-8',index=False)\n",
    "#     df_relation.to_csv('df_relation.csv', encoding='utf-8',index=False)\n",
    "    for idxTokens in range(len(sentence_df)):\n",
    "        token, URI_1 = sentence_df.iloc[idxTokens]['deps'], sentence_df.iloc[idxTokens]['grafo']\n",
    "        for idxTokens2 in range(len(sentence_df)):\n",
    "            if idxTokens != idxTokens2:\n",
    "                token2, URI_2 = sentence_df.iloc[idxTokens2]['deps'], sentence_df.iloc[idxTokens2]['grafo']\n",
    "                has_relation = False\n",
    "                relation_type = go_through_relations(URI_1,URI_2)\n",
    "                if relation_type: \n",
    "                    print(\"-------------\")\n",
    "                    print('sentence =', sent_numb)\n",
    "                    is_to_save = True\n",
    "                    has_relation = True\n",
    "                    Ent1, Ent2 = token.replace(\"B=\",\"\"), token2.replace(\"B=\",\"\")\n",
    "                    relation_type = verifica_pares_entidade_interesse(Ent1,Ent2,relation_type)\n",
    "                    lista_relacoes_sentence.append(relation_type)\n",
    "\n",
    "                    #criar df_bert para BERT RE com codigo do Fabio\n",
    "                    df_bert = create_bert_dataframe(df_bert,idxTokens,idxTokens2,sentence_df,\n",
    "                                                    URI_1,URI_2,\n",
    "                                                    has_relation,relation_type,originalSentenceNumber)\n",
    "\n",
    "                    #para contabilizar os pares de entidade por relacao\n",
    "                    df_relation = create_relations_dataframe(df_relation,token,token2,\n",
    "                                                             URI_1,URI_2,relation_type,originalSentenceNumber)\n",
    "                    #listas para contabilizar relacoes, uris e classes\n",
    "                    lista_relacoes.append(relation_type)\n",
    "                    lista_uris.append(URI_1)\n",
    "                    lista_uris.append(URI_2)         \n",
    "                    lista_classes.append(Ent1)\n",
    "                    lista_classes.append(Ent2)\n",
    "\n",
    "                    if is_to_createJsons: #se quiser criar Jsons para LabelStudio\n",
    "                        df_entity = create_df_JsonFiles(df_entity,relation_type,token,token2,URI_1,URI_2,\n",
    "                                                        idxTokens,idxTokens2,from_id,to_id,sentence_df)\n",
    "                        from_id.append(idxTokens)\n",
    "                        to_id.append(idxTokens2) \n",
    "\n",
    "                else: #nao achou relacao\n",
    "                    relation_type = 'no_relation'\n",
    "\n",
    "                    df_bert = create_bert_dataframe(df_bert,idxTokens,idxTokens2,sentence_df,\n",
    "                                                    URI_1,URI_2,has_relation,relation_type,originalSentenceNumber)\n",
    "                        \n",
    "    return lista_relacoes,lista_uris,lista_classes,\\\n",
    "            df_bert, df_relation, df_entity, \\\n",
    "            lista_relacoes_sentence, from_id, to_id, is_to_save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ler arquivo csv (ou pkl) com as sentenças pós filtragem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero total de sentenças pos-filtragem ->  446\n"
     ]
    }
   ],
   "source": [
    "#df_filtred_sentences = pickle.load(open('df_filtred_petroner_uri_2023_04_05.conllu.pkl', 'rb'))\n",
    "#df_filtred_sentences = pd.read_csv('df_filtred_petroner_uri_2023_04_05_conllu.csv')\n",
    "\n",
    "df_filtred_sentences = pickle.load(open('df_filtred_petroner_uri_valid.conllu.pkl', 'rb'))\n",
    "df_filtred_sentences = pd.read_csv('df_filtred_petroner_uri_valid_conllu.csv')\n",
    "\n",
    "df_group = df_filtred_sentences.groupby('sentence')\n",
    "print('Numero total de sentenças pos-filtragem -> ',len(df_group))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Escolher se deseja criar Jsons para labelstudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_to_createJsons = True\n",
    "# is_to_createJsons = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Folder outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_folder_path = \"./JSONs_04_05\" #local onde são salvos os Jsons para labelstudio\n",
    "save_folder_path = \"./JSONs_valid\"\n",
    "save_csv_name = 'df_bert_sentences_valid.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rotina para processar as sentenças já filtradas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------\n",
      "sentence = 34\n",
      "Token 1 =   poço 7-BRG-12-SE --- Class 1 =  POÇO --- URI 1 =  #POCO_CD_POCO_007553\n",
      "Token 2 =   Bacia de Sergipe/Alagoas --- Class 2 =  BACIA --- URI 2 =  #BASE_CD_BACIA_116\n",
      "Relation Type =  located_in\n",
      "Observações:  1.a biozona foi originalmente definida em depósitos    [E2] Bacia de Sergipe/Alagoas [/E2] (testemunhos    [E1] poço 7-BRG-12-SE [/E1]), por Beurlen et al (1987)..\n",
      "-------------\n",
      "Saved Json -> True\n",
      "-------------\n",
      "-------------\n",
      "sentence = 47\n",
      "Token 1 =   Albiano --- Class 1 =  UNIDADE_CRONO --- URI 1 =  #Albian\n",
      "Token 2 =   Aptiano --- Class 2 =  UNIDADE_CRONO --- URI 2 =  #Aptian\n",
      "Relation Type =  temporal_relation\n",
      "Cronoestratigrafia: [E1] Albiano [/E1], podendo, entretanto, englobar parte    [E2] Aptiano [/E2]..\n",
      "-------------\n",
      "-------------\n",
      "sentence = 47\n",
      "Token 1 =   Aptiano --- Class 1 =  UNIDADE_CRONO --- URI 1 =  #Aptian\n",
      "Token 2 =   Albiano --- Class 2 =  UNIDADE_CRONO --- URI 2 =  #Albian\n",
      "Relation Type =  temporal_relation\n",
      "Cronoestratigrafia: [E2] Albiano [/E2], podendo, entretanto, englobar parte    [E1] Aptiano [/E1]..\n",
      "-------------\n",
      "Saved Json -> True\n",
      "-------------\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "numberSentences = df_filtred_sentences.iloc[-1]['sentence'] #numero de sentencas diferentes no arquivo ja filtrado\n",
    "lista_relacoes, lista_uris, lista_classes, list_sentences_dict = [], [], [], []\n",
    "df_relation, df_bert = pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "countJsons=0\n",
    "for idx in range(1,len(df_group)):\n",
    "    filtred_sentence = df_group.get_group(idx)#aqui filtred_sentence é um dataframe da sentenca\n",
    "    originalSentenceNumber = filtred_sentence.iloc[0]['#sentence_original']\n",
    "    text = filtred_sentence.iloc[0]['text']\n",
    "    lista_relacoes,lista_uris,lista_classes,df_bert,\\\n",
    "    df_relation,df_entity,lista_relacoes_sentence,\\\n",
    "    from_id,to_id, is_to_save = go_through_sentence(filtred_sentence,df_relation,df_bert,originalSentenceNumber) \n",
    "    df_bert.to_csv(save_csv_name, encoding='utf-8',index=False)\n",
    "    df_relation.to_csv('df_relation_valid.csv', encoding='utf-8',index=False)\n",
    "    if is_to_save and is_to_createJsons:#Jsons somente criados para sentencas com relacao\n",
    "        countJsons+=1\n",
    "        saveJsonFiles(df_entity,from_id,to_id, \n",
    "                      lista_relacoes_sentence,filtred_sentence,originalSentenceNumber,save_folder_path)\n",
    "#     raise SystemExit(\"Stop right there!\")    \n",
    "print(\"-------------\")\n",
    "print(\"Number of Jsons saved = \", countJsons)\n",
    "\n",
    "# pickle.dump(df_relation, open('df_relation.pkl','wb'), protocol=pickle.HIGHEST_PROTOCOL)\n",
    "df_relation.to_csv('df_relation_valid.csv', encoding='utf-8',index=False)\n",
    "df_bert.to_csv(save_csv_name, encoding='utf-8',index=False)\n",
    "\n",
    "relacoes, numb_rel = np.unique(lista_relacoes, return_counts = True)\n",
    "pickle.dump(lista_relacoes,open('lista_relacoes_valid.pkl','wb'),protocol = pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "uris, numb_uris = np.unique(lista_uris, return_counts = True)\n",
    "pickle.dump(lista_uris,open('lista_uris_valid.pkl','wb'),protocol = pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "classes, numb_classes = np.unique(lista_classes, return_counts = True)\n",
    "pickle.dump(lista_classes,open('lista_classes_valid.pkl','wb'),protocol = pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contabilização das relações encontradas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['constituted_by',\n",
       " 'crosses',\n",
       " 'has_age',\n",
       " 'interval_finishes',\n",
       " 'located_in',\n",
       " 'temporal_relation']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relacoes.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[402, 29, 191, 1, 526, 1143]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numb_rel.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contabilização das classes encontradas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BACIA',\n",
       " 'CAMPO',\n",
       " 'NÃOCONSOLID',\n",
       " 'POÇO',\n",
       " 'ROCHA',\n",
       " 'UNIDADE_CRONO',\n",
       " 'UNIDADE_LITO']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[492, 162, 7, 118, 402, 2246, 1157]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numb_classes.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contabilização das URIs encontradas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#Alagoas_Age',\n",
       " '#Albian',\n",
       " '#Aptian',\n",
       " '#Aratu_Age',\n",
       " '#Archean',\n",
       " '#Atokan_Age',\n",
       " '#BASE_CD_BACIA_020',\n",
       " '#BASE_CD_BACIA_030',\n",
       " '#BASE_CD_BACIA_051',\n",
       " '#BASE_CD_BACIA_076',\n",
       " '#BASE_CD_BACIA_080',\n",
       " '#BASE_CD_BACIA_090',\n",
       " '#BASE_CD_BACIA_096',\n",
       " '#BASE_CD_BACIA_100',\n",
       " '#BASE_CD_BACIA_106',\n",
       " '#BASE_CD_BACIA_116',\n",
       " '#BASE_CD_BACIA_210',\n",
       " '#BASE_CD_BACIA_215',\n",
       " '#BASE_CD_BACIA_230',\n",
       " '#BASE_CD_BACIA_240',\n",
       " '#BASE_CD_BACIA_250',\n",
       " '#BASE_CD_BACIA_256',\n",
       " '#BASE_CD_BACIA_260',\n",
       " '#BASE_CD_BACIA_266',\n",
       " '#BASE_CD_BACIA_270',\n",
       " '#BASE_CD_BACIA_281',\n",
       " '#BASE_CD_BACIA_300',\n",
       " '#BASE_CD_BACIA_316',\n",
       " '#BASE_CD_BACIA_381',\n",
       " '#Barremian',\n",
       " '#Bartonian',\n",
       " '#Bashkirian',\n",
       " '#Berriasian',\n",
       " '#Buracica_Age',\n",
       " '#Burdigalian',\n",
       " '#CAMP_CD_CAMPO_0003',\n",
       " '#CAMP_CD_CAMPO_0004',\n",
       " '#CAMP_CD_CAMPO_0012',\n",
       " '#CAMP_CD_CAMPO_0017',\n",
       " '#CAMP_CD_CAMPO_0027',\n",
       " '#CAMP_CD_CAMPO_0065',\n",
       " '#CAMP_CD_CAMPO_0077',\n",
       " '#CAMP_CD_CAMPO_0082',\n",
       " '#CAMP_CD_CAMPO_0093',\n",
       " '#CAMP_CD_CAMPO_0118',\n",
       " '#CAMP_CD_CAMPO_0174',\n",
       " '#CAMP_CD_CAMPO_0179',\n",
       " '#CAMP_CD_CAMPO_0199',\n",
       " '#CAMP_CD_CAMPO_0214',\n",
       " '#CAMP_CD_CAMPO_0234',\n",
       " '#CAMP_CD_CAMPO_0239',\n",
       " '#CAMP_CD_CAMPO_0247',\n",
       " '#CAMP_CD_CAMPO_0248',\n",
       " '#CAMP_CD_CAMPO_0249',\n",
       " '#CAMP_CD_CAMPO_0264',\n",
       " '#CAMP_CD_CAMPO_0279',\n",
       " '#CAMP_CD_CAMPO_0309',\n",
       " '#CAMP_CD_CAMPO_0322',\n",
       " '#CAMP_CD_CAMPO_0352',\n",
       " '#CAMP_CD_CAMPO_0362',\n",
       " '#CAMP_CD_CAMPO_0363',\n",
       " '#CAMP_CD_CAMPO_0373',\n",
       " '#CAMP_CD_CAMPO_0383',\n",
       " '#CAMP_CD_CAMPO_0388',\n",
       " '#CAMP_CD_CAMPO_0393',\n",
       " '#CAMP_CD_CAMPO_0403',\n",
       " '#CAMP_CD_CAMPO_0444',\n",
       " '#CAMP_CD_CAMPO_0477',\n",
       " '#CAMP_CD_CAMPO_0519',\n",
       " '#CAMP_CD_CAMPO_0523',\n",
       " '#CAMP_CD_CAMPO_0536',\n",
       " '#CAMP_CD_CAMPO_0545',\n",
       " '#CAMP_CD_CAMPO_0575',\n",
       " '#CAMP_CD_CAMPO_0603',\n",
       " '#CAMP_CD_CAMPO_0637',\n",
       " '#CAMP_CD_CAMPO_0668',\n",
       " '#CAMP_CD_CAMPO_0682',\n",
       " '#CAMP_CD_CAMPO_0683',\n",
       " '#CAMP_CD_CAMPO_0684',\n",
       " '#CAMP_CD_CAMPO_0718',\n",
       " '#CAMP_CD_CAMPO_0725',\n",
       " '#CAMP_CD_CAMPO_0734',\n",
       " '#CAMP_CD_CAMPO_0750',\n",
       " '#CAMP_CD_CAMPO_0760',\n",
       " '#CAMP_CD_CAMPO_0773',\n",
       " '#CAMP_CD_CAMPO_0803',\n",
       " '#CAMP_CD_CAMPO_0819',\n",
       " '#CAMP_CD_CAMPO_0830',\n",
       " '#CAMP_CD_CAMPO_0861',\n",
       " '#CAMP_CD_CAMPO_0881',\n",
       " '#CAMP_CD_CAMPO_0888',\n",
       " '#CAMP_CD_CAMPO_0912',\n",
       " '#CAMP_CD_CAMPO_0916',\n",
       " '#CAMP_CD_CAMPO_0929',\n",
       " '#CAMP_CD_CAMPO_0932',\n",
       " '#CAMP_CD_CAMPO_0951',\n",
       " '#CAMP_CD_CAMPO_0958',\n",
       " '#CAMP_CD_CAMPO_0970',\n",
       " '#CAMP_CD_CAMPO_0975',\n",
       " '#CAMP_CD_CAMPO_0995',\n",
       " '#Cambrian',\n",
       " '#Campanian',\n",
       " '#Carboniferous',\n",
       " '#Cenomanian',\n",
       " '#Cenozoic',\n",
       " '#Chattian',\n",
       " '#Coniacian',\n",
       " '#Cretaceous',\n",
       " '#Danian',\n",
       " '#Desmoinesian_Age',\n",
       " '#Devonian',\n",
       " '#Dom_Joao_Age',\n",
       " '#Eoburacica_Subage',\n",
       " '#Eocene',\n",
       " '#Eojiquia_Subage',\n",
       " '#Frasnian',\n",
       " '#Hauterivian',\n",
       " '#Holocene',\n",
       " '#Jiquia_Age',\n",
       " '#Jurassic',\n",
       " '#LowerCretaceous',\n",
       " '#LowerDevonian',\n",
       " '#LowerTriassic',\n",
       " '#Lower_Albian_Subage',\n",
       " '#Lower_Aptian_Subage',\n",
       " '#Lower_Cenomanian_Subage',\n",
       " '#Lower_Maastrichtian_Subage',\n",
       " '#Lower_Turonian_Subage',\n",
       " '#Lutetian',\n",
       " '#Maastrichtian',\n",
       " '#Mesoriodaserra_Subage',\n",
       " '#Mesozoic',\n",
       " '#MiddleJurassic',\n",
       " '#Middle_Albian_Subage',\n",
       " '#Miocene',\n",
       " '#Mississippian',\n",
       " '#Missourian_Age',\n",
       " '#Morrowan_Age',\n",
       " '#Neoburacica_Subage',\n",
       " '#Neogene',\n",
       " '#Neojiquia_Subage',\n",
       " '#Neoriodaserra_Subage',\n",
       " '#Oligocene',\n",
       " '#Oxfordian',\n",
       " '#POCO_CD_POCO_005665',\n",
       " '#POCO_CD_POCO_006088',\n",
       " '#POCO_CD_POCO_006789',\n",
       " '#POCO_CD_POCO_006870',\n",
       " '#POCO_CD_POCO_006882',\n",
       " '#POCO_CD_POCO_006960',\n",
       " '#POCO_CD_POCO_007020',\n",
       " '#POCO_CD_POCO_007040',\n",
       " '#POCO_CD_POCO_007052',\n",
       " '#POCO_CD_POCO_007076',\n",
       " '#POCO_CD_POCO_007140',\n",
       " '#POCO_CD_POCO_007553',\n",
       " '#POCO_CD_POCO_007746',\n",
       " '#POCO_CD_POCO_007748',\n",
       " '#POCO_CD_POCO_008339',\n",
       " '#POCO_CD_POCO_009032',\n",
       " '#POCO_CD_POCO_009065',\n",
       " '#POCO_CD_POCO_009422',\n",
       " '#POCO_CD_POCO_009757',\n",
       " '#POCO_CD_POCO_010471',\n",
       " '#POCO_CD_POCO_010526',\n",
       " '#POCO_CD_POCO_010643',\n",
       " '#POCO_CD_POCO_010669',\n",
       " '#POCO_CD_POCO_010670',\n",
       " '#POCO_CD_POCO_010689',\n",
       " '#POCO_CD_POCO_010691',\n",
       " '#POCO_CD_POCO_010726',\n",
       " '#POCO_CD_POCO_010831',\n",
       " '#POCO_CD_POCO_011890',\n",
       " '#POCO_CD_POCO_011894',\n",
       " '#POCO_CD_POCO_011907',\n",
       " '#POCO_CD_POCO_011911',\n",
       " '#POCO_CD_POCO_012020',\n",
       " '#POCO_CD_POCO_012040',\n",
       " '#POCO_CD_POCO_012091',\n",
       " '#POCO_CD_POCO_012097',\n",
       " '#POCO_CD_POCO_012120',\n",
       " '#POCO_CD_POCO_012153',\n",
       " '#POCO_CD_POCO_012155',\n",
       " '#POCO_CD_POCO_012186',\n",
       " '#POCO_CD_POCO_012864',\n",
       " '#POCO_CD_POCO_012914',\n",
       " '#POCO_CD_POCO_012983',\n",
       " '#POCO_CD_POCO_012996',\n",
       " '#POCO_CD_POCO_015948',\n",
       " '#POCO_CD_POCO_016511',\n",
       " '#POCO_CD_POCO_016933',\n",
       " '#POCO_CD_POCO_017402',\n",
       " '#POCO_CD_POCO_017478',\n",
       " '#POCO_CD_POCO_018470',\n",
       " '#POCO_CD_POCO_020490',\n",
       " '#POCO_CD_POCO_020848',\n",
       " '#POCO_CD_POCO_021725',\n",
       " '#POCO_CD_POCO_021906',\n",
       " '#POCO_CD_POCO_021985',\n",
       " '#POCO_CD_POCO_022660',\n",
       " '#POCO_CD_POCO_022687',\n",
       " '#POCO_CD_POCO_022866',\n",
       " '#POCO_CD_POCO_022975',\n",
       " '#POCO_CD_POCO_023049',\n",
       " '#POCO_CD_POCO_023169',\n",
       " '#POCO_CD_POCO_023172',\n",
       " '#POCO_CD_POCO_023241',\n",
       " '#POCO_CD_POCO_023376',\n",
       " '#POCO_CD_POCO_023745',\n",
       " '#POCO_CD_POCO_023746',\n",
       " '#POCO_CD_POCO_024176',\n",
       " '#POCO_CD_POCO_024384',\n",
       " '#POCO_CD_POCO_024465',\n",
       " '#POCO_CD_POCO_024505',\n",
       " '#POCO_CD_POCO_024918',\n",
       " '#POCO_CD_POCO_025010',\n",
       " '#POCO_CD_POCO_025253',\n",
       " '#Paleocene',\n",
       " '#Paleogene',\n",
       " '#Paleoproterozoic',\n",
       " '#Paleozoic',\n",
       " '#Pennsylvanian',\n",
       " '#Permian',\n",
       " '#Phanerozoic',\n",
       " '#Pleistocene',\n",
       " '#Pliensbachian',\n",
       " '#Pliocene',\n",
       " '#Priabonian',\n",
       " '#Proterozoic',\n",
       " '#Quaternary',\n",
       " '#Rupelian',\n",
       " '#Santonian',\n",
       " '#Silurian',\n",
       " '#Sinemurian',\n",
       " '#Thanetian',\n",
       " '#Tithonian',\n",
       " '#Toarcian',\n",
       " '#Triassic',\n",
       " '#Turonian',\n",
       " '#UpperCretaceous',\n",
       " '#UpperDevonian',\n",
       " '#UpperJurassic',\n",
       " '#UpperTriassic',\n",
       " '#Upper_Albian_Subage',\n",
       " '#Upper_Aptian_Subage',\n",
       " '#Upper_Campanian_Subage',\n",
       " '#Upper_Maastrichtian_Subage',\n",
       " '#Valanginian',\n",
       " '#Virgilian_Age',\n",
       " '#Ypresian',\n",
       " '#anhydrite',\n",
       " '#basalt',\n",
       " '#calciarenite',\n",
       " '#calcilutite',\n",
       " '#carnallite',\n",
       " '#clay',\n",
       " '#coal',\n",
       " '#conglomerate',\n",
       " '#diabase',\n",
       " '#diamictite',\n",
       " '#formacao_002',\n",
       " '#formacao_003',\n",
       " '#formacao_009',\n",
       " '#formacao_013',\n",
       " '#formacao_016',\n",
       " '#formacao_019',\n",
       " '#formacao_025',\n",
       " '#formacao_027',\n",
       " '#formacao_028',\n",
       " '#formacao_044',\n",
       " '#formacao_050',\n",
       " '#formacao_056',\n",
       " '#formacao_059',\n",
       " '#formacao_083',\n",
       " '#formacao_084',\n",
       " '#formacao_095',\n",
       " '#formacao_096',\n",
       " '#formacao_102',\n",
       " '#formacao_103',\n",
       " '#formacao_110',\n",
       " '#formacao_113',\n",
       " '#formacao_116',\n",
       " '#formacao_117',\n",
       " '#formacao_119',\n",
       " '#formacao_121',\n",
       " '#formacao_122',\n",
       " '#formacao_125',\n",
       " '#formacao_126',\n",
       " '#formacao_130',\n",
       " '#formacao_131',\n",
       " '#formacao_136',\n",
       " '#formacao_142',\n",
       " '#formacao_145',\n",
       " '#formacao_149',\n",
       " '#formacao_153',\n",
       " '#formacao_154',\n",
       " '#formacao_157',\n",
       " '#formacao_158',\n",
       " '#formacao_159',\n",
       " '#formacao_163',\n",
       " '#formacao_166',\n",
       " '#formacao_171',\n",
       " '#formacao_183',\n",
       " '#formacao_187',\n",
       " '#formacao_193',\n",
       " '#formacao_200',\n",
       " '#formacao_204',\n",
       " '#formacao_207',\n",
       " '#formacao_210',\n",
       " '#formacao_215',\n",
       " '#formacao_216',\n",
       " '#formacao_217',\n",
       " '#formacao_222',\n",
       " '#formacao_224',\n",
       " '#formacao_228',\n",
       " '#formacao_229',\n",
       " '#formacao_232',\n",
       " '#formacao_236',\n",
       " '#formacao_239',\n",
       " '#formacao_242',\n",
       " '#formacao_246',\n",
       " '#formacao_249',\n",
       " '#formacao_251',\n",
       " '#formacao_253',\n",
       " '#formacao_254',\n",
       " '#formacao_255',\n",
       " '#formacao_256',\n",
       " '#formacao_259',\n",
       " '#formacao_260',\n",
       " '#formacao_262',\n",
       " '#formacao_264',\n",
       " '#formacao_266',\n",
       " '#formacao_277',\n",
       " '#formacao_278',\n",
       " '#formacao_281',\n",
       " '#formacao_286',\n",
       " '#formacao_287',\n",
       " '#formacao_289',\n",
       " '#formacao_292',\n",
       " '#formacao_294',\n",
       " '#formacao_295',\n",
       " '#formacao_296',\n",
       " '#formacao_305',\n",
       " '#formacao_319',\n",
       " '#formacao_328',\n",
       " '#formacao_331',\n",
       " '#formacao_335',\n",
       " '#formacao_338',\n",
       " '#formacao_341',\n",
       " '#formacao_343',\n",
       " '#formacao_344',\n",
       " '#grupo_000',\n",
       " '#grupo_008',\n",
       " '#grupo_009',\n",
       " '#grupo_017',\n",
       " '#grupo_018',\n",
       " '#grupo_022',\n",
       " '#grupo_024',\n",
       " '#grupo_032',\n",
       " '#grupo_033',\n",
       " '#grupo_038',\n",
       " '#grupo_040',\n",
       " '#grupo_048',\n",
       " '#grupo_049',\n",
       " '#grupo_05',\n",
       " '#grupo_055',\n",
       " '#grupo_057',\n",
       " '#grupo_062',\n",
       " '#halite',\n",
       " '#igneous_rock',\n",
       " '#limestone',\n",
       " '#marlstone',\n",
       " '#membro_000',\n",
       " '#membro_002',\n",
       " '#membro_005',\n",
       " '#membro_007',\n",
       " '#membro_010',\n",
       " '#membro_013',\n",
       " '#membro_019',\n",
       " '#membro_020',\n",
       " '#membro_022',\n",
       " '#membro_025',\n",
       " '#membro_027',\n",
       " '#membro_028',\n",
       " '#membro_032',\n",
       " '#membro_036',\n",
       " '#membro_037',\n",
       " '#membro_040',\n",
       " '#membro_042',\n",
       " '#membro_044',\n",
       " '#membro_045',\n",
       " '#membro_050',\n",
       " '#membro_052',\n",
       " '#membro_058',\n",
       " '#membro_060',\n",
       " '#membro_061',\n",
       " '#membro_070',\n",
       " '#membro_071',\n",
       " '#membro_075',\n",
       " '#membro_078',\n",
       " '#pyroclast',\n",
       " '#sand',\n",
       " '#sandstone',\n",
       " '#shale',\n",
       " '#siltstone',\n",
       " '#tachyhydrite']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uris.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[64,\n",
       " 129,\n",
       " 112,\n",
       " 31,\n",
       " 6,\n",
       " 40,\n",
       " 2,\n",
       " 12,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 35,\n",
       " 49,\n",
       " 4,\n",
       " 111,\n",
       " 1,\n",
       " 4,\n",
       " 11,\n",
       " 55,\n",
       " 8,\n",
       " 1,\n",
       " 9,\n",
       " 26,\n",
       " 27,\n",
       " 75,\n",
       " 28,\n",
       " 22,\n",
       " 4,\n",
       " 19,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 45,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 6,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 13,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 8,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 6,\n",
       " 1,\n",
       " 1,\n",
       " 12,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 6,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 46,\n",
       " 135,\n",
       " 51,\n",
       " 47,\n",
       " 36,\n",
       " 18,\n",
       " 36,\n",
       " 98,\n",
       " 10,\n",
       " 16,\n",
       " 35,\n",
       " 29,\n",
       " 18,\n",
       " 28,\n",
       " 2,\n",
       " 2,\n",
       " 14,\n",
       " 52,\n",
       " 46,\n",
       " 30,\n",
       " 94,\n",
       " 2,\n",
       " 2,\n",
       " 17,\n",
       " 11,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 6,\n",
       " 120,\n",
       " 2,\n",
       " 36,\n",
       " 2,\n",
       " 5,\n",
       " 13,\n",
       " 4,\n",
       " 8,\n",
       " 24,\n",
       " 26,\n",
       " 8,\n",
       " 2,\n",
       " 3,\n",
       " 20,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 6,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 48,\n",
       " 16,\n",
       " 4,\n",
       " 56,\n",
       " 4,\n",
       " 58,\n",
       " 2,\n",
       " 54,\n",
       " 6,\n",
       " 3,\n",
       " 12,\n",
       " 14,\n",
       " 8,\n",
       " 14,\n",
       " 59,\n",
       " 6,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 8,\n",
       " 24,\n",
       " 45,\n",
       " 34,\n",
       " 6,\n",
       " 49,\n",
       " 4,\n",
       " 6,\n",
       " 29,\n",
       " 2,\n",
       " 10,\n",
       " 26,\n",
       " 4,\n",
       " 14,\n",
       " 5,\n",
       " 8,\n",
       " 1,\n",
       " 11,\n",
       " 1,\n",
       " 5,\n",
       " 3,\n",
       " 15,\n",
       " 3,\n",
       " 19,\n",
       " 1,\n",
       " 1,\n",
       " 20,\n",
       " 8,\n",
       " 9,\n",
       " 1,\n",
       " 2,\n",
       " 27,\n",
       " 12,\n",
       " 2,\n",
       " 4,\n",
       " 20,\n",
       " 1,\n",
       " 7,\n",
       " 1,\n",
       " 19,\n",
       " 1,\n",
       " 11,\n",
       " 17,\n",
       " 19,\n",
       " 20,\n",
       " 1,\n",
       " 17,\n",
       " 2,\n",
       " 2,\n",
       " 8,\n",
       " 5,\n",
       " 1,\n",
       " 12,\n",
       " 2,\n",
       " 1,\n",
       " 7,\n",
       " 5,\n",
       " 19,\n",
       " 17,\n",
       " 11,\n",
       " 7,\n",
       " 6,\n",
       " 6,\n",
       " 3,\n",
       " 2,\n",
       " 5,\n",
       " 4,\n",
       " 8,\n",
       " 6,\n",
       " 19,\n",
       " 9,\n",
       " 2,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 10,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 6,\n",
       " 7,\n",
       " 46,\n",
       " 29,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 18,\n",
       " 15,\n",
       " 5,\n",
       " 6,\n",
       " 30,\n",
       " 4,\n",
       " 20,\n",
       " 6,\n",
       " 5,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 9,\n",
       " 8,\n",
       " 3,\n",
       " 2,\n",
       " 22,\n",
       " 25,\n",
       " 12,\n",
       " 7,\n",
       " 25,\n",
       " 5,\n",
       " 6,\n",
       " 27,\n",
       " 11,\n",
       " 1,\n",
       " 7,\n",
       " 3,\n",
       " 22,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 14,\n",
       " 3,\n",
       " 10,\n",
       " 52,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 9,\n",
       " 14,\n",
       " 3,\n",
       " 2,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 5,\n",
       " 4,\n",
       " 10,\n",
       " 5,\n",
       " 8,\n",
       " 42,\n",
       " 4,\n",
       " 9,\n",
       " 10,\n",
       " 2,\n",
       " 9,\n",
       " 8,\n",
       " 1,\n",
       " 2,\n",
       " 8,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 11,\n",
       " 6,\n",
       " 4,\n",
       " 10,\n",
       " 2,\n",
       " 1,\n",
       " 12,\n",
       " 8,\n",
       " 7,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 198,\n",
       " 109,\n",
       " 19,\n",
       " 1]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numb_uris.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verificação de pares de entidades por tipo de relação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of types of relations -> 6\n"
     ]
    }
   ],
   "source": [
    "# df_filtred = pickle.load(open('df_relation.pkl', 'rb'))\n",
    "df_relations = pd.read_csv('df_relation.csv')\n",
    "df_grp = df_relations.groupby('Relation')\n",
    "relations_groups = df_grp.groups\n",
    "relations = list(relations_groups)\n",
    "lista_pares = []\n",
    "for relation in relations:\n",
    "    df_rel = df_grp.get_group(relation)\n",
    "    list_rel = []\n",
    "    for idx_rel in range(0,len(df_rel)):\n",
    "        par = df_rel.iloc[idx_rel]['Ent1'] + ' + ' + df_rel.iloc[idx_rel]['Ent2']\n",
    "        list_rel.append(par)\n",
    "    lista_pares.append(list_rel)\n",
    "print('Number of types of relations ->', len(lista_pares))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avaliar idx_pair de 0 ao tamanho apresentado acima para verificar os pares de entidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relation ->  constituted_by\n",
      "Entities pair ->  ['UNIDADE_LITO + ROCHA']\n",
      "Number of ocorrences ->  [402]\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "pares, numb_pares = np.unique(lista_pares[idx], return_counts = True)\n",
    "print('Relation -> ',relations[idx])\n",
    "print('Entities pair -> ',pares.tolist())\n",
    "print('Number of ocorrences -> ',numb_pares.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
