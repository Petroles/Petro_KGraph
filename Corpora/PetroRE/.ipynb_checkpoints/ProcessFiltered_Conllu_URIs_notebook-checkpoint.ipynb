{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "* Owlready2 * Warning: optimized Cython parser module 'owlready2_optimized' is not available, defaulting to slower Python implementation\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import operator as op\n",
    "import warnings \n",
    "from owlready2 import * #\n",
    "import random\n",
    "import unicodedata\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregar ontologia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "get_ontology(\"http://www.semanticweb.org/bg40/ontologies/2022/5/untitled-ontology-2#\")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onto_name = \"OntoGeoLogicaInstanciasRelacoes\"\n",
    "onto = get_ontology(\"../../KnowledgeGraph/OntoGeoLogicaInstanciasRelacoes.owl\")\n",
    "onto.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções para procurar na ontologia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relations_between_uris(uri_1, uri_2): \n",
    "    #funcao que acessa a ontologia e procura relacao entre URIs\n",
    "    dict_relation_uris = {}\n",
    "    #Pega as relacoes que a URI1 tem\n",
    "    relation_query_results = list(default_world.sparql(\"\"\"\n",
    "            SELECT DISTINCT ?rel\n",
    "            WHERE{?uri ?rel ?obj\n",
    "                 FILTER(contains(str(?rel), \"http://www.semanticweb.org/bg40/ontologies/2022/5/untitled-ontology-2#\"))\n",
    "                 FILTER (contains(str(?uri), \"\"\" + '\"' + uri_1 + '\"' + \"\"\"))\n",
    "                 }\n",
    "            \"\"\"))\n",
    "    \n",
    "    relations_str = []\n",
    "    for relation_uris in relation_query_results:\n",
    "        relations_str.append(str(relation_uris[0]).rsplit(\".\",1)[-1])\n",
    "        \n",
    "    # Para cada tipo de relação procura se existe match entre URI1 e URI2\n",
    "    for relation in relations_str:\n",
    "        relation_between_words = list(default_world.sparql(\"\"\"\n",
    "                PREFIX prefix: <http://www.semanticweb.org/bg40/ontologies/2022/5/untitled-ontology-2#>\n",
    "                SELECT distinct ?y ?x2\n",
    "                WHERE{?y prefix:\"\"\" +  relation  +  \"\"\" ?x1\n",
    "\n",
    "                      FILTER (contains(str(?y), \"\"\" + '\"' + uri_1  + '\"' + \"\"\"))        \n",
    "\n",
    "                      ?x2 rdf:type ?j                                   \n",
    "                      FILTER (contains(str(?x2), \"\"\" + '\"' + uri_2  + '\"' + \"\"\"))\n",
    "\n",
    "                      FILTER ( ?x2 = ?x1 )\n",
    "                    }\n",
    "                \"\"\"))\n",
    "        dict_relation_uris[relation] = relation_between_words\n",
    "    return dict_relation_uris\n",
    "\n",
    "def go_through_relations(uri1,uri2):\n",
    "    relation_uris = get_relations_between_uris(uri1, uri2)            \n",
    "    if relation_uris != {}: #talvez exista relacao entre URIs, dicionario pode vir vazio -> []\n",
    "        for x, y in relation_uris.items():#procurar por relacao\n",
    "            if y != []: #existe alguma relacao\n",
    "#                 print(x)\n",
    "                return x\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funções para printar informações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sentence_text(sentence):\n",
    "    #printa e retorna o texto original da sentenca\n",
    "    size_sentence = int(sentence.iloc[-1][\"end\"])\n",
    "    text = \" \"*size_sentence\n",
    "    for index, row in sentence.iterrows():\n",
    "        text = text[:int(row[\"start\"])] + row[\"form\"] +text[int(row[\"end\"]):]\n",
    "    print(text)\n",
    "    print(\"-------------\")\n",
    "    return text\n",
    "\n",
    "def print_relation_entities(word1,word2,ent1,ent2,URI_1,URI_2,relation_type,text):\n",
    "    #printa as entidades e relacao entre elas\n",
    "    print('Token 1 = ', word1, '--- Class 1 = ', ent1, '--- URI 1 = ', URI_1)\n",
    "    print('Token 2 = ', word2, '--- Class 2 = ', ent2,'--- URI 2 = ',URI_2)\n",
    "    print('Relation Type = ', relation_type)\n",
    "    print(text)\n",
    "    print(\"-------------\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções para gerar Jsons a serem lidos no labelstudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResultRelationJson(object):\n",
    "    def __init__(self, from_id, to_id, relations, direction = \"right\"):\n",
    "        self.dict = {\n",
    "            \"from_id\": str(from_id),\n",
    "            \"to_id\": str(to_id),\n",
    "            \"type\": \"relation\",\n",
    "            \"direction\": direction,\n",
    "            \"labels\": relations\n",
    "        }\n",
    "    def get_dict(self):\n",
    "        return self.dict\n",
    "class ResultNERJson(object):\n",
    "    def __init__(self, row):     \n",
    "        self.result_dict = {\n",
    "            \"value\": {\n",
    "            \"start\": row[\"start_word\"],\n",
    "            \"end\": row[\"end_word\"],\n",
    "            \"text\": row[\"word_join\"],\n",
    "            \"labels\": [\n",
    "              row[\"label_word\"]\n",
    "            ],\n",
    "            \"URI\": row[\"URI\"]\n",
    "            },\n",
    "            \n",
    "            \"id\": row[\"index_e\"],\n",
    "            \"from_name\": \"label\",\n",
    "            \"to_name\": \"text\",\n",
    "            \"type\": \"labels\",\n",
    "            \"origin\": \"prediction\"\n",
    "        }\n",
    "    def get_dict(self):\n",
    "        return self.result_dict  \n",
    "class CreateOutput(object):\n",
    "    def __init__(self, text, filtred_sentence, entity_name_new):\n",
    "        self.filtred_sentence = filtred_sentence\n",
    "        self.entity_name_new = entity_name_new\n",
    "        self.main_dict = {\n",
    "            \"id\": 1,\n",
    "            \"data\": {\n",
    "              \"text\": text #sentenca inteira\n",
    "            },\n",
    "            \"annotations\": []\n",
    "        }\n",
    "        self._add_annotations()      \n",
    "    def _add_annotations(self):\n",
    "        results = []\n",
    "        count = 0        \n",
    "        for index, row in self.entity_name_new.iterrows(): \n",
    "            results.append(ResultNERJson(row).get_dict())        \n",
    "        item = [{\n",
    "              \"id\": 1,\n",
    "              \"created_username\": \" null, 0\",\n",
    "              \"created_ago\": \"\",\n",
    "              \"result\": results\n",
    "            }]\n",
    "        self.main_dict[\"annotations\"] = item\n",
    "    def get_output(self):\n",
    "        return self.main_dict\n",
    "    def add_relationship(self, from_id, to_id, relations, direction):\n",
    "        results = self.main_dict.get(\"annotations\")[0].get(\"result\")\n",
    "        relation = ResultRelationJson(from_id, to_id, [relations], direction).get_dict()\n",
    "        results.append(relation)\n",
    "        self.main_dict[\"annotations\"][0][\"result\"] = results   \n",
    "        \n",
    "def combine_itens_from_lists_add_in_json(from_id_vec, to_id_vec, relation_from_vec, output):\n",
    "    for idxRelation in range(0,len(from_id_vec)):\n",
    "        direction = \"right\"\n",
    "        output.add_relationship(from_id=from_id_vec[idxRelation], to_id=to_id_vec[idxRelation], relations = relation_from_vec[idxRelation], direction=direction)\n",
    "    return output\n",
    "def saveJsonFiles(df,from_id,to_id, lista_relaoces_sentence,sentence,SentenceNum,path):\n",
    "    #cria e salva o arquivo Json para labelstudio\n",
    "    text = sentence.iloc[0]['text']\n",
    "    print('Saved Json ->', True)\n",
    "    output = CreateOutput(text,sentence, df)\n",
    "    combine_itens_from_lists_add_in_json(from_id, to_id, lista_relacoes_sentence, output)\n",
    "    print(\"-------------\")\n",
    "    with open(os.path.join(path,f\"{SentenceNum}.json\"), \"w\") as outfile: \n",
    "        json.dump(output.get_output(), outfile) \n",
    "        \n",
    "def get_df_forJsons(sentence,idxTokens):\n",
    "    #retorna um dataframe com as informações das entidades e uma string contendo o nome completo da entidade\n",
    "    df_save_words = pd.DataFrame(columns=['index_e', \"LABEL\", \"START\", \"END\",\\\n",
    "                                      \"TEXT\", \"word_join\", \"start_word\", \"end_word\", \"label_word\",\"URI\"])\n",
    "\n",
    "    index_e = sentence.iloc[idxTokens]['index_e']\n",
    "    label = sentence.iloc[idxTokens]['deps']\n",
    "    start = sentence.iloc[idxTokens]['word_join_start']\n",
    "    end = start + len(sentence.iloc[idxTokens]['form'])\n",
    "    text_ent = sentence.iloc[idxTokens]['form']\n",
    "    word_join = sentence.iloc[idxTokens]['word_join']\n",
    "    start_word = sentence.iloc[idxTokens]['word_join_start']\n",
    "    end_word = sentence.iloc[idxTokens]['word_join_end']\n",
    "    label_word = label.replace(\"B=\",\"\")\n",
    "    URI = sentence.iloc[idxTokens]['grafo']\n",
    "\n",
    "    df_save_words.loc[len(df_save_words.index)] = [index_e, label, start, end, text_ent, word_join,\n",
    "                                                   start_word,\n",
    "                                                   end_word,\n",
    "                                                   label_word,\n",
    "                                                   URI]\n",
    "\n",
    "    return df_save_words, word_join\n",
    "        \n",
    "def create_df_JsonFiles(df_entity,x,token,token2,URI_1,URI_2,idxTokens,idxTokens2,from_id,to_id,sentence):\n",
    "    #retorna o dataframe utilizado para criacao dos arquivos Json para labelstudio\n",
    "    entity_name_new_token1,wordjoin_1 = get_df_forJsons(sentence,idxTokens)\n",
    "    entity_name_new_token2,wordjoin_2 = get_df_forJsons(sentence,idxTokens2)\n",
    "    if idxTokens not in from_id and idxTokens not in to_id:\n",
    "        df_entity = pd.concat([df_entity, entity_name_new_token1])\n",
    "    if idxTokens2 not in from_id and idxTokens2 not in to_id:\n",
    "        df_entity = pd.concat([df_entity, entity_name_new_token2])\n",
    "\n",
    "#     print_relation_entities(wordjoin_1,wordjoin_2,token.replace('B=',''),token2.replace('B=',''),URI_1,URI_2,x)\n",
    "    return df_entity  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funções para criar dataframe para modelo BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_moddedText_BERT(text,start1,end1,start2,end2,Ent1_inic,Ent1_end,Ent2_inic,Ent2_end):\n",
    "    new_end_ent1 = end1 + len(Ent1_inic)\n",
    "    new_start_ent2 = start2 + len(Ent1_inic) + len(Ent1_end)\n",
    "    new_end_ent2 = end2 + len(Ent1_inic) + len(Ent1_end) + len(Ent2_inic)      \n",
    "    #adicionando [E1] e [/E1]\n",
    "    text_new = text[:start1] + Ent1_inic + text[start1:]\n",
    "    text_new = text_new[:new_end_ent1] + Ent1_end + text_new[new_end_ent1:]\n",
    "    #adicionando [E2] e [/E2]\n",
    "    text_new2 = text_new[:new_start_ent2] + Ent2_inic + text_new[new_start_ent2:]\n",
    "    text_new2 = text_new2[:new_end_ent2] + Ent2_end + text_new2[new_end_ent2:]\n",
    "    \n",
    "    return text_new2\n",
    "\n",
    "def createText_sentence_BERT(text,start_1,end_1,start_2,end_2):\n",
    "    #funcao que retorna um novo texto para sentenca com [E1] e [E2] adicionados junto de cada entidade\n",
    "    start_ent1, start_ent2 = start_1, start_2\n",
    "    end_ent1, end_ent2 = end_1, end_2\n",
    "    Ent1_inic, Ent1_end = '[E1] ', ' [/E1]'\n",
    "    Ent2_inic, Ent2_end = '[E2] ', ' [/E2]'\n",
    "    \n",
    "    if start_ent1 < start_ent2: #[E1] vem antes de [E2]\n",
    "        text_new = create_moddedText_BERT(text,start_ent1,end_ent1,start_ent2,end_ent2,\\\n",
    "                                  Ent1_inic,Ent1_end,Ent2_inic,Ent2_end)\n",
    "    else: #[E2] vem antes de [E1]      \n",
    "        text_new = create_moddedText_BERT(text,start_ent2,end_ent2,start_ent1,end_ent1,\\\n",
    "                                      Ent2_inic,Ent2_end,Ent1_inic,Ent1_end)  \n",
    "    return text_new\n",
    "\n",
    "def create_bert_dataframe(df_bert,idxTokens,idxTokens2,sentence,URI_1,URI_2,has_relation,relation_type,SentenceNumber):\n",
    "    #retorna o dataframe com as informacoes de cada sentenca para utilizar no modelo BERT\n",
    "    df_bert_temp = pd.DataFrame(columns=['#Sentence','sentence','Ent1','Ent2','URI_1','URI_2','has_relation','relation'])\n",
    "    text = sentence.iloc[0]['text']\n",
    "    wordjoin_1, wordjoin_2 = sentence.iloc[idxTokens]['word_join'], sentence.iloc[idxTokens2]['word_join']\n",
    "    ent1, ent2 = sentence.iloc[idxTokens]['deps'], sentence.iloc[idxTokens2]['deps']\n",
    "    ent1, ent2 = ent1.replace(\"B=\",\"\"), ent2.replace(\"B=\",\"\")\n",
    "    start_1, start_2 = sentence.iloc[idxTokens]['word_join_start'], sentence.iloc[idxTokens2]['word_join_start']\n",
    "    end_1, end_2 = sentence.iloc[idxTokens]['word_join_end'], sentence.iloc[idxTokens2]['word_join_end']\n",
    "    text_bert_ents = createText_sentence_BERT(text,start_1,end_1,start_2,end_2)\n",
    "    df_bert_temp.loc[0] = [SentenceNumber,\n",
    "                           text_bert_ents,\n",
    "                           ent1,\n",
    "                           ent2,\n",
    "                           URI_1,\n",
    "                           URI_2,\n",
    "                           has_relation,\n",
    "                           relation_type]\n",
    "    df_bert = pd.concat([df_bert, df_bert_temp])\n",
    "    if relation_type!='no_relation':\n",
    "        print_relation_entities(wordjoin_1,wordjoin_2,ent1,ent2,URI_1,URI_2,relation_type,text_bert_ents)\n",
    "    return df_bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções para processar as sentenças"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utils\n",
    "def create_relations_dataframe(df_relation,token,token2,URI_1,URI_2,x,originalSentenceNumber):\n",
    "    #retorna dataframe das entidades e suas relações em cada linha\n",
    "    #importante para contabilizar os tipos de relação\n",
    "    df_relation_new = pd.DataFrame(columns=['Relation','Ent1','Ent2','URI_1','URI_2','#Sentence'])\n",
    "    df_relation_new.loc[0] = [x,\n",
    "                            token.replace('B=',''),\n",
    "                            token2.replace('B=',''),\n",
    "                            URI_1,\n",
    "                            URI_2,\n",
    "                            originalSentenceNumber]\n",
    "    df_relation = pd.concat([df_relation, df_relation_new])\n",
    "    return df_relation\n",
    "\n",
    "def verifica_pares_entidade_interesse(ENT_1, ENT_2,relation_type):\n",
    "    #verifica se a relacao encontrada vai ser do tipo temporal_relation CRONO->CRONO\n",
    "    #funcao talvez precise ser atualizada no futuro conforme a ontologia for povoada\n",
    "    lista_from = ['POÇO','UNIDADE_LITO','UNIDADE_LITO','CAMPO','POÇO','POÇO','UNIDADE_LITO','UNIDADE_LITO']\n",
    "    lista_to = ['UNIDADE_LITO','NÂOCONSOLID','ROCHA','BACIA','BACIA','CAMPO','BACIA','UNIDADE_CRONO']        \n",
    "    for idx in range(0,len(lista_to)):\n",
    "        if lista_from[idx] == ENT_1 and lista_to[idx] == ENT_2:\n",
    "            return relation_type\n",
    "    return 'temporal_relation'\n",
    "\n",
    "def go_through_sentence(sentence_df,df_relation,df_bert,sent_numb):\n",
    "    #percorre a sentenca em busca de relacoes entre entidades anotadas com URIs\n",
    "    df_entity = pd.DataFrame()\n",
    "    from_id, to_id = [], []\n",
    "    relation_from, relation_to = [], []\n",
    "    lista_relacoes_sentence = []\n",
    "    is_to_save = False\n",
    "#     df_bert.to_csv(save_csv_name, encoding='utf-8',index=False)\n",
    "#     df_relation.to_csv('df_relation.csv', encoding='utf-8',index=False)\n",
    "    for idxTokens in range(len(sentence_df)):\n",
    "        token, URI_1 = sentence_df.iloc[idxTokens]['deps'], sentence_df.iloc[idxTokens]['grafo']\n",
    "        for idxTokens2 in range(len(sentence_df)):\n",
    "            if idxTokens != idxTokens2:\n",
    "                token2, URI_2 = sentence_df.iloc[idxTokens2]['deps'], sentence_df.iloc[idxTokens2]['grafo']\n",
    "                has_relation = False\n",
    "                relation_type = go_through_relations(URI_1,URI_2)\n",
    "                if relation_type: \n",
    "                    print(\"-------------\")\n",
    "                    print('sentence =', sent_numb)\n",
    "                    is_to_save = True\n",
    "                    has_relation = True\n",
    "                    Ent1, Ent2 = token.replace(\"B=\",\"\"), token2.replace(\"B=\",\"\")\n",
    "                    relation_type = verifica_pares_entidade_interesse(Ent1,Ent2,relation_type)\n",
    "                    lista_relacoes_sentence.append(relation_type)\n",
    "\n",
    "                    #criar df_bert para BERT RE com codigo do Fabio\n",
    "                    df_bert = create_bert_dataframe(df_bert,idxTokens,idxTokens2,sentence_df,\n",
    "                                                    URI_1,URI_2,\n",
    "                                                    has_relation,relation_type,originalSentenceNumber)\n",
    "\n",
    "                    #para contabilizar os pares de entidade por relacao\n",
    "                    df_relation = create_relations_dataframe(df_relation,token,token2,\n",
    "                                                             URI_1,URI_2,relation_type,originalSentenceNumber)\n",
    "                    #listas para contabilizar relacoes, uris e classes\n",
    "                    lista_relacoes.append(relation_type)\n",
    "                    lista_uris.append(URI_1)\n",
    "                    lista_uris.append(URI_2)         \n",
    "                    lista_classes.append(Ent1)\n",
    "                    lista_classes.append(Ent2)\n",
    "\n",
    "                    if is_to_createJsons: #se quiser criar Jsons para LabelStudio\n",
    "                        df_entity = create_df_JsonFiles(df_entity,relation_type,token,token2,URI_1,URI_2,\n",
    "                                                        idxTokens,idxTokens2,from_id,to_id,sentence_df)\n",
    "                        from_id.append(idxTokens)\n",
    "                        to_id.append(idxTokens2) \n",
    "\n",
    "                else: #nao achou relacao\n",
    "                    relation_type = 'no_relation'\n",
    "\n",
    "                    df_bert = create_bert_dataframe(df_bert,idxTokens,idxTokens2,sentence_df,\n",
    "                                                    URI_1,URI_2,has_relation,relation_type,originalSentenceNumber)\n",
    "                        \n",
    "    return lista_relacoes,lista_uris,lista_classes,\\\n",
    "            df_bert, df_relation, df_entity, \\\n",
    "            lista_relacoes_sentence, from_id, to_id, is_to_save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ler arquivo csv (ou pkl) com as sentenças pós filtragem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero total de sentenças pos-filtragem ->  4542\n"
     ]
    }
   ],
   "source": [
    "df_filtred_sentences = pickle.load(open('df_filtred_petroner_uri_2023_04_05.conllu.pkl', 'rb'))\n",
    "df_filtred_sentences = pd.read_csv('df_filtred_petroner_uri_2023_04_05_conllu.csv')\n",
    "df_group = df_filtred_sentences.groupby('sentence')\n",
    "print('Numero total de sentenças pos-filtragem -> ',len(df_group))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Escolher se deseja criar Jsons para labelstudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_to_createJsons = True\n",
    "# is_to_createJsons = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Folder outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_folder_path = \"./JSONs_04_05\" #local onde são salvos os Jsons para labelstudio\n",
    "save_csv_name = 'df_bert_sentences_15_05.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rotina para processar as sentenças já filtradas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------\n",
      "sentence = 6\n",
      "Token 1 =   Membro Mucuri --- Class 1 =  UNIDADE_LITO --- URI 1 =  #membro_010\n",
      "Token 2 =   Bacia Espirito Santo --- Class 2 =  BACIA --- URI 2 =  #BASE_CD_BACIA_270\n",
      "Relation Type =  located_in\n",
      "[E1] Membro Mucuri [/E1], Eocretáceo    [E2] Bacia    Espirito Santo [/E2]..\n",
      "-------------\n",
      "Saved Json -> True\n",
      "-------------\n",
      "-------------\n",
      "sentence = 97\n",
      "Token 1 =   Formação Pendência --- Class 1 =  UNIDADE_LITO --- URI 1 =  #formacao_319\n",
      "Token 2 =   Bacia Potiguar --- Class 2 =  BACIA --- URI 2 =  #BASE_CD_BACIA_100\n",
      "Relation Type =  located_in\n",
      "2.2 - Estratigrafia de Sequências    [E1] Formação Pendência [/E1], [E2] Bacia Potiguar [/E2]  Della Favera et al. (1992) analisam a seção sin-rift    Bacia Potiguar,    qual identificaram quatro sequências balizadas por discordâncias e suas concordâncias relativas dentro    Formação Pendência..\n",
      "-------------\n",
      "-------------\n",
      "sentence = 97\n",
      "Token 1 =   Formação Pendência --- Class 1 =  UNIDADE_LITO --- URI 1 =  #formacao_319\n",
      "Token 2 =   Bacia Potiguar --- Class 2 =  BACIA --- URI 2 =  #BASE_CD_BACIA_100\n",
      "Relation Type =  located_in\n",
      "2.2 - Estratigrafia de Sequências    [E1] Formação Pendência [/E1], Bacia Potiguar  Della Favera et al. (1992) analisam a seção sin-rift    [E2] Bacia Potiguar [/E2],    qual identificaram quatro sequências balizadas por discordâncias e suas concordâncias relativas dentro    Formação Pendência..\n",
      "-------------\n",
      "-------------\n",
      "sentence = 97\n",
      "Token 1 =   Formação Pendência --- Class 1 =  UNIDADE_LITO --- URI 1 =  #formacao_319\n",
      "Token 2 =   Bacia Potiguar --- Class 2 =  BACIA --- URI 2 =  #BASE_CD_BACIA_100\n",
      "Relation Type =  located_in\n",
      "2.2 - Estratigrafia de Sequências    Formação Pendência, [E2] Bacia Potiguar [/E2]  Della Favera et al. (1992) analisam a seção sin-rift    Bacia Potiguar,    qual identificaram quatro sequências balizadas por discordâncias e suas concordâncias relativas dentro    [E1] Formação Pendência [/E1]..\n",
      "-------------\n",
      "-------------\n",
      "sentence = 97\n",
      "Token 1 =   Formação Pendência --- Class 1 =  UNIDADE_LITO --- URI 1 =  #formacao_319\n",
      "Token 2 =   Bacia Potiguar --- Class 2 =  BACIA --- URI 2 =  #BASE_CD_BACIA_100\n",
      "Relation Type =  located_in\n",
      "2.2 - Estratigrafia de Sequências    Formação Pendência, Bacia Potiguar  Della Favera et al. (1992) analisam a seção sin-rift    [E2] Bacia Potiguar [/E2],    qual identificaram quatro sequências balizadas por discordâncias e suas concordâncias relativas dentro    [E1] Formação Pendência [/E1]..\n",
      "-------------\n",
      "Saved Json -> True\n",
      "-------------\n",
      "-------------\n",
      "sentence = 180\n",
      "Token 1 =   Formação Pendência --- Class 1 =  UNIDADE_LITO --- URI 1 =  #formacao_319\n",
      "Token 2 =   Bacia Potiguar --- Class 2 =  BACIA --- URI 2 =  #BASE_CD_BACIA_100\n",
      "Relation Type =  located_in\n",
      "Della Fávera ef al. (1992), entretanto,    tentarem utilizar essa mesma terminologia para a análise    [E1] Formação Pendência [/E1] ([E2] Bacia Potiguar [/E2]), acabaram traduzindo esses termos de modo diferente..\n",
      "-------------\n",
      "Saved Json -> True\n",
      "-------------\n",
      "-------------\n",
      "sentence = 234\n",
      "Token 1 =   Formação Pendéncia --- Class 1 =  UNIDADE_LITO --- URI 1 =  #formacao_319\n",
      "Token 2 =   Bacia Potiguar --- Class 2 =  BACIA --- URI 2 =  #BASE_CD_BACIA_100\n",
      "Relation Type =  located_in\n",
      "DELLA FÁVERA, J. C., ROSSETI, E. L., GUZZO, J. MATSUDA, N., SOARES, V. M., HASHIMOTO, A. T. ALVES, D. B., CASTRO, J. C., AZAMBUJA, N. C. RODRIGUES, R. Estratigrafa de seqliéncias de [E1] Formação Pendéncia [/E1], [E2] Bacia Potiguar [/E2]..\n",
      "-------------\n",
      "Saved Json -> True\n",
      "-------------\n",
      "-------------\n",
      "sentence = 351\n",
      "Token 1 =   poço 7-BRG-12-SE --- Class 1 =  POÇO --- URI 1 =  #POCO_CD_POCO_007553\n",
      "Token 2 =   Bacia de Sergipe/Alagoas --- Class 2 =  BACIA --- URI 2 =  #BASE_CD_BACIA_116\n",
      "Relation Type =  located_in\n",
      "Observações:  1.a biozona foi originalmente definida em depósitos    [E2] Bacia de Sergipe/Alagoas [/E2] (testemunhos    [E1] poço 7-BRG-12-SE [/E1]), por Beurlen et al (1987)..\n",
      "-------------\n",
      "Saved Json -> True\n",
      "-------------\n",
      "-------------\n",
      "sentence = 364\n",
      "Token 1 =   Albiano --- Class 1 =  UNIDADE_CRONO --- URI 1 =  #Albian\n",
      "Token 2 =   Aptiano --- Class 2 =  UNIDADE_CRONO --- URI 2 =  #Aptian\n",
      "Relation Type =  temporal_relation\n",
      "Cronoestratigrafia: [E1] Albiano [/E1], podendo, entretanto, englobar parte    [E2] Aptiano [/E2]..\n",
      "-------------\n",
      "-------------\n",
      "sentence = 364\n",
      "Token 1 =   Aptiano --- Class 1 =  UNIDADE_CRONO --- URI 1 =  #Aptian\n",
      "Token 2 =   Albiano --- Class 2 =  UNIDADE_CRONO --- URI 2 =  #Albian\n",
      "Relation Type =  temporal_relation\n",
      "Cronoestratigrafia: [E2] Albiano [/E2], podendo, entretanto, englobar parte    [E1] Aptiano [/E1]..\n",
      "-------------\n",
      "Saved Json -> True\n",
      "-------------\n",
      "-------------\n",
      "sentence = 380\n",
      "Token 1 =   poço 7-BRG-12-SE --- Class 1 =  POÇO --- URI 1 =  #POCO_CD_POCO_007553\n",
      "Token 2 =   Bacia de Sergipe/Alagoas --- Class 2 =  BACIA --- URI 2 =  #BASE_CD_BACIA_116\n",
      "Relation Type =  located_in\n",
      "entanto, Trôelsen e Quadros (1971) já haviam fotografado - alguns de seus - exemplares ([E2] Bacia de Sergipe/Alagoas [/E2], [E1] poço 7-BRG-12-SE [/E1])..\n",
      "-------------\n",
      "Saved Json -> True\n",
      "-------------\n",
      "-------------\n",
      "sentence = 382\n",
      "Token 1 =   Aptiano --- Class 1 =  UNIDADE_CRONO --- URI 1 =  #Aptian\n",
      "Token 2 =   Albiano --- Class 2 =  UNIDADE_CRONO --- URI 2 =  #Albian\n",
      "Relation Type =  temporal_relation\n",
      "Em termos ¢ronoestratigraficos, entretanto, Freitas et al. (op. cit.), preferiram não firmar posição quanto    andar em que as tais associações estavam inseridas,              a afirmar que as espécies reconhecidas não eram suficientes para distinguir o [E1] Aptiano [/E1]    [E2] Albiano [/E2]..\n",
      "-------------\n",
      "-------------\n",
      "sentence = 382\n",
      "Token 1 =   Albiano --- Class 1 =  UNIDADE_CRONO --- URI 1 =  #Albian\n",
      "Token 2 =   Aptiano --- Class 2 =  UNIDADE_CRONO --- URI 2 =  #Aptian\n",
      "Relation Type =  temporal_relation\n",
      "Em termos ¢ronoestratigraficos, entretanto, Freitas et al. (op. cit.), preferiram não firmar posição quanto    andar em que as tais associações estavam inseridas,              a afirmar que as espécies reconhecidas não eram suficientes para distinguir o [E2] Aptiano [/E2]    [E1] Albiano [/E1]..\n",
      "-------------\n",
      "Saved Json -> True\n",
      "-------------\n",
      "-------------\n",
      "sentence = 395\n",
      "Token 1 =   andares Aptiano --- Class 1 =  UNIDADE_CRONO --- URI 1 =  #Aptian\n",
      "Token 2 =   Albiano --- Class 2 =  UNIDADE_CRONO --- URI 2 =  #Albian\n",
      "Relation Type =  temporal_relation\n",
      "Em face     problemas mencionados,             que a delimitação entre os [E1] andares Aptiano [/E1] e [E2] Albiano [/E2], com base em exemplares    gênero Nannoconus, seja uma tarefa difícil..\n",
      "-------------\n",
      "-------------\n",
      "sentence = 395\n",
      "Token 1 =   Albiano --- Class 1 =  UNIDADE_CRONO --- URI 1 =  #Albian\n",
      "Token 2 =   andares Aptiano --- Class 2 =  UNIDADE_CRONO --- URI 2 =  #Aptian\n",
      "Relation Type =  temporal_relation\n",
      "Em face     problemas mencionados,             que a delimitação entre os [E2] andares Aptiano [/E2] e [E1] Albiano [/E1], com base em exemplares    gênero Nannoconus, seja uma tarefa difícil..\n",
      "-------------\n",
      "Saved Json -> True\n",
      "-------------\n",
      "-------------\n",
      "sentence = 413\n",
      "Token 1 =   intra-albiano --- Class 1 =  UNIDADE_CRONO --- URI 1 =  #Albian\n",
      "Token 2 =   aptianos --- Class 2 =  UNIDADE_CRONO --- URI 2 =  #Aptian\n",
      "Relation Type =  temporal_relation\n",
      "Esta constatação induz   formulação de algumas hipóteses para explicar o não-registro       última biozona     bacias mencionadas: a) a existência de amplo hiato [E1] intra-albiano [/E1], correspondente    tempo compreendido      biozona Nannoconus fragilis; b) as condicionantes ambientais    água    mar necessárias para a proliferação e sedimentação (após a morte    organismo) de Nannoconus fragilis não teriam sido atingidas; c) a recristalização sofrida       carbonatos durante a diagénese teria destruído ou dificultaria muito a liberação     nanofósseis    matriz    rocha, impedindo, assim, o reconhecimento    biozona em análises rotineiras; “e d) eventualmente, já em tempos [E2] aptianos [/E2] (?), a margem equatorial receberia influxos de águas oriundas    oceano situado a norte    primitivo Atlântico Sul. b) as condicionantes ambientais    água    mar necessárias para a proliferação e sedimentação (após a morte    organismo) de Nannoconus fragilis não teriam sido atingidas; c) a recristalização sofrida       carbonatos durante a diagénese teria destruído ou dificultaria muito a liberação     nanofósseis    matriz    rocha, impedindo, assim, o reconhecimento    biozona em análises rotineiras; mo d) eventualmente, já em tempos aptianos (?), a margem equatorial receberia influxos de águas oriundas    oceano situado a norte    primitivo Atlântico Sul..\n",
      "-------------\n",
      "-------------\n",
      "sentence = 413\n",
      "Token 1 =   intra-albiano --- Class 1 =  UNIDADE_CRONO --- URI 1 =  #Albian\n",
      "Token 2 =   aptianos --- Class 2 =  UNIDADE_CRONO --- URI 2 =  #Aptian\n",
      "Relation Type =  temporal_relation\n",
      "Esta constatação induz   formulação de algumas hipóteses para explicar o não-registro       última biozona     bacias mencionadas: a) a existência de amplo hiato [E1] intra-albiano [/E1], correspondente    tempo compreendido      biozona Nannoconus fragilis; b) as condicionantes ambientais    água    mar necessárias para a proliferação e sedimentação (após a morte    organismo) de Nannoconus fragilis não teriam sido atingidas; c) a recristalização sofrida       carbonatos durante a diagénese teria destruído ou dificultaria muito a liberação     nanofósseis    matriz    rocha, impedindo, assim, o reconhecimento    biozona em análises rotineiras; “e d) eventualmente, já em tempos aptianos (?), a margem equatorial receberia influxos de águas oriundas    oceano situado a norte    primitivo Atlântico Sul. b) as condicionantes ambientais    água    mar necessárias para a proliferação e sedimentação (após a morte    organismo) de Nannoconus fragilis não teriam sido atingidas; c) a recristalização sofrida       carbonatos durante a diagénese teria destruído ou dificultaria muito a liberação     nanofósseis    matriz    rocha, impedindo, assim, o reconhecimento    biozona em análises rotineiras; mo d) eventualmente, já em tempos [E2] aptianos [/E2] (?), a margem equatorial receberia influxos de águas oriundas    oceano situado a norte    primitivo Atlântico Sul..\n",
      "-------------\n",
      "-------------\n",
      "sentence = 413\n",
      "Token 1 =   aptianos --- Class 1 =  UNIDADE_CRONO --- URI 1 =  #Aptian\n",
      "Token 2 =   intra-albiano --- Class 2 =  UNIDADE_CRONO --- URI 2 =  #Albian\n",
      "Relation Type =  temporal_relation\n",
      "Esta constatação induz   formulação de algumas hipóteses para explicar o não-registro       última biozona     bacias mencionadas: a) a existência de amplo hiato [E2] intra-albiano [/E2], correspondente    tempo compreendido      biozona Nannoconus fragilis; b) as condicionantes ambientais    água    mar necessárias para a proliferação e sedimentação (após a morte    organismo) de Nannoconus fragilis não teriam sido atingidas; c) a recristalização sofrida       carbonatos durante a diagénese teria destruído ou dificultaria muito a liberação     nanofósseis    matriz    rocha, impedindo, assim, o reconhecimento    biozona em análises rotineiras; “e d) eventualmente, já em tempos [E1] aptianos [/E1] (?), a margem equatorial receberia influxos de águas oriundas    oceano situado a norte    primitivo Atlântico Sul. b) as condicionantes ambientais    água    mar necessárias para a proliferação e sedimentação (após a morte    organismo) de Nannoconus fragilis não teriam sido atingidas; c) a recristalização sofrida       carbonatos durante a diagénese teria destruído ou dificultaria muito a liberação     nanofósseis    matriz    rocha, impedindo, assim, o reconhecimento    biozona em análises rotineiras; mo d) eventualmente, já em tempos aptianos (?), a margem equatorial receberia influxos de águas oriundas    oceano situado a norte    primitivo Atlântico Sul..\n",
      "-------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "* Owlready2 * Warning: ignoring cyclic type of, involving storid 370\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------\n",
      "sentence = 413\n",
      "Token 1 =   aptianos --- Class 1 =  UNIDADE_CRONO --- URI 1 =  #Aptian\n",
      "Token 2 =   aptianos --- Class 2 =  UNIDADE_CRONO --- URI 2 =  #Aptian\n",
      "Relation Type =  temporal_relation\n",
      "Esta constatação induz   formulação de algumas hipóteses para explicar o não-registro       última biozona     bacias mencionadas: a) a existência de amplo hiato intra-albiano, correspondente    tempo compreendido      biozona Nannoconus fragilis; b) as condicionantes ambientais    água    mar necessárias para a proliferação e sedimentação (após a morte    organismo) de Nannoconus fragilis não teriam sido atingidas; c) a recristalização sofrida       carbonatos durante a diagénese teria destruído ou dificultaria muito a liberação     nanofósseis    matriz    rocha, impedindo, assim, o reconhecimento    biozona em análises rotineiras; “e d) eventualmente, já em tempos [E1] aptianos [/E1] (?), a margem equatorial receberia influxos de águas oriundas    oceano situado a norte    primitivo Atlântico Sul. b) as condicionantes ambientais    água    mar necessárias para a proliferação e sedimentação (após a morte    organismo) de Nannoconus fragilis não teriam sido atingidas; c) a recristalização sofrida       carbonatos durante a diagénese teria destruído ou dificultaria muito a liberação     nanofósseis    matriz    rocha, impedindo, assim, o reconhecimento    biozona em análises rotineiras; mo d) eventualmente, já em tempos [E2] aptianos [/E2] (?), a margem equatorial receberia influxos de águas oriundas    oceano situado a norte    primitivo Atlântico Sul..\n",
      "-------------\n",
      "-------------\n",
      "sentence = 413\n",
      "Token 1 =   aptianos --- Class 1 =  UNIDADE_CRONO --- URI 1 =  #Aptian\n",
      "Token 2 =   intra-albiano --- Class 2 =  UNIDADE_CRONO --- URI 2 =  #Albian\n",
      "Relation Type =  temporal_relation\n",
      "Esta constatação induz   formulação de algumas hipóteses para explicar o não-registro       última biozona     bacias mencionadas: a) a existência de amplo hiato [E2] intra-albiano [/E2], correspondente    tempo compreendido      biozona Nannoconus fragilis; b) as condicionantes ambientais    água    mar necessárias para a proliferação e sedimentação (após a morte    organismo) de Nannoconus fragilis não teriam sido atingidas; c) a recristalização sofrida       carbonatos durante a diagénese teria destruído ou dificultaria muito a liberação     nanofósseis    matriz    rocha, impedindo, assim, o reconhecimento    biozona em análises rotineiras; “e d) eventualmente, já em tempos aptianos (?), a margem equatorial receberia influxos de águas oriundas    oceano situado a norte    primitivo Atlântico Sul. b) as condicionantes ambientais    água    mar necessárias para a proliferação e sedimentação (após a morte    organismo) de Nannoconus fragilis não teriam sido atingidas; c) a recristalização sofrida       carbonatos durante a diagénese teria destruído ou dificultaria muito a liberação     nanofósseis    matriz    rocha, impedindo, assim, o reconhecimento    biozona em análises rotineiras; mo d) eventualmente, já em tempos [E1] aptianos [/E1] (?), a margem equatorial receberia influxos de águas oriundas    oceano situado a norte    primitivo Atlântico Sul..\n",
      "-------------\n",
      "-------------\n",
      "sentence = 413\n",
      "Token 1 =   aptianos --- Class 1 =  UNIDADE_CRONO --- URI 1 =  #Aptian\n",
      "Token 2 =   aptianos --- Class 2 =  UNIDADE_CRONO --- URI 2 =  #Aptian\n",
      "Relation Type =  temporal_relation\n",
      "Esta constatação induz   formulação de algumas hipóteses para explicar o não-registro       última biozona     bacias mencionadas: a) a existência de amplo hiato intra-albiano, correspondente    tempo compreendido      biozona Nannoconus fragilis; b) as condicionantes ambientais    água    mar necessárias para a proliferação e sedimentação (após a morte    organismo) de Nannoconus fragilis não teriam sido atingidas; c) a recristalização sofrida       carbonatos durante a diagénese teria destruído ou dificultaria muito a liberação     nanofósseis    matriz    rocha, impedindo, assim, o reconhecimento    biozona em análises rotineiras; “e d) eventualmente, já em tempos [E2] aptianos [/E2] (?), a margem equatorial receberia influxos de águas oriundas    oceano situado a norte    primitivo Atlântico Sul. b) as condicionantes ambientais    água    mar necessárias para a proliferação e sedimentação (após a morte    organismo) de Nannoconus fragilis não teriam sido atingidas; c) a recristalização sofrida       carbonatos durante a diagénese teria destruído ou dificultaria muito a liberação     nanofósseis    matriz    rocha, impedindo, assim, o reconhecimento    biozona em análises rotineiras; mo d) eventualmente, já em tempos [E1] aptianos [/E1] (?), a margem equatorial receberia influxos de águas oriundas    oceano situado a norte    primitivo Atlântico Sul..\n",
      "-------------\n",
      "Saved Json -> True\n",
      "-------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "* Owlready2 * Warning: ignoring cyclic type of, involving storid 367\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------\n",
      "sentence = 415\n",
      "Token 1 =   Albiano --- Class 1 =  UNIDADE_CRONO --- URI 1 =  #Albian\n",
      "Token 2 =   intra-albiano --- Class 2 =  UNIDADE_CRONO --- URI 2 =  #Albian\n",
      "Relation Type =  temporal_relation\n",
      "De acordo com os arcabouços bioestratigráficos elaborados    partir de outros grupos fósseis (palinomorfos, principalmente), o [E1] Albiano [/E1],    margem sudeste, é dividido em duas ou mais biozonas e as investigações não sugerem a existência de qualquer hiato [E2] intra-albiano [/E2] de grande expressão geográfica (Azevedo et al. 1987a; Viviers et al. 1986; Oliveira et al. 1993)..\n",
      "-------------\n",
      "-------------\n",
      "sentence = 415\n",
      "Token 1 =   intra-albiano --- Class 1 =  UNIDADE_CRONO --- URI 1 =  #Albian\n",
      "Token 2 =   Albiano --- Class 2 =  UNIDADE_CRONO --- URI 2 =  #Albian\n",
      "Relation Type =  temporal_relation\n",
      "De acordo com os arcabouços bioestratigráficos elaborados    partir de outros grupos fósseis (palinomorfos, principalmente), o [E2] Albiano [/E2],    margem sudeste, é dividido em duas ou mais biozonas e as investigações não sugerem a existência de qualquer hiato [E1] intra-albiano [/E1] de grande expressão geográfica (Azevedo et al. 1987a; Viviers et al. 1986; Oliveira et al. 1993)..\n",
      "-------------\n",
      "Saved Json -> True\n",
      "-------------\n",
      "-------------\n",
      "sentence = 422\n",
      "Token 1 =   Formação Macaé --- Class 1 =  UNIDADE_LITO --- URI 1 =  #grupo_000\n",
      "Token 2 =   Campos --- Class 2 =  BACIA --- URI 2 =  #BASE_CD_BACIA_281\n",
      "Relation Type =  located_in\n",
      "É sabido; por exemplo, que a porção superior    [E1] Formação Macaé [/E1] (Albiano/Turoniano    Bacia de ([E2] Campos [/E2]) encerra níveis ricos em nanofósseis, observáveis somente com o auxílio    microscópio eletrônico de varredura (MEV, Spadini et al. 1988)..\n",
      "-------------\n",
      "Saved Json -> True\n",
      "-------------\n",
      "-------------\n",
      "sentence = 457\n",
      "Token 1 =   poço 1-CES-75 --- Class 1 =  POÇO --- URI 1 =  #POCO_CD_POCO_012996\n",
      "Token 2 =   Bacia Ceará --- Class 2 =  BACIA --- URI 2 =  #BASE_CD_BACIA_096\n",
      "Relation Type =  located_in\n",
      "[E2] Bacia    Ceará [/E2], Cunha (1990a) sugere que a espécie Nannoconus truitti truitti teria sido extinta    Turoniano ([E1] poço 1-CES-75 [/E1]), o que também pode ser verificado    perfuração 1-US-1-SE    Bacia de Sergipe/Alagoas (Cunha, informação verbal)..\n",
      "-------------\n",
      "-------------\n",
      "sentence = 457\n",
      "Token 1 =   1-US-1-SE --- Class 1 =  POÇO --- URI 1 =  #POCO_CD_POCO_010471\n",
      "Token 2 =   Bacia de Sergipe --- Class 2 =  BACIA --- URI 2 =  #BASE_CD_BACIA_116\n",
      "Relation Type =  located_in\n",
      "Bacia    Ceará, Cunha (1990a) sugere que a espécie Nannoconus truitti truitti teria sido extinta    Turoniano (poço 1-CES-75), o que também pode ser verificado    perfuração [E1] 1-US-1-SE [/E1]    [E2] Bacia de Sergipe [/E2]/Alagoas (Cunha, informação verbal)..\n",
      "-------------\n",
      "-------------\n",
      "sentence = 457\n",
      "Token 1 =   1-US-1-SE --- Class 1 =  POÇO --- URI 1 =  #POCO_CD_POCO_010471\n",
      "Token 2 =   Alagoas --- Class 2 =  BACIA --- URI 2 =  #BASE_CD_BACIA_116\n",
      "Relation Type =  located_in\n",
      "Bacia    Ceará, Cunha (1990a) sugere que a espécie Nannoconus truitti truitti teria sido extinta    Turoniano (poço 1-CES-75), o que também pode ser verificado    perfuração [E1] 1-US-1-SE [/E1]    Bacia de Sergipe/[E2] Alagoas [/E2] (Cunha, informação verbal)..\n",
      "-------------\n",
      "Saved Json -> True\n",
      "-------------\n",
      "-------------\n",
      "sentence = 466\n",
      "Token 1 =   poço 1-CES-75 --- Class 1 =  POÇO --- URI 1 =  #POCO_CD_POCO_012996\n",
      "Token 2 =   Bacia Ceará --- Class 2 =  BACIA --- URI 2 =  #BASE_CD_BACIA_096\n",
      "Relation Type =  located_in\n",
      "[E2] Bacia    Ceará [/E2], Cunha (1990a) sugere que a espécie Nannoconus truitti truitti teria sido extinta    Turoniano ([E1] poço 1-CES-75 [/E1]), o que também pode ser verificado    perfuração 1-US-1-SE    Bacia de Sergipe/Alagoas (Cunha, informação verbal)..\n",
      "-------------\n",
      "-------------\n",
      "sentence = 466\n",
      "Token 1 =   1-US-1-SE --- Class 1 =  POÇO --- URI 1 =  #POCO_CD_POCO_010471\n",
      "Token 2 =   Bacia de Sergipe --- Class 2 =  BACIA --- URI 2 =  #BASE_CD_BACIA_116\n",
      "Relation Type =  located_in\n",
      "Bacia    Ceará, Cunha (1990a) sugere que a espécie Nannoconus truitti truitti teria sido extinta    Turoniano (poço 1-CES-75), o que também pode ser verificado    perfuração [E1] 1-US-1-SE [/E1]    [E2] Bacia de Sergipe [/E2]/Alagoas (Cunha, informação verbal)..\n",
      "-------------\n",
      "-------------\n",
      "sentence = 466\n",
      "Token 1 =   1-US-1-SE --- Class 1 =  POÇO --- URI 1 =  #POCO_CD_POCO_010471\n",
      "Token 2 =   Alagoas --- Class 2 =  BACIA --- URI 2 =  #BASE_CD_BACIA_116\n",
      "Relation Type =  located_in\n",
      "Bacia    Ceará, Cunha (1990a) sugere que a espécie Nannoconus truitti truitti teria sido extinta    Turoniano (poço 1-CES-75), o que também pode ser verificado    perfuração [E1] 1-US-1-SE [/E1]    Bacia de Sergipe/[E2] Alagoas [/E2] (Cunha, informação verbal)..\n",
      "-------------\n",
      "Saved Json -> True\n",
      "-------------\n",
      "-------------\n",
      "sentence = 471\n",
      "Token 1 =   cenomanianos --- Class 1 =  UNIDADE_CRONO --- URI 1 =  #Cenomanian\n",
      "Token 2 =   turonianos --- Class 2 =  UNIDADE_CRONO --- URI 2 =  #Turonian\n",
      "Relation Type =  temporal_relation\n",
      "Iniciada    parte sul    costa brasileira, próximo    término do-Albiano, esta mudança teria migrado, gradativamente, para norte, atingindo as bacias de Sergipe/Alagoas, Potiguar e    Ceará somente em tempos [E1] cenomanianos [/E1]/[E2] turonianos [/E2]..\n",
      "-------------\n",
      "-------------\n",
      "sentence = 471\n",
      "Token 1 =   turonianos --- Class 1 =  UNIDADE_CRONO --- URI 1 =  #Turonian\n",
      "Token 2 =   cenomanianos --- Class 2 =  UNIDADE_CRONO --- URI 2 =  #Cenomanian\n",
      "Relation Type =  temporal_relation\n",
      "Iniciada    parte sul    costa brasileira, próximo    término do-Albiano, esta mudança teria migrado, gradativamente, para norte, atingindo as bacias de Sergipe/Alagoas, Potiguar e    Ceará somente em tempos [E2] cenomanianos [/E2]/[E1] turonianos [/E1]..\n",
      "-------------\n",
      "Saved Json -> True\n",
      "-------------\n",
      "-------------\n",
      "sentence = 480\n",
      "Token 1 =   Formação Itajaí --- Class 1 =  UNIDADE_LITO --- URI 1 =  #formacao_163\n",
      "Token 2 =   folhelhos --- Class 2 =  ROCHA --- URI 2 =  #shale\n",
      "Relation Type =  constituted_by\n",
      "sítio sedimentar, raros exemplares de Nannoconus truitti têm sido observados     estratos basais    [E1] Formação Itajaí [/E1] ([E2] folhelhos [/E2] e margas) que jazem imediatamente acima    Formação Guarujá (carbonatos)..\n",
      "-------------\n",
      "-------------\n",
      "sentence = 480\n",
      "Token 1 =   Formação Itajaí --- Class 1 =  UNIDADE_LITO --- URI 1 =  #formacao_163\n",
      "Token 2 =   margas --- Class 2 =  ROCHA --- URI 2 =  #marlstone\n",
      "Relation Type =  constituted_by\n",
      "sítio sedimentar, raros exemplares de Nannoconus truitti têm sido observados     estratos basais    [E1] Formação Itajaí [/E1] (folhelhos e [E2] margas [/E2]) que jazem imediatamente acima    Formação Guarujá (carbonatos)..\n",
      "-------------\n",
      "-------------\n",
      "sentence = 480\n",
      "Token 1 =   Formação Guarujá --- Class 1 =  UNIDADE_LITO --- URI 1 =  #formacao_166\n",
      "Token 2 =   folhelhos --- Class 2 =  ROCHA --- URI 2 =  #shale\n",
      "Relation Type =  constituted_by\n",
      "sítio sedimentar, raros exemplares de Nannoconus truitti têm sido observados     estratos basais    Formação Itajaí ([E2] folhelhos [/E2] e margas) que jazem imediatamente acima    [E1] Formação Guarujá [/E1] (carbonatos)..\n",
      "-------------\n",
      "-------------\n",
      "sentence = 480\n",
      "Token 1 =   Formação Guarujá --- Class 1 =  UNIDADE_LITO --- URI 1 =  #formacao_166\n",
      "Token 2 =   margas --- Class 2 =  ROCHA --- URI 2 =  #marlstone\n",
      "Relation Type =  constituted_by\n",
      "sítio sedimentar, raros exemplares de Nannoconus truitti têm sido observados     estratos basais    Formação Itajaí (folhelhos e [E2] margas [/E2]) que jazem imediatamente acima    [E1] Formação Guarujá [/E1] (carbonatos)..\n",
      "-------------\n",
      "Saved Json -> True\n",
      "-------------\n",
      "-------------\n",
      "sentence = 482\n",
      "Token 1 =   cenomanianos --- Class 1 =  UNIDADE_CRONO --- URI 1 =  #Cenomanian\n",
      "Token 2 =   albianos --- Class 2 =  UNIDADE_CRONO --- URI 2 =  #Albian\n",
      "Relation Type =  temporal_relation\n",
      "Se o desaparecimento       taxon             vinculado a uma mudança climática (obs. 3), que se deu gradativamente de sul para norte, o mesmo não deveria ter os últimos registros     depósitos [E1] cenomanianos [/E1]    Bacia de Santos, e [E2] albianos [/E2]     bacias de Campos e Espírito Santo..\n",
      "-------------\n",
      "-------------\n",
      "sentence = 482\n",
      "Token 1 =   albianos --- Class 1 =  UNIDADE_CRONO --- URI 1 =  #Albian\n",
      "Token 2 =   cenomanianos --- Class 2 =  UNIDADE_CRONO --- URI 2 =  #Cenomanian\n",
      "Relation Type =  temporal_relation\n",
      "Se o desaparecimento       taxon             vinculado a uma mudança climática (obs. 3), que se deu gradativamente de sul para norte, o mesmo não deveria ter os últimos registros     depósitos [E2] cenomanianos [/E2]    Bacia de Santos, e [E1] albianos [/E1]     bacias de Campos e Espírito Santo..\n",
      "-------------\n",
      "Saved Json -> True\n",
      "-------------\n",
      "-------------\n",
      "sentence = 484\n",
      "Token 1 =   andares Albiano --- Class 1 =  UNIDADE_CRONO --- URI 1 =  #Albian\n",
      "Token 2 =   Cenomaniano --- Class 2 =  UNIDADE_CRONO --- URI 2 =  #Cenomanian\n",
      "Relation Type =  temporal_relation\n",
      "foi aferido com relação     outros grupos fósseis — não guardam o registro sedimentar continuo     [E1] andares Albiano [/E1] e [E2] Cenomaniano [/E2]..\n",
      "-------------\n",
      "-------------\n",
      "sentence = 484\n",
      "Token 1 =   Cenomaniano --- Class 1 =  UNIDADE_CRONO --- URI 1 =  #Cenomanian\n",
      "Token 2 =   andares Albiano --- Class 2 =  UNIDADE_CRONO --- URI 2 =  #Albian\n",
      "Relation Type =  temporal_relation\n",
      "foi aferido com relação     outros grupos fósseis — não guardam o registro sedimentar continuo     [E2] andares Albiano [/E2] e [E1] Cenomaniano [/E1]..\n",
      "-------------\n",
      "Saved Json -> True\n",
      "-------------\n",
      "-------------\n",
      "sentence = 489\n",
      "Token 1 =   Albiano --- Class 1 =  UNIDADE_CRONO --- URI 1 =  #Albian\n",
      "Token 2 =   Cenomaniano --- Class 2 =  UNIDADE_CRONO --- URI 2 =  #Cenomanian\n",
      "Relation Type =  temporal_relation\n",
      "Ainda não há argumentos consistentes para firmar uma posição quanto    limite superior    biozona ([E1] Albiano [/E1] ou [E2] Cenomaniano [/E2] - parte inferior)..\n",
      "-------------\n",
      "-------------\n",
      "sentence = 489\n",
      "Token 1 =   Cenomaniano --- Class 1 =  UNIDADE_CRONO --- URI 1 =  #Cenomanian\n",
      "Token 2 =   Albiano --- Class 2 =  UNIDADE_CRONO --- URI 2 =  #Albian\n",
      "Relation Type =  temporal_relation\n",
      "Ainda não há argumentos consistentes para firmar uma posição quanto    limite superior    biozona ([E2] Albiano [/E2] ou [E1] Cenomaniano [/E1] - parte inferior)..\n",
      "-------------\n",
      "Saved Json -> True\n",
      "-------------\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "numberSentences = df_filtred_sentences.iloc[-1]['sentence'] #numero de sentencas diferentes no arquivo ja filtrado\n",
    "lista_relacoes, lista_uris, lista_classes, list_sentences_dict = [], [], [], []\n",
    "df_relation, df_bert = pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "countJsons=0\n",
    "for idx in range(1,len(df_group)):\n",
    "    filtred_sentence = df_group.get_group(idx)#aqui filtred_sentence é um dataframe da sentenca\n",
    "    originalSentenceNumber = filtred_sentence.iloc[0]['#sentence_original']\n",
    "    text = filtred_sentence.iloc[0]['text']\n",
    "    lista_relacoes,lista_uris,lista_classes,df_bert,\\\n",
    "    df_relation,df_entity,lista_relacoes_sentence,\\\n",
    "    from_id,to_id, is_to_save = go_through_sentence(filtred_sentence,df_relation,df_bert,originalSentenceNumber) \n",
    "    df_bert.to_csv(save_csv_name, encoding='utf-8',index=False)\n",
    "    df_relation.to_csv('df_relation.csv', encoding='utf-8',index=False)\n",
    "    if is_to_save and is_to_createJsons:#Jsons somente criados para sentencas com relacao\n",
    "        countJsons+=1\n",
    "        saveJsonFiles(df_entity,from_id,to_id, \n",
    "                      lista_relacoes_sentence,filtred_sentence,originalSentenceNumber,save_folder_path)\n",
    "#     raise SystemExit(\"Stop right there!\")    \n",
    "print(\"-------------\")\n",
    "print(\"Number of Jsons saved = \", countJsons)\n",
    "\n",
    "# pickle.dump(df_relation, open('df_relation.pkl','wb'), protocol=pickle.HIGHEST_PROTOCOL)\n",
    "df_relation.to_csv('df_relation.csv', encoding='utf-8',index=False)\n",
    "df_bert.to_csv(save_csv_name, encoding='utf-8',index=False)\n",
    "\n",
    "relacoes, numb_rel = np.unique(lista_relacoes, return_counts = True)\n",
    "pickle.dump(lista_relacoes,open('lista_relacoes.pkl','wb'),protocol = pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "uris, numb_uris = np.unique(lista_uris, return_counts = True)\n",
    "pickle.dump(lista_uris,open('lista_uris.pkl','wb'),protocol = pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "classes, numb_classes = np.unique(lista_classes, return_counts = True)\n",
    "pickle.dump(lista_classes,open('lista_classes.pkl','wb'),protocol = pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contabilização das relações encontradas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relacoes.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numb_rel.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contabilização das classes encontradas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numb_classes.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contabilização das URIs encontradas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "uris.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "numb_uris.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verificação de pares de entidades por tipo de relação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_filtred = pickle.load(open('df_relation.pkl', 'rb'))\n",
    "df_relations = pd.read_csv('df_relation.csv')\n",
    "df_grp = df_relations.groupby('Relation')\n",
    "relations_groups = df_grp.groups\n",
    "relations = list(relations_groups)\n",
    "lista_pares = []\n",
    "for relation in relations:\n",
    "    df_rel = df_grp.get_group(relation)\n",
    "    list_rel = []\n",
    "    for idx_rel in range(0,len(df_rel)):\n",
    "        par = df_rel.iloc[idx_rel]['Ent1'] + ' + ' + df_rel.iloc[idx_rel]['Ent2']\n",
    "        list_rel.append(par)\n",
    "    lista_pares.append(list_rel)\n",
    "print('Number of types of relations ->', len(lista_pares))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avaliar idx_pair de 0 ao tamanho apresentado acima para verificar os pares de entidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "pares, numb_pares = np.unique(lista_pares[idx], return_counts = True)\n",
    "print('Relation -> ',relations[idx])\n",
    "print('Entities pair -> ',pares.tolist())\n",
    "print('Number of ocorrences -> ',numb_pares.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
