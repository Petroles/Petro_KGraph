{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook para filtrar o arquivo conllu em sentenças que contenham pelo menos duas URIS anotadas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "from conllu import parse\n",
    "import pandas as pd\n",
    "from conllu import parse_incr\n",
    "import pickle\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import unicodedata\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funções "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utils\n",
    "def get_start(x, offset):\n",
    "    start = str(int(x[\"start_char\"]) - offset)\n",
    "    return start\n",
    "def get_end(x, offset):\n",
    "    end = str(int(x[\"end_char\"]) - offset)\n",
    "    return end\n",
    "def correct_start_end(df):\n",
    "    offset = int(df.iloc[0][\"misc\"].get(\"start_char\"))\n",
    "    df['start'] = df['misc'].apply(lambda x: get_start(x, offset))\n",
    "    df['end'] = df['misc'].apply(lambda x: get_end(x, offset))\n",
    "    return df\n",
    "\n",
    "def remove_null_tokens(df):#retira tokens nulos na sentenca\n",
    "    df = df[df['misc'].notna()]\n",
    "    df = df.reset_index(drop=True)\n",
    "    df = df[df['deps'].notna()]\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def ents_uris_counter_prefiltering(sentence,ents,uris): #contagem de entidades e uris pre-filtragem\n",
    "    entidades_sentenca_old, uris_sentenca_old = ents, uris\n",
    "    ents_list, uris_list = ents, uris\n",
    "    for idx_row in range(len(sentence)):\n",
    "        token = sentence.iloc[idx_row]['deps']\n",
    "        grafo = sentence.iloc[idx_row]['misc'].get('grafo')\n",
    "        if \"B=\" in token:\n",
    "            ents_list.append(token)\n",
    "        if grafo:\n",
    "            uris_list.append(grafo)     \n",
    "    return ents_list, uris_list\n",
    "\n",
    "def verify_sentence_is_functioning(df_sentence):\n",
    "    if df_sentence.iloc[0][\"misc\"].get('eliminar') == '':#retirar linha com eliminar\n",
    "        print('eliminada')\n",
    "        return 0\n",
    "    for idx_row in range(len(df_sentence)):\n",
    "        if df_sentence.iloc[idx_row]['misc'].get('start_char') == None:#problema com start_char\n",
    "            print('start_char com problema')\n",
    "            return 0\n",
    "    return 1\n",
    "\n",
    "def get_text_sentence(df):\n",
    "    size_sentence = int(df_sentence.iloc[-1][\"end\"])\n",
    "    text = \" \"*size_sentence\n",
    "    for index, row in df_sentence.iterrows():\n",
    "        text = text[:int(row[\"start\"])] + row[\"form\"] +text[int(row[\"end\"]):]\n",
    "    return text \n",
    "\n",
    "def get_word_join(df, index):\n",
    "    entity = df.iloc[index]['deps']\n",
    "    entity_I = entity.replace(\"B=\",\"I=\")\n",
    "    count = 1\n",
    "    word_join = \"\"\n",
    "    row_main = df.iloc[index]\n",
    "    start_word, end_word = row_main['start'], row_main['end'] \n",
    "    word_join = \" \".join([word_join, row_main['form']])\n",
    "    while index+count != len(df) and (df.iloc[index+count][\"deps\"] == entity_I or check_I_entities(df, index+count,entity_I)):\n",
    "        row = df.iloc[index+count]\n",
    "        word_join = \" \".join([word_join, row['form']])\n",
    "        end_word = row['end']\n",
    "        count+=1\n",
    "    return word_join, start_word, end_word\n",
    "\n",
    "def check_I_entities(df, i,entity):\n",
    "    #verifica se ainda está dentro da entidade usando o sistema BIO de tokens\n",
    "    next_entity_is_I = (df.iloc[i][\"deps\"] == entity) or (df.iloc[i][\"deps\"] == None and df.iloc[i+1][\"deps\"] == entity)\n",
    "    return next_entity_is_I\n",
    "\n",
    "def check_sentence_for_pair_uris(df_sentence, ents, uris):\n",
    "    df_sent_new = pd.DataFrame()\n",
    "    ents_list_old, uris_list_old = ents, uris\n",
    "    ents_list, uris_list = ents, uris\n",
    "    \n",
    "    countURIs = 0\n",
    "    for idx_row in range(len(df_sentence)):\n",
    "        token = df_sentence.iloc[idx_row]['deps']\n",
    "        grafo = df_sentence.iloc[idx_row]['misc'].get('grafo')\n",
    "        if 'B=' in token and grafo:\n",
    "            countURIs+=1\n",
    "            ents_list = ents_list + [token]\n",
    "            uris_list = uris_list + [grafo]\n",
    "            \n",
    "            df_sentence.loc[idx_row,'grafo'] = grafo\n",
    "            df_sentence.loc[idx_row,'text'] = get_text_sentence(df_sentence)\n",
    "            word_join, start_word_join, end_word_join = get_word_join(df_sentence, idx_row)\n",
    "            df_sentence.loc[idx_row,'word_join'] = word_join\n",
    "            df_sentence.loc[idx_row,'word_join_start'] = start_word_join\n",
    "            df_sentence.loc[idx_row,'word_join_end'] = end_word_join\n",
    "            df_sentence.loc[idx_row,'index_e'] = int(idx_row)\n",
    "            \n",
    "            df_sent_new = df_sent_new.append(df_sentence.iloc[idx_row],ignore_index = True)\n",
    "#             raise SystemExit(\"Stop right there!\")   \n",
    "    if countURIs > 1:\n",
    "        return 1, df_sent_new, ents_list, uris_list\n",
    "    return 0, df_sentence, ents_list_old, uris_list_old,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ler conllu para filtrar sentencas que contenham pelo menos duas URIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of sentences in conllu -> 2467\n"
     ]
    }
   ],
   "source": [
    "#CONLLU_PATH = \"../PetroNER/petroner-uri-2023-04-05.conllu\"\n",
    "CONLLU_PATH = \"../PetroNER/petroner-uri-validação.conllu\"\n",
    "data_file = open(CONLLU_PATH, \"r\", encoding=\"utf-8\")\n",
    "sentences=[]\n",
    "for tokenlist in parse_incr(data_file):\n",
    "    sentences.append(tokenlist)\n",
    "print('Total number of sentences in conllu ->',len(sentences))\n",
    "#algum problema nessas sentencas em diferentes petroner\n",
    "sentences_with_issues = [12408, 13636, 15264, 21023, 21122, 23920, 24017]\n",
    "# sentences_with_issues = [1749, 4747, 4935, 5066, 5235, 5798, 6802, 7858, 8881, 9271, 10689, 10691, 10695, 10750, \n",
    "#                          11144, 12408, 13393, 13636, 15264, 17494, 18462, 18476, 19346, 21023, 21105, 21122, \n",
    "#                         22163, 23920, 24017]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rotina para contar entidades e URIs no conllu (é necessário rodar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_char com problema\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "------------\n",
      "lista de diferentes tipos de entidades pós filtragem\n",
      "['B=BACIA', 'B=CAMPO', 'B=ESTRUTURA_FÍSICA', 'B=FLUIDO', 'B=FLUIDODATERRA_i', 'B=FLUIDODATERRA_o', 'B=NÃOCONSOLID', 'B=POÇO', 'B=POÇO_Q', 'B=POÇO_T', 'B=ROCHA', 'B=TIPO_POROSIDADE', 'B=UNIDADE_CRONO', 'B=UNIDADE_LITO']\n",
      "------------\n",
      "quantidades de diferentes tipos de entidades pós filtragem\n",
      "[278, 41, 100, 8, 38, 8, 102, 98, 1, 1, 216, 12, 279, 82]\n",
      "------------\n",
      "Total de URIs pós filtragem -> 1264\n"
     ]
    }
   ],
   "source": [
    "df_new = pd.DataFrame()\n",
    "contSentences = 0\n",
    "ents, uris = [], []\n",
    "for idxSentence in range(0,len(sentences)):\n",
    "    #if idxSentence not in sentences_with_issues:\n",
    "    try:\n",
    "        #print('sentence = ' ,idxSentence)\n",
    "        sentence = sentences[idxSentence]\n",
    "        json_sent = json.dumps(sentence)\n",
    "        df_sentence = pd.read_json(json_sent)\n",
    "        df_sentence = remove_null_tokens(df_sentence) \n",
    "        if verify_sentence_is_functioning(df_sentence):\n",
    "            df_sentence = correct_start_end(df_sentence)    \n",
    "            \n",
    "            checkQtdTokenURI, df_test, ents, uris = check_sentence_for_pair_uris(df_sentence,ents,uris)\n",
    "            if checkQtdTokenURI == 1:\n",
    "                contSentences+= 1\n",
    "                df_test['sentence'] = contSentences\n",
    "                df_test['#sentence_original'] = idxSentence\n",
    "                df_new = df_new.append(df_test)\n",
    "                       \n",
    "                #print('sentences after filtering ->', contSentences)\n",
    "#                 raise SystemExit(\"Stop right there!\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "df_new.drop(columns=[\"start\",\"end\",\"head\",\"id\",\"feats\",\"upos\",\"lemma\",\"xpos\"], inplace=True)\n",
    "\n",
    "#pickle.dump(df_new, open('df_filtred_petroner_uri_2023_04_05.conllu.pkl', 'wb'), protocol=pickle.HIGHEST_PROTOCOL)\n",
    "#df_new.to_csv('df_filtred_petroner_uri_2023_04_05_conllu.csv',encoding = 'utf-8',index=False)\n",
    "\n",
    "pickle.dump(df_new, open('df_filtred_petroner_uri_valid.conllu.pkl', 'wb'), protocol=pickle.HIGHEST_PROTOCOL)\n",
    "df_new.to_csv('df_filtred_petroner_uri_valid_conllu.csv',encoding = 'utf-8',index=False)\n",
    "\n",
    "\n",
    "#entidades é a lista de diferentes tipos de entidades que aparecem após filtragem das sentencas\n",
    "entidades, numb_ents = np.unique(ents, return_counts = True)\n",
    "#grafos é a lista de diferentes URIs anotadas que aparecem após filtragem das sentencas\n",
    "grafos, numb_grafos = np.unique(uris, return_counts = True)\n",
    "print('------------')\n",
    "print('lista de diferentes tipos de entidades pós filtragem')\n",
    "print(entidades.tolist())\n",
    "print('------------')\n",
    "print('quantidades de diferentes tipos de entidades pós filtragem')\n",
    "print(numb_ents.tolist())\n",
    "print('------------')\n",
    "print('Total de URIs pós filtragem ->', sum(numb_grafos.tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rotina para contar entidades e URIS no arquivo conllu (não é necessário rodar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "start_char com problema\n",
      "eliminada\n",
      "start_char com problema\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "start_char com problema\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "start_char com problema\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "start_char com problema\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "------------\n",
      "lista de diferentes tipos de entidades no conllu\n",
      "['B=BACIA', 'B=CAMPO', 'B=ESTRUTURA_FÍSICA', 'B=EVENTO_PETRO', 'B=FLUIDO', 'B=FLUIDODATERRA_i', 'B=FLUIDODATERRA_o', 'B=NÃOCONSOLID', 'B=POÇO', 'B=POÇO_Q', 'B=POÇO_R', 'B=POÇO_T', 'B=ROCHA', 'B=TEXTURA', 'B=TIPO_POROSIDADE', 'B=UNIDADE_CRONO', 'B=UNIDADE_LITO']\n",
      "------------\n",
      "quantidades de diferentes tipos de entidades no conllu\n",
      "[3992, 702, 2029, 492, 174, 1372, 245, 1034, 1200, 9, 3, 35, 2768, 139, 22, 2906, 1486]\n",
      "------------\n",
      "Total de URIs no Conllu original -> 17941\n"
     ]
    }
   ],
   "source": [
    "ents_pre = []\n",
    "uris_pre = []\n",
    "for idxGroups in range(0,len(sentences)):\n",
    "    if idxGroups not in sentences_with_issues: \n",
    "        #print('sentence = ' ,idxGroups)\n",
    "        sentence = sentences[idxGroups]\n",
    "        json_temp = json.dumps(sentence)\n",
    "        df_get_start_end = pd.read_json(json_temp)\n",
    "        df_get_start_end = remove_null_tokens(df_get_start_end) \n",
    "        if verify_sentence_is_functioning(df_get_start_end):\n",
    "            df_get_start_end = correct_start_end(df_get_start_end) \n",
    "        \n",
    "            ents_pre, uris_pre = ents_uris_counter_prefiltering(df_get_start_end,ents_pre,uris_pre)\n",
    "        \n",
    "entidades_pre, numb_ents_pre = np.unique(ents_pre, return_counts = True)\n",
    "grafos_pre, numb_grafos_pre = np.unique(uris_pre, return_counts = True)\n",
    "print('------------')\n",
    "print('lista de diferentes tipos de entidades no conllu')\n",
    "print(entidades_pre.tolist())\n",
    "print('------------')\n",
    "print('quantidades de diferentes tipos de entidades no conllu')\n",
    "print(numb_ents_pre.tolist())\n",
    "print('------------')\n",
    "print('Total de URIs no Conllu original ->',sum(numb_grafos_pre.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
