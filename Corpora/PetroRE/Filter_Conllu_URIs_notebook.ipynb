{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook para filtrar o arquivo conllu em sentenças que contenham pelo menos duas URIS anotadas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "from conllu import parse\n",
    "import pandas as pd\n",
    "from conllu import parse_incr\n",
    "import pickle\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import unicodedata\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funções "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utils\n",
    "def ents_uris_counter_prefiltering(sentence,ents,uris): #contagem de entidades e uris pre-filtragem\n",
    "    entidades_sentenca_old = ents\n",
    "    uris_sentenca_old = uris\n",
    "    ents_list = ents\n",
    "    uris_list = uris\n",
    "    if sentence.iloc[0][\"misc\"].get('eliminar') == '':#retirar linha com eliminar\n",
    "        print('ELIMINADA')\n",
    "        return 0, entidades_sentenca_old, uris_sentenca_old   \n",
    "    for idx_row in range(0,len(sentence)):\n",
    "        if sentence.iloc[idx_row]['misc'].get('start_char') == None:#problema com start_char\n",
    "            return 0, entidades_sentenca_old, uris_sentenca_old\n",
    "        token = sentence.iloc[idx_row]['deps']\n",
    "        grafo = sentence.iloc[idx_row]['misc'].get('grafo')\n",
    "        if \"B=\" in token:\n",
    "            ents_list.append(token)\n",
    "        if grafo:\n",
    "            uris_list.append(grafo)     \n",
    "    return 1, ents_list, uris_list\n",
    "\n",
    "def check_sentence_for_uris(sentence, ents, uris): #funcao que conta quantas URIS tem dentro da sentenca\n",
    "    entidades_sentenca_old = ents\n",
    "    uris_sentenca_old = uris\n",
    "    ents_list = ents\n",
    "    uris_list = uris\n",
    "    countURIs = 0 \n",
    "    if sentence.iloc[0][\"misc\"].get('eliminar') == '':#retirar linha com eliminar\n",
    "        print('eliminada')\n",
    "        return 0, entidades_sentenca_old, uris_sentenca_old \n",
    "    for idx_row in range(0,len(sentence)):\n",
    "        if sentence.iloc[idx_row]['misc'].get('start_char') == None:#problema com start_char\n",
    "            print('start_char com problema')\n",
    "            return 0, entidades_sentenca_old, uris_sentenca_old\n",
    "        token = sentence.iloc[idx_row]['deps']\n",
    "        grafo = sentence.iloc[idx_row]['misc'].get('grafo')\n",
    "        if \"B=\" in token and grafo: \n",
    "            ents_list = ents_list + [token]\n",
    "            uris_list = uris_list + [grafo]\n",
    "            countURIs += 1\n",
    "    if countURIs > 1: #tem no minimo 2 entidades com URI\n",
    "        return 1, ents_list, uris_list\n",
    "    return 0, entidades_sentenca_old, uris_sentenca_old\n",
    "\n",
    "def get_start(x, offset):\n",
    "    start = str(int(x[\"start_char\"]) - offset)\n",
    "    return start\n",
    "def get_end(x, offset):\n",
    "    end = str(int(x[\"end_char\"]) - offset)\n",
    "    return end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ler conllu para filtrar sentencas que contenham pelo menos duas URIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of sentences in conllu -> 24035\n"
     ]
    }
   ],
   "source": [
    "CONLLU_PATH = \"petroner-uri-2023-04-05.conllu\"\n",
    "data_file = open(CONLLU_PATH, \"r\", encoding=\"utf-8\")\n",
    "sentences=[]\n",
    "for tokenlist in parse_incr(data_file):\n",
    "    sentences.append(tokenlist)\n",
    "print('Total number of sentences in conllu ->',len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#algum problema nessas sentencas em diferentes petroner\n",
    "sentences_with_issues = [12408, 13636, 15264, 21023, 21122, 23920, 24017]\n",
    "# sentences_with_issues = [1749, 4747, 4935, 5066, 5235, 5798, 6802, 7858, 8881, 9271, 10689, 10691, 10695, 10750, \n",
    "#                          11144, 12408, 13393, 13636, 15264, 17494, 18462, 18476, 19346, 21023, 21105, 21122, \n",
    "#                         22163, 23920, 24017]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rotina para contar entidades e URIs no conllu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ents_pre = []\n",
    "uris_pre = []\n",
    "for idxGroups in range(0,len(sentences)):\n",
    "    if idxGroups not in sentences_with_issues: \n",
    "        \n",
    "        print('sentence = ' ,idxGroups)\n",
    "        sentence = sentences[idxGroups]\n",
    "        json_temp = json.dumps(sentence)\n",
    "        df_get_start_end = pd.read_json(json_temp)\n",
    "\n",
    "        df_get_start_end = df_get_start_end[df_get_start_end['misc'].notna()]\n",
    "        df_get_start_end = df_get_start_end.reset_index(drop=True)\n",
    "        df_get_start_end = df_get_start_end[df_get_start_end['deps'].notna()]\n",
    "        df_get_start_end = df_get_start_end.reset_index(drop=True)\n",
    "        \n",
    "        checkQtdTokenURI, ents_pre, uris_pre = ents_uris_counter_prefiltering(df_get_start_end,ents_pre,uris_pre)\n",
    "        \n",
    "entidades_pre, numb_ents_pre = np.unique(ents_pre, return_counts = True)\n",
    "grafos_pre, numb_grafos_pre = np.unique(uris_pre, return_counts = True)\n",
    "print('------------')\n",
    "print('lista de diferentes tipos de entidades no conllu')\n",
    "print(entidades_pre.tolist())\n",
    "print('------------')\n",
    "print('quantidades de diferentes tipos de entidades no conllu')\n",
    "print(numb_ents_pre.tolist())\n",
    "print('------------')\n",
    "print('Total de URIs no Conllu original ->',sum(numb_grafos_pre.tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rotina para filtrar o arquivo conllu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtred = pd.DataFrame()\n",
    "contSentences = 0\n",
    "ents = []\n",
    "uris = []\n",
    "for idxGroups in range(0,len(sentences)):\n",
    "    if idxGroups not in sentences_with_issues:\n",
    "        print('sentence = ' ,idxGroups)\n",
    "        sentence = sentences[idxGroups]\n",
    "        json_sent = json.dumps(sentence)\n",
    "        df_sentence = pd.read_json(json_sent)\n",
    "        \n",
    "        #retira tokens nulos na sentenca\n",
    "        df_sentence = df_sentence[df_sentence['misc'].notna()]\n",
    "        df_sentence = df_sentence.reset_index(drop=True)\n",
    "        df_sentence = df_sentence[df_sentence['deps'].notna()]\n",
    "        df_sentence = df_sentence.reset_index(drop=True)\n",
    "\n",
    "        #checa se a sentenca tem minimo de 2 URIs\n",
    "        checkQtdTokenURI, ents, uris = check_sentence_for_uris(df_sentence,ents,uris)\n",
    "        if checkQtdTokenURI == 1:\n",
    "            contSentences+= 1\n",
    "            #arruma o offset nas colunas 'start' e 'end'\n",
    "            offset = int(df_sentence.iloc[0][\"misc\"].get(\"start_char\"))\n",
    "            df_sentence['start'] = df_sentence['misc'].apply(lambda x: get_start(x, offset))\n",
    "            df_sentence['end'] = df_sentence['misc'].apply(lambda x: get_end(x, offset))\n",
    "\n",
    "            df_sentence['sentence'] = contSentences\n",
    "            df_sentence['#sentence_original'] = idxGroups\n",
    "            print('sentences after filtering ->', contSentences)\n",
    "            df_filtred = pd.concat([df_filtred, df_sentence ])\n",
    "            \n",
    "#salvar o dataframe com as sentencas ja filtradas em arquivo .pkl                   \n",
    "# pickle.dump(df_filtred, open('df_filtred_petroner_uri_2022_04_05.conllu.pkl', 'wb'), protocol=pickle.HIGHEST_PROTOCOL)\n",
    "df_filtred.to_csv('df_filtred_petroner_uri_2022_04_05_conllu.csv',encoding = 'utf-8',index=False)\n",
    "\n",
    "#entidades é a lista de diferentes tipos de entidades que aparecem após filtragem das sentencas\n",
    "entidades, numb_ents = np.unique(ents, return_counts = True)\n",
    "#grafos é a lista de diferentes URIs anotadas que aparecem após filtragem das sentencas\n",
    "grafos, numb_grafos = np.unique(uris, return_counts = True)\n",
    "print('------------')\n",
    "print('lista de diferentes tipos de entidades pós filtragem')\n",
    "print(entidades.tolist())\n",
    "print('------------')\n",
    "print('quantidades de diferentes tipos de entidades pós filtragem')\n",
    "print(numb_ents.tolist())\n",
    "print('------------')\n",
    "print('Total de URIs pós filtragem ->', sum(numb_grafos.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
