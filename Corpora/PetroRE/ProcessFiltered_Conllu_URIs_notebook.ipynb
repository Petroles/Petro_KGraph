{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import operator as op\n",
    "import warnings \n",
    "from owlready2 import * #\n",
    "import random\n",
    "import unicodedata\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregar ontologia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "* Owlready2 * Warning: ignoring cyclic subclass of/subproperty of, involving:\n",
      "  http://www.semanticweb.org/bg40/ontologies/2022/5/untitled-ontology-2#has_beginning\n",
      "\n",
      "* Owlready2 * Warning: ignoring cyclic subclass of/subproperty of, involving:\n",
      "  http://www.semanticweb.org/bg40/ontologies/2022/5/untitled-ontology-2#has_end\n",
      "\n",
      "* Owlready2 * Warning: ignoring cyclic subclass of/subproperty of, involving:\n",
      "  http://www.semanticweb.org/bg40/ontologies/2022/5/untitled-ontology-2#interval_contains\n",
      "\n",
      "* Owlready2 * Warning: ignoring cyclic subclass of/subproperty of, involving:\n",
      "  http://www.semanticweb.org/bg40/ontologies/2022/5/untitled-ontology-2#interval_during\n",
      "\n",
      "* Owlready2 * Warning: ignoring cyclic subclass of/subproperty of, involving:\n",
      "  http://www.semanticweb.org/bg40/ontologies/2022/5/untitled-ontology-2#interval_in\n",
      "\n",
      "* Owlready2 * Warning: ignoring cyclic subclass of/subproperty of, involving:\n",
      "  http://www.semanticweb.org/bg40/ontologies/2022/5/untitled-ontology-2#interval_finished_by\n",
      "\n",
      "* Owlready2 * Warning: ignoring cyclic subclass of/subproperty of, involving:\n",
      "  http://www.semanticweb.org/bg40/ontologies/2022/5/untitled-ontology-2#interval_finishes\n",
      "\n",
      "* Owlready2 * Warning: ignoring cyclic subclass of/subproperty of, involving:\n",
      "  http://www.semanticweb.org/bg40/ontologies/2022/5/untitled-ontology-2#interval_started_by\n",
      "\n",
      "* Owlready2 * Warning: ignoring cyclic subclass of/subproperty of, involving:\n",
      "  http://www.semanticweb.org/bg40/ontologies/2022/5/untitled-ontology-2#interval_starts\n",
      "\n",
      "* Owlready2 * Warning: ignoring cyclic subclass of/subproperty of, involving:\n",
      "  http://www.semanticweb.org/bg40/ontologies/2022/5/untitled-ontology-2#temporal_reference_system_used\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "get_ontology(\"http://www.semanticweb.org/bg40/ontologies/2022/5/untitled-ontology-2#\")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onto_name = \"OntoGeoLogicaPovoadaInstanciasRelacoes\"\n",
    "onto = get_ontology(\"./ner_geologica/OntoGeoLogicaPovoadaInstanciasRelacoes.owl\")\n",
    "onto.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Folder outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_folder_path = \"./JSONs_24_04\"\n",
    "save_csv_name = 'df_bert_sentences_24_04.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções para gerar Jsons a serem lidos no labelstudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResultRelationJson(object):\n",
    "    def __init__(self, from_id, to_id, relations, direction = \"right\"):\n",
    "        self.dict = {\n",
    "            \"from_id\": str(from_id),\n",
    "            \"to_id\": str(to_id),\n",
    "            \"type\": \"relation\",\n",
    "            \"direction\": direction,\n",
    "            \"labels\": relations\n",
    "        }\n",
    "    def get_dict(self):\n",
    "        return self.dict\n",
    "    \n",
    "class ResultNERJson(object):\n",
    "    def __init__(self, row):     \n",
    "        self.result_dict = {\n",
    "            \"value\": {\n",
    "            \"start\": row[\"start_word\"],\n",
    "            \"end\": row[\"end_word\"],\n",
    "            \"text\": row[\"word_join\"],\n",
    "            \"labels\": [\n",
    "              row[\"label_word\"]\n",
    "            ],\n",
    "            \"URI\": row[\"URI\"]\n",
    "            },\n",
    "            \n",
    "            \"id\": row[\"index_e\"],\n",
    "            \"from_name\": \"label\",\n",
    "            \"to_name\": \"text\",\n",
    "            \"type\": \"labels\",\n",
    "            \"origin\": \"prediction\"\n",
    "        }\n",
    "    def get_dict(self):\n",
    "        return self.result_dict\n",
    "    \n",
    "    \n",
    "class CreateOutput(object):\n",
    "    def __init__(self, text, filtred_sentence, entity_name_new):\n",
    "        self.filtred_sentence = filtred_sentence\n",
    "        self.entity_name_new = entity_name_new\n",
    "        self.main_dict = {\n",
    "            \"id\": 1,\n",
    "            \"data\": {\n",
    "              \"text\": text #sentenca inteira\n",
    "            },\n",
    "            \"annotations\": []\n",
    "        }\n",
    "        self._add_annotations()\n",
    "        \n",
    "    def _add_annotations(self):\n",
    "        results = []\n",
    "        count = 0        \n",
    "        for index, row in self.entity_name_new.iterrows(): \n",
    "            results.append(ResultNERJson(row).get_dict())        \n",
    "        item = [{\n",
    "              \"id\": 1,\n",
    "              \"created_username\": \" null, 0\",\n",
    "              \"created_ago\": \"\",\n",
    "              \"result\": results\n",
    "            }]\n",
    "        self.main_dict[\"annotations\"] = item\n",
    "    \n",
    "    def get_output(self):\n",
    "        return self.main_dict\n",
    "    \n",
    "    def add_relationship(self, from_id, to_id, relations, direction):\n",
    "        results = self.main_dict.get(\"annotations\")[0].get(\"result\")\n",
    "        relation = ResultRelationJson(from_id, to_id, [relations], direction).get_dict()\n",
    "        results.append(relation)\n",
    "        print('-----------')\n",
    "        print(\"relation\")\n",
    "        print(relation)\n",
    "        self.main_dict[\"annotations\"][0][\"result\"] = results\n",
    "        \n",
    "def combine_itens_from_lists_add_in_json(from_id_vec, to_id_vec, relation_from_vec, output):\n",
    "    for idxRelation in range(0,len(from_id_vec)):\n",
    "            direction = \"right\"\n",
    "            output.add_relationship(from_id=from_id_vec[idxRelation], to_id=to_id_vec[idxRelation], relations = relation_from_vec[idxRelation], direction=direction)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções para processar as sentenças"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utils\n",
    "def check_I_entities(df_get_start_end, i,entity):\n",
    "    next_entity_is_I = (df_get_start_end.iloc[i][\"deps\"] == entity) or (df_get_start_end.iloc[i][\"deps\"] == None and df_get_start_end.iloc[i+1][\"deps\"] == entity)\n",
    "    return next_entity_is_I\n",
    "\n",
    "def get_words_by_entities(indexes, df_get_start_end):\n",
    "    df_save_words = pd.DataFrame(columns=['index_e', \"LABEL\", \"START\", \"END\", \"TEXT\", \"word_join\", \"start_word\", \"end_word\", \"label_word\",\"URI\"])\n",
    "    \n",
    "    for index in indexes:\n",
    "        entity = df_get_start_end.iloc[index]['deps']\n",
    "        entity_I = entity.replace(\"B=\",\"I=\")\n",
    "        count = 1\n",
    "        word_join = \"\"\n",
    "        row_main = df_get_start_end.iloc[index]\n",
    "        word_join = \" \".join([word_join, row_main['form']])\n",
    "        start_word = row_main['start']\n",
    "        end_word = row_main['end']\n",
    "        label_word = row_main['deps'].replace(\"B=\", \"\")\n",
    "        URI = df_get_start_end.iloc[index]['misc'].get('grafo')\n",
    "        while index+count != len(df_get_start_end) and (df_get_start_end.iloc[index+count][\"deps\"] == entity_I or check_I_entities(df_get_start_end, index+count,entity_I)):\n",
    "            row = df_get_start_end.iloc[index+count]\n",
    "            word_join = \" \".join([word_join, row['form']])\n",
    "            end_word = row['end']\n",
    "            count+=1\n",
    "\n",
    "        df_save_words.loc[len(df_save_words.index)] = [index, \n",
    "                                                       row_main['deps'],\n",
    "                                                       df_get_start_end.iloc[index]['start'], #so da primeira linha\n",
    "                                                       df_get_start_end.iloc[index]['end'], #so da primeira linha\n",
    "                                                       row_main['form'],\n",
    "                                                       word_join.strip(),\n",
    "                                                       start_word,\n",
    "                                                       end_word,\n",
    "                                                       label_word,\n",
    "                                                       URI]\n",
    "    return df_save_words, word_join\n",
    "\n",
    "def get_relations_between_uris(uri_1, uri_2):\n",
    "    dict_relation_uris = {}\n",
    "    \n",
    "    #Pega as relacoes que a URI1 tem\n",
    "    relation_query_results = list(default_world.sparql(\"\"\"\n",
    "            SELECT DISTINCT ?rel\n",
    "            WHERE{?uri ?rel ?obj\n",
    "                 FILTER(contains(str(?rel), \"http://www.semanticweb.org/bg40/ontologies/2022/5/untitled-ontology-2#\"))\n",
    "                 FILTER (contains(str(?uri), \"\"\" + '\"' + uri_1 + '\"' + \"\"\"))\n",
    "                 }\n",
    "            \"\"\"))\n",
    "    \n",
    "    relations_str = []\n",
    "    for relation_uris in relation_query_results:\n",
    "        relations_str.append(str(relation_uris[0]).rsplit(\".\",1)[-1])\n",
    "        \n",
    "    # Para cada tipo de relação procura se existe match entre URI1 e URI2\n",
    "    for relation in relations_str:\n",
    "        relation_between_words = list(default_world.sparql(\"\"\"\n",
    "                PREFIX prefix: <http://www.semanticweb.org/bg40/ontologies/2022/5/untitled-ontology-2#>\n",
    "                SELECT distinct ?y ?x2\n",
    "                WHERE{?y prefix:\"\"\" +  relation  +  \"\"\" ?x1\n",
    "\n",
    "                      FILTER (contains(str(?y), \"\"\" + '\"' + uri_1  + '\"' + \"\"\"))        \n",
    "\n",
    "                      ?x2 rdf:type ?j                                   \n",
    "                      FILTER (contains(str(?x2), \"\"\" + '\"' + uri_2  + '\"' + \"\"\"))\n",
    "\n",
    "                      FILTER ( ?x2 = ?x1 )\n",
    "                    }\n",
    "                \"\"\"))\n",
    "        dict_relation_uris[relation] = relation_between_words\n",
    "    return dict_relation_uris\n",
    "\n",
    "def create_relations_dataframe(df_relation,token,token2,URI_1,URI_2,x,originalSentenceNumber):\n",
    "    df_relation_new = pd.DataFrame(columns=['Relation','Ent1','Ent2','URI_1','URI_2','#Sentence'])\n",
    "    df_relation_new.loc[0] = [x,\n",
    "                            token.replace('B=',''),\n",
    "                            token2.replace('B=',''),\n",
    "                            URI_1,\n",
    "                            URI_2,\n",
    "                            originalSentenceNumber]\n",
    "    df_relation = pd.concat([df_relation, df_relation_new])\n",
    "    return df_relation\n",
    "\n",
    "def verifica_pares_entidade_interesse(ENT_1, ENT_2):  \n",
    "    lista_from = ['POÇO','UNIDADE_LITO','UNIDADE_LITO','CAMPO','POÇO','POÇO','UNIDADE_LITO','UNIDADE_LITO']\n",
    "    lista_to = ['UNIDADE_LITO','NÂOCONSOLID','ROCHA','BACIA','BACIA','CAMPO','BACIA','UNIDADE_CRONO']   \n",
    "#     lista_from = ['POÇO','UNIDADE_LITO','UNIDADE_LITO','CAMPO','POÇO','POÇO','UNIDADE_LITO','UNIDADE_CRONO']\n",
    "#     lista_to = ['UNIDADE_LITO','NÂOCONSOLID','ROCHA','BACIA','BACIA','CAMPO','BACIA','UNIDADE_CRONO']      \n",
    "    for idx in range(0,len(lista_to)):\n",
    "        if lista_from[idx] == ENT_1 and lista_to[idx] == ENT_2:\n",
    "            return True  \n",
    "    return False\n",
    "\n",
    "def createText_added_entities(text,df_1,df_2):\n",
    "    start_ent1, end_ent1, = int(df_1.iloc[-1]['start_word']), int(df_1.iloc[-1]['end_word'])\n",
    "    start_ent2, end_ent2 = int(df_2.iloc[-1]['start_word']), int(df_2.iloc[-1]['end_word'])\n",
    "    \n",
    "    Ent1_inic, Ent1_end = '[E1] ', ' [/E1]'\n",
    "    Ent2_inic, Ent2_end = '[E2] ', ' [/E2]'\n",
    "    \n",
    "    if start_ent1 < start_ent2: #[E1] vem antes de [E2]\n",
    "        new_end_ent1 = end_ent1 + len(Ent1_inic)\n",
    "        new_start_ent2 = start_ent2 + len(Ent1_inic) + len(Ent1_end)\n",
    "        new_end_ent2 = end_ent2 + len(Ent1_inic) + len(Ent1_end) + len(Ent2_inic)\n",
    "        \n",
    "        #adicionando [E1] e [/E1]\n",
    "        text_new = text[:start_ent1] + Ent1_inic + text[start_ent1:]\n",
    "        text_new = text_new[:new_end_ent1] + Ent1_end + text_new[new_end_ent1:]\n",
    "        #adicionando [E2] e [/E2]\n",
    "        text_new2 = text_new[:new_start_ent2] + Ent2_inic + text_new[new_start_ent2:]\n",
    "        text_new2 = text_new2[:new_end_ent2] + Ent2_end + text_new2[new_end_ent2:]\n",
    "    \n",
    "    else: #[E2] vem antes de [E1]      \n",
    "        new_end_ent2 = end_ent2 + len(Ent2_inic)\n",
    "        new_start_ent1 = start_ent1 + len(Ent2_inic) + len(Ent2_end)\n",
    "        new_end_ent1 = end_ent1 + len(Ent2_inic) + len(Ent2_end) + len(Ent1_inic)\n",
    "        \n",
    "        #adicionando [E2] e [/E2]\n",
    "        text_new = text[:start_ent2] + Ent2_inic + text[start_ent2:]\n",
    "        text_new = text_new[:new_end_ent2] + Ent2_end + text_new[new_end_ent2:]\n",
    "        #adicionando [E1] e [/E1]\n",
    "        text_new2 = text_new[:new_start_ent1] + Ent1_inic + text_new[new_start_ent1:]\n",
    "        text_new2 = text_new2[:new_end_ent1] + Ent1_end + text_new2[new_end_ent1:]\n",
    "        \n",
    "    return text_new2\n",
    "\n",
    "def print_sentence_text(sentence):\n",
    "    size_sentence = int(sentence.iloc[-1][\"end\"])\n",
    "    text = \" \"*size_sentence\n",
    "    for index, row in sentence.iterrows():\n",
    "        text = text[:int(row[\"start\"])] + row[\"form\"] +text[int(row[\"end\"]):]\n",
    "    print(text)\n",
    "    print(\"-------------\")\n",
    "    return text\n",
    "\n",
    "def create_bert_dataframe(df_bert,idxTokens,idxTokens2,sentence,text,has_relation,relation_type,SentenceNumber):\n",
    "    df_1, wordjoin_1_trash = get_words_by_entities([idxTokens],sentence)\n",
    "    df_2, wordjoin_2_trash = get_words_by_entities([idxTokens2],sentence)\n",
    "    ent1, ent2 = df_1.iloc[-1]['LABEL'], df_2.iloc[-1]['LABEL']\n",
    "    ent1, ent2 = ent1.replace('B=',''), ent2.replace('B=','')\n",
    "    text_bert_ents = createText_added_entities(text,df_1,df_2)\n",
    "#     print(text_bert_ents)\n",
    "    df_bert_temp = pd.DataFrame(columns=['index_e','sentence','Ent1','Ent2','has_relation','relation'])\n",
    "    df_bert_temp.loc[0] = [SentenceNumber,\n",
    "                           text_bert_ents,\n",
    "                           ent1,\n",
    "                           ent2,\n",
    "                           has_relation,\n",
    "                           relation_type]\n",
    "    df_bert = pd.concat([df_bert, df_bert_temp])\n",
    "    return df_bert\n",
    "\n",
    "def saveJsonFiles(df,text,from_id,to_id, lista_relaoces_sentence,sentence,SentenceNum,path):\n",
    "    print(\"-------------\")\n",
    "    print(df.head(50))\n",
    "    print(\"-------------\")\n",
    "    print(\"sentence-> \",SentenceNum)\n",
    "    print(\"-------------\")\n",
    "    print(text)\n",
    "    print(\"-------------\")\n",
    "    print('Saved Json ->', True)\n",
    "    output = CreateOutput(text,sentence, df)\n",
    "    combine_itens_from_lists_add_in_json(from_id, to_id, lista_relacoes_sentence, output)\n",
    "    print(\"-------------\")\n",
    "    with open(os.path.join(path,f\"{SentenceNum}.json\"), \"w\") as outfile: \n",
    "        json.dump(output.get_output(), outfile) \n",
    "        \n",
    "def create_df_JsonFiles(df_entity,x,token,token2,URI_1,URI_2,idxTokens,idxTokens2,from_id,to_id,sentence):\n",
    "    entity_name_new_token1, wordjoin_1 = get_words_by_entities([idxTokens],sentence)\n",
    "    if idxTokens not in from_id and idxTokens not in to_id:\n",
    "        df_entity = pd.concat([df_entity, entity_name_new_token1])\n",
    "    entity_name_new_token2, wordjoin_2 = get_words_by_entities([idxTokens2],sentence)\n",
    "    if idxTokens2 not in from_id and idxTokens2 not in to_id:\n",
    "        df_entity = pd.concat([df_entity, entity_name_new_token2])\n",
    "    print('Token 1 = ', wordjoin_1, '--- Class 1 = ', token.replace('B=',''), '--- URI 1 = ', URI_1)\n",
    "    print('Token 2 = ', wordjoin_2, '--- Class 2 = ', token2.replace('B=',''),'--- URI 2 = ',URI_2)\n",
    "    print('Relation Type = ', x)\n",
    "    print(\"-------------\")\n",
    "    return df_entity\n",
    "    \n",
    "def getDictBert(df,text,lista_relacoes_sentence,from_id,to_id,list_sentences_dict):\n",
    "    sentence_dict = []\n",
    "    list_tokens_dict = []\n",
    "    list_relations_dict = []\n",
    "    document = text\n",
    "    for idxTokenList in range(0,df.shape[0]):\n",
    "        word_join = df.iloc[idxTokenList]['word_join']\n",
    "        start = int(df.iloc[idxTokenList]['start_word'])\n",
    "        end = int(df.iloc[idxTokenList]['end_word'])\n",
    "        token_start = df.iloc[idxTokenList]['index_e'] #filtred_sentence.iloc[5]\n",
    "        token_end = token_start + op.countOf(word_join,\" \")\n",
    "        entity_label = df.iloc[idxTokenList]['LABEL'].replace('B=','')\n",
    "        tokens_dict = {'text': word_join,\n",
    "                       'start': start,\n",
    "                       'end': end,\n",
    "                       'token_start': token_start,\n",
    "                       'token_end': token_end,\n",
    "                       'entity_label': entity_label\n",
    "                      }\n",
    "        list_tokens_dict.append(tokens_dict)\n",
    "    for idxRelList in range(0,len(lista_relacoes_sentence)):\n",
    "        relation_from_id = from_id[idxRelList]\n",
    "        relation_to_id = to_id[idxRelList]\n",
    "        relation_label = lista_relacoes_sentence[idxRelList]\n",
    "        relations_dict = {'child': relation_from_id,\n",
    "                          'head': relation_to_id,\n",
    "                          'relationLabel': relation_label} \n",
    "        list_relations_dict.append(relations_dict)\n",
    "    sentence_dict = {'document': document,\n",
    "                     'tokens': list_tokens_dict,\n",
    "                     'relations': list_relations_dict\n",
    "                    }\n",
    "    list_sentences_dict.append(sentence_dict)\n",
    "    with open('file_sentencasBERT.json', 'w') as fout:\n",
    "        json.dump(list_sentences_dict , fout)\n",
    "        \n",
    "    return list_sentences_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ler arquivo csv (ou pkl) com as sentenças pós filtragem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero total de sentenças pos-filtragem ->  4526\n"
     ]
    }
   ],
   "source": [
    "df_filtred = pickle.load(open('df_filtred_petroner_uri_2023_04_05.conllu.pkl', 'rb'))\n",
    "# df_filtred = pd.read_csv('df_filtred_petroner_uri_2022_04_05_conllu.csv')\n",
    "df_group = df_filtred.groupby('sentence')\n",
    "print('Numero total de sentenças pos-filtragem -> ',len(df_group))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rotina para processar as sentenças já filtradas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------\n",
      "sentence = 6\n",
      "idx in filtred file = 1\n",
      "Membro Mucuri, Eocretáceo    Bacia    Espirito Santo..\n",
      "-------------\n",
      "Token 1 =   Membro Mucuri --- Class 1 =  UNIDADE_LITO --- URI 1 =  #membro_010\n",
      "Token 2 =   Bacia Espirito Santo --- Class 2 =  BACIA --- URI 2 =  #BASE_CD_BACIA_270\n",
      "Relation Type =  located_in\n",
      "-------------\n",
      "-------------\n",
      "  index_e           LABEL START END    TEXT             word_join start_word  \\\n",
      "0       0  B=UNIDADE_LITO     0   6  Membro         Membro Mucuri          0   \n",
      "0       4         B=BACIA    29  34   Bacia  Bacia Espirito Santo         29   \n",
      "\n",
      "  end_word    label_word                 URI  \n",
      "0       13  UNIDADE_LITO         #membro_010  \n",
      "0       52         BACIA  #BASE_CD_BACIA_270  \n",
      "-------------\n",
      "sentence->  6\n",
      "-------------\n",
      "Membro Mucuri, Eocretáceo    Bacia    Espirito Santo..\n",
      "-------------\n",
      "Saved Json -> True\n",
      "-----------\n",
      "relation\n",
      "{'from_id': '0', 'to_id': '4', 'type': 'relation', 'direction': 'right', 'labels': ['located_in']}\n",
      "-------------\n",
      "-------------\n",
      "sentence = 36\n",
      "idx in filtred file = 2\n",
      "Ferreira 121 a Expressão de Reativações Pós-intrusão    Enxame de Diques Rio Ceará Mirim (Mesozóico): Implicações Evolução Tectônica    Bacia Potiguar .\n",
      "-------------\n",
      "-------------\n",
      "sentence = 49\n",
      "idx in filtred file = 3\n",
      "Esses conceitos foram inicialmente propostos para bacias marginais marinhas; todavia, seu sucesso foi tão grande, que os mesmos foram ajustados para outras classes de bacias..\n",
      "-------------\n",
      "-------------\n",
      "sentence = 54\n",
      "idx in filtred file = 4\n",
      "Vail et al. (1977) observaram evidências de onlap sobre discordâncias regionais em seções sismicas dip através de bacias de margem passiva..\n",
      "-------------\n",
      "-------------\n",
      "sentence = 66\n",
      "idx in filtred file = 5\n",
      "artigo, o autor propõe as bases que governam a sedimentação siliciclástica em bacias sedimentares: 1) sedimentos siliciclásticos devem ser trazidos até a margem    bacia por sistemas fluviais; 2) o preenchimento de uma bacia é alcançado      repetição de intervalos deposicionais e não-deposicionais; 3) em uma bacia sendo preenchida por sedimentos siliciclásticos, todos os pontos em uma superfície hiatal são diacrônicos..\n",
      "-------------\n",
      "-------------\n",
      "sentence = 83\n",
      "idx in filtred file = 6\n",
      "O tectonismo é responsável: 1)      gênese    depressão onde a bacia se instalará; 2)      modelamento    arcabouço estrutural    bacia; 3)      controle     principais sítios de acumulação de sedimentos; 4)      criação de barreiras e rotas preferenciais para entrada de sedimentos    bacia; 5)      controle    relevo    bacia de drenagem..\n",
      "-------------\n",
      "-------------\n",
      "sentence = 84\n",
      "idx in filtred file = 7\n",
      "As flutuações climáticas controlam: 1) o volume de água e de sedimentos que entram    bacia; 2) o tipo de sedimento gerado, seja detrítico, biológico ou quimico; 3) a precipitação pluviométrica e cobertura vegetal..\n",
      "-------------\n",
      "-------------\n",
      "sentence = 94\n",
      "idx in filtred file = 8\n",
      "Durante fases de nível (de base) baixo, fácies grossas             em canais fluviais     margens    bacia, em fan        adjacentes   falha de borda e em        progradantes de pequenas dimensões (fig. 4)..\n",
      "-------------\n",
      "-------------\n",
      "sentence = 97\n",
      "idx in filtred file = 9\n",
      "2.2 - Estratigrafia de Sequências    Formação Pendência, Bacia Potiguar  Della Favera et al. (1992) analisam a seção sin-rift    Bacia Potiguar,    qual identificaram quatro sequências balizadas por discordâncias e suas concordâncias relativas dentro    Formação Pendência..\n",
      "-------------\n",
      "Token 1 =   Formação Pendência --- Class 1 =  UNIDADE_LITO --- URI 1 =  #formacao_319\n",
      "Token 2 =   Bacia Potiguar --- Class 2 =  BACIA --- URI 2 =  #BASE_CD_BACIA_100\n",
      "Relation Type =  located_in\n",
      "-------------\n",
      "Token 1 =   Formação Pendência --- Class 1 =  UNIDADE_LITO --- URI 1 =  #formacao_319\n",
      "Token 2 =   Bacia Potiguar --- Class 2 =  BACIA --- URI 2 =  #BASE_CD_BACIA_100\n",
      "Relation Type =  located_in\n",
      "-------------\n",
      "Token 1 =   Formação Pendência --- Class 1 =  UNIDADE_LITO --- URI 1 =  #formacao_319\n",
      "Token 2 =   Bacia Potiguar --- Class 2 =  BACIA --- URI 2 =  #BASE_CD_BACIA_100\n",
      "Relation Type =  located_in\n",
      "-------------\n",
      "Token 1 =   Formação Pendência --- Class 1 =  UNIDADE_LITO --- URI 1 =  #formacao_319\n",
      "Token 2 =   Bacia Potiguar --- Class 2 =  BACIA --- URI 2 =  #BASE_CD_BACIA_100\n",
      "Relation Type =  located_in\n",
      "-------------\n",
      "-------------\n",
      "  index_e           LABEL START  END      TEXT           word_join start_word  \\\n",
      "0       5  B=UNIDADE_LITO    37   45  Formação  Formação Pendência         37   \n",
      "0       8         B=BACIA    57   62     Bacia      Bacia Potiguar         57   \n",
      "0      21         B=BACIA   129  134     Bacia      Bacia Potiguar        129   \n",
      "0      36  B=UNIDADE_LITO   254  262  Formação  Formação Pendência        254   \n",
      "\n",
      "  end_word    label_word                 URI  \n",
      "0       55  UNIDADE_LITO       #formacao_319  \n",
      "0       71         BACIA  #BASE_CD_BACIA_100  \n",
      "0      143         BACIA  #BASE_CD_BACIA_100  \n",
      "0      272  UNIDADE_LITO       #formacao_319  \n",
      "-------------\n",
      "sentence->  97\n",
      "-------------\n",
      "2.2 - Estratigrafia de Sequências    Formação Pendência, Bacia Potiguar  Della Favera et al. (1992) analisam a seção sin-rift    Bacia Potiguar,    qual identificaram quatro sequências balizadas por discordâncias e suas concordâncias relativas dentro    Formação Pendência..\n",
      "-------------\n",
      "Saved Json -> True\n",
      "-----------\n",
      "relation\n",
      "{'from_id': '5', 'to_id': '8', 'type': 'relation', 'direction': 'right', 'labels': ['located_in']}\n",
      "-----------\n",
      "relation\n",
      "{'from_id': '5', 'to_id': '21', 'type': 'relation', 'direction': 'right', 'labels': ['located_in']}\n",
      "-----------\n",
      "relation\n",
      "{'from_id': '36', 'to_id': '8', 'type': 'relation', 'direction': 'right', 'labels': ['located_in']}\n",
      "-----------\n",
      "relation\n",
      "{'from_id': '36', 'to_id': '21', 'type': 'relation', 'direction': 'right', 'labels': ['located_in']}\n",
      "-------------\n",
      "-------------\n",
      "sentence = 103\n",
      "idx in filtred file = 10\n",
      "O autor justifica a introdução de nomenclatura nova, pois a existente, criada para bacias marginais marinhas, não abrange, de modo claro, o empilhamento vertical de fácies em uma bacia tectonicamente ativa..\n",
      "-------------\n",
      "-------------\n",
      "sentence = 119\n",
      "idx in filtred file = 11\n",
      "modelo proposto      autor, são discutidos conceitos, apresentada nova terminologia para +ifts intracontinentais, e é aplicado    pacote sin-rift    Bacia    Recôncavo, onde o autor interpreta sistemas deposicionais, áreas-fonte, principais vias de ingresso de sedimentos    bacia e sucessão vertical dentro    que se denominou de tectono-sequência    Cretáceo Inferior    Bacia    Recôncavo..\n",
      "-------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-a1bfb8cf40e6>\u001b[0m in \u001b[0;36mget_relations_between_uris\u001b[0;34m(uri_1, uri_2)\u001b[0m\n\u001b[1;32m     46\u001b[0m                  \u001b[0mFILTER\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcontains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;31m?\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\" + '\"' + uri_1 + '\"' + \"\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                  }\n\u001b[0;32m---> 48\u001b[0;31m             \"\"\"))\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mrelations_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/owlready2/sparql/main.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mPreparedSelectQuery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPreparedQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mPreparedQuery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m       \u001b[0ml2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m       \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/owlready2/sparql/main.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0msql_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_rdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameter_datatypes\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msql_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_rdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msql_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mPreparedSelectQuery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPreparedQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "numberSentences = df_filtred.iloc[-1]['sentence'] #numero de sentencas diferentes no arquivo ja filtrado\n",
    "\n",
    "lista_relacoes, lista_uris, lista_classes, list_sentences_dict = [], [], [], []\n",
    "\n",
    "df_relation, df_bert = pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "countJsons=0\n",
    "for idx in range(1,len(df_group)):\n",
    "    filtred_sentence = df_group.get_group(idx)#aqui filtred_sentence é um dataframe da sentenca\n",
    "    originalSentenceNumber = filtred_sentence.iloc[0]['#sentence_original']\n",
    "    print(\"-------------\")\n",
    "    print('sentence =', originalSentenceNumber)\n",
    "    print('idx in filtred file =', idx)   \n",
    "    text = print_sentence_text(filtred_sentence)\n",
    "    \n",
    "    df_entity = pd.DataFrame()\n",
    "    from_id, to_id = [], []\n",
    "    relation_from, relation_to = [], []\n",
    "    lista_relacoes_sentence = []\n",
    "    is_to_save = False #se cada sentenca vai ser salva ao fim de seu processamento\n",
    "    df_bert.to_csv(save_csv_name, encoding='utf-8',index=False)\n",
    "    for idxTokens in range(0,len(filtred_sentence)):\n",
    "        token = filtred_sentence.iloc[idxTokens]['deps']     \n",
    "        if \"B=\" in token and filtred_sentence.iloc[idxTokens]['misc'].get('grafo'): #encontrou o comeco de uma entidade com URI\n",
    "            URI_1 = filtred_sentence.iloc[idxTokens]['misc'].get('grafo')    \n",
    "            for idxTokens2 in range(0,len(filtred_sentence)):\n",
    "                token2 = filtred_sentence.iloc[idxTokens2]['deps']   \n",
    "                if idxTokens!= idxTokens2 and \"B=\" in token2 and filtred_sentence.iloc[idxTokens2]['misc'].get('grafo'): #encontrou o comeco de uma  outra entidade com URI\n",
    "                    URI_2 = filtred_sentence.iloc[idxTokens2]['misc'].get('grafo')\n",
    "                    has_relation = False\n",
    "                    relation_URIs = get_relations_between_uris(URI_1, URI_2)            \n",
    "                    if relation_URIs != {}: #talvez exista relacao entre URIs, dicionario pode vir vazio -> []\n",
    "                        for x, y in relation_URIs.items():\n",
    "                            if y != []: #existe alguma relacao\n",
    "                                is_to_save = True\n",
    "                                has_relation = True\n",
    "                                relation_type = x\n",
    "                                Ent1 = token.replace(\"B=\",\"\")\n",
    "                                Ent2 = token2.replace(\"B=\",\"\")\n",
    "                                is_rel_interesse = verifica_pares_entidade_interesse(Ent1,Ent2)\n",
    "                                if is_rel_interesse == False:\n",
    "                                    relation_type = 'temporal_relation'\n",
    "                                lista_relacoes_sentence.append(relation_type)\n",
    "                                \n",
    "                                #para depois contabilizar os pares de entidade por relacao\n",
    "                                df_relation = create_relations_dataframe(df_relation,token,token2,URI_1,URI_2,x,originalSentenceNumber)\n",
    "                            \n",
    "                                #criar df_bert para BERT RE com codigo do Fabio\n",
    "                                df_bert = create_bert_dataframe(df_bert,idxTokens,idxTokens2,filtred_sentence,\n",
    "                                                                text,has_relation,relation_type,originalSentenceNumber)\n",
    "                                \n",
    "                                df_entity = create_df_JsonFiles(df_entity,x,token,token2,URI_1,URI_2,\n",
    "                                                                idxTokens,idxTokens2,from_id,to_id,filtred_sentence)\n",
    "                                \n",
    "                                from_id.append(idxTokens)\n",
    "                                to_id.append(idxTokens2)    \n",
    "                                \n",
    "                                #listas para contabilizar relacoes, uris e classes\n",
    "                                lista_relacoes.append(relation_type)\n",
    "                                lista_uris.append(URI_1)\n",
    "                                lista_uris.append(URI_2)         \n",
    "                                lista_classes.append(Ent1)\n",
    "                                lista_classes.append(Ent2)\n",
    "\n",
    "                    if not has_relation: #nao achou relacao\n",
    "                        relation_type = 'no_relation'\n",
    "                        df_bert = create_bert_dataframe(df_bert,idxTokens,idxTokens2,filtred_sentence,\n",
    "                                                        text,has_relation,relation_type,originalSentenceNumber)\n",
    "                        \n",
    "    if is_to_save:\n",
    "        countJsons+=1\n",
    "        #salvar arquivo json para labelstudio\n",
    "        saveJsonFiles(df_entity,text,from_id,to_id, \n",
    "                      lista_relacoes_sentence,filtred_sentence,originalSentenceNumber,save_folder_path)\n",
    "        #salvar json para bert (modelo nao foi usado)\n",
    "#         list_sentences_dict = getDictBert(df_entity,text,\n",
    "#                                           lista_relacoes_sentence,from_id,to_id,list_sentences_dict)\n",
    "        \n",
    "print(\"-------------\")\n",
    "print(\"Number of Jsons saved = \", countJsons )\n",
    "\n",
    "# pickle.dump(df_relation, open('df_relation.pkl', 'wb'), protocol=pickle.HIGHEST_PROTOCOL)\n",
    "df_relation.to_csv('df_relation.csv', encoding='utf-8',index=False)\n",
    "df_bert.to_csv(save_csv_name, encoding='utf-8',index=False)\n",
    "\n",
    "relacoes, numb_rel = np.unique(lista_relacoes, return_counts = True)\n",
    "uris, numb_uris = np.unique(lista_uris, return_counts = True)\n",
    "classes, numb_classes = np.unique(lista_classes, return_counts = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contabilização das relações encontradas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'relacoes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-b4bd58ac02b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrelacoes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'relacoes' is not defined"
     ]
    }
   ],
   "source": [
    "relacoes.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numb_rel.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contabilização das classes encontradas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numb_classes.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contabilização das URIs encontradas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#Alagoas_Age',\n",
       " '#Albian',\n",
       " '#Aptian',\n",
       " '#Aratu_Age',\n",
       " '#Archean',\n",
       " '#Atokan_Age',\n",
       " '#BASE_CD_BACIA_020',\n",
       " '#BASE_CD_BACIA_030',\n",
       " '#BASE_CD_BACIA_051',\n",
       " '#BASE_CD_BACIA_076',\n",
       " '#BASE_CD_BACIA_080',\n",
       " '#BASE_CD_BACIA_090',\n",
       " '#BASE_CD_BACIA_096',\n",
       " '#BASE_CD_BACIA_100',\n",
       " '#BASE_CD_BACIA_106',\n",
       " '#BASE_CD_BACIA_116',\n",
       " '#BASE_CD_BACIA_210',\n",
       " '#BASE_CD_BACIA_215',\n",
       " '#BASE_CD_BACIA_230',\n",
       " '#BASE_CD_BACIA_240',\n",
       " '#BASE_CD_BACIA_250',\n",
       " '#BASE_CD_BACIA_256',\n",
       " '#BASE_CD_BACIA_260',\n",
       " '#BASE_CD_BACIA_266',\n",
       " '#BASE_CD_BACIA_270',\n",
       " '#BASE_CD_BACIA_281',\n",
       " '#BASE_CD_BACIA_300',\n",
       " '#BASE_CD_BACIA_316',\n",
       " '#BASE_CD_BACIA_381',\n",
       " '#Barremian',\n",
       " '#Bartonian',\n",
       " '#Berriasian',\n",
       " '#Buracica_Age',\n",
       " '#Burdigalian',\n",
       " '#CAMP_CD_CAMPO_0003',\n",
       " '#CAMP_CD_CAMPO_0004',\n",
       " '#CAMP_CD_CAMPO_0012',\n",
       " '#CAMP_CD_CAMPO_0017',\n",
       " '#CAMP_CD_CAMPO_0027',\n",
       " '#CAMP_CD_CAMPO_0065',\n",
       " '#CAMP_CD_CAMPO_0077',\n",
       " '#CAMP_CD_CAMPO_0082',\n",
       " '#CAMP_CD_CAMPO_0093',\n",
       " '#CAMP_CD_CAMPO_0118',\n",
       " '#CAMP_CD_CAMPO_0174',\n",
       " '#CAMP_CD_CAMPO_0179',\n",
       " '#CAMP_CD_CAMPO_0199',\n",
       " '#CAMP_CD_CAMPO_0214',\n",
       " '#CAMP_CD_CAMPO_0234',\n",
       " '#CAMP_CD_CAMPO_0239',\n",
       " '#CAMP_CD_CAMPO_0247',\n",
       " '#CAMP_CD_CAMPO_0248',\n",
       " '#CAMP_CD_CAMPO_0249',\n",
       " '#CAMP_CD_CAMPO_0264',\n",
       " '#CAMP_CD_CAMPO_0279',\n",
       " '#CAMP_CD_CAMPO_0309',\n",
       " '#CAMP_CD_CAMPO_0322',\n",
       " '#CAMP_CD_CAMPO_0352',\n",
       " '#CAMP_CD_CAMPO_0362',\n",
       " '#CAMP_CD_CAMPO_0363',\n",
       " '#CAMP_CD_CAMPO_0373',\n",
       " '#CAMP_CD_CAMPO_0383',\n",
       " '#CAMP_CD_CAMPO_0388',\n",
       " '#CAMP_CD_CAMPO_0393',\n",
       " '#CAMP_CD_CAMPO_0403',\n",
       " '#CAMP_CD_CAMPO_0444',\n",
       " '#CAMP_CD_CAMPO_0477',\n",
       " '#CAMP_CD_CAMPO_0519',\n",
       " '#CAMP_CD_CAMPO_0523',\n",
       " '#CAMP_CD_CAMPO_0536',\n",
       " '#CAMP_CD_CAMPO_0545',\n",
       " '#CAMP_CD_CAMPO_0575',\n",
       " '#CAMP_CD_CAMPO_0603',\n",
       " '#CAMP_CD_CAMPO_0637',\n",
       " '#CAMP_CD_CAMPO_0668',\n",
       " '#CAMP_CD_CAMPO_0682',\n",
       " '#CAMP_CD_CAMPO_0683',\n",
       " '#CAMP_CD_CAMPO_0684',\n",
       " '#CAMP_CD_CAMPO_0718',\n",
       " '#CAMP_CD_CAMPO_0725',\n",
       " '#CAMP_CD_CAMPO_0734',\n",
       " '#CAMP_CD_CAMPO_0750',\n",
       " '#CAMP_CD_CAMPO_0760',\n",
       " '#CAMP_CD_CAMPO_0773',\n",
       " '#CAMP_CD_CAMPO_0803',\n",
       " '#CAMP_CD_CAMPO_0819',\n",
       " '#CAMP_CD_CAMPO_0830',\n",
       " '#CAMP_CD_CAMPO_0861',\n",
       " '#CAMP_CD_CAMPO_0881',\n",
       " '#CAMP_CD_CAMPO_0888',\n",
       " '#CAMP_CD_CAMPO_0912',\n",
       " '#CAMP_CD_CAMPO_0916',\n",
       " '#CAMP_CD_CAMPO_0929',\n",
       " '#CAMP_CD_CAMPO_0932',\n",
       " '#CAMP_CD_CAMPO_0951',\n",
       " '#CAMP_CD_CAMPO_0958',\n",
       " '#CAMP_CD_CAMPO_0970',\n",
       " '#CAMP_CD_CAMPO_0975',\n",
       " '#CAMP_CD_CAMPO_0995',\n",
       " '#Cambrian',\n",
       " '#Campanian',\n",
       " '#Carboniferous',\n",
       " '#Cenomanian',\n",
       " '#Cenozoic',\n",
       " '#Chattian',\n",
       " '#Coniacian',\n",
       " '#Cretaceous',\n",
       " '#Danian',\n",
       " '#Desmoinesian_Age',\n",
       " '#Devonian',\n",
       " '#Dom_Joao_Age',\n",
       " '#Eoburacica_Subage',\n",
       " '#Eocene',\n",
       " '#Eojiquia_Subage',\n",
       " '#Frasnian',\n",
       " '#Hauterivian',\n",
       " '#Holocene',\n",
       " '#Jiquia_Age',\n",
       " '#Jurassic',\n",
       " '#LowerCretaceous',\n",
       " '#LowerDevonian',\n",
       " '#LowerTriassic',\n",
       " '#Lower_Albian_Subage',\n",
       " '#Lower_Aptian_Subage',\n",
       " '#Lower_Maastrichtian_Subage',\n",
       " '#Lower_Turonian_Subage',\n",
       " '#Lutetian',\n",
       " '#Maastrichtian',\n",
       " '#Mesoriodaserra_Subage',\n",
       " '#Mesozoic',\n",
       " '#MiddleJurassic',\n",
       " '#Middle_Albian_Subage',\n",
       " '#Miocene',\n",
       " '#Mississippian',\n",
       " '#Missourian_Age',\n",
       " '#Morrowan_Age',\n",
       " '#Neoburacica_Subage',\n",
       " '#Neogene',\n",
       " '#Neojiquia_Subage',\n",
       " '#Neoriodaserra_Subage',\n",
       " '#Oligocene',\n",
       " '#Oxfordian',\n",
       " '#POCO_CD_POCO_005665',\n",
       " '#POCO_CD_POCO_006088',\n",
       " '#POCO_CD_POCO_006789',\n",
       " '#POCO_CD_POCO_006870',\n",
       " '#POCO_CD_POCO_006882',\n",
       " '#POCO_CD_POCO_006960',\n",
       " '#POCO_CD_POCO_007020',\n",
       " '#POCO_CD_POCO_007040',\n",
       " '#POCO_CD_POCO_007052',\n",
       " '#POCO_CD_POCO_007076',\n",
       " '#POCO_CD_POCO_007140',\n",
       " '#POCO_CD_POCO_007553',\n",
       " '#POCO_CD_POCO_007746',\n",
       " '#POCO_CD_POCO_007748',\n",
       " '#POCO_CD_POCO_008339',\n",
       " '#POCO_CD_POCO_009032',\n",
       " '#POCO_CD_POCO_009065',\n",
       " '#POCO_CD_POCO_009422',\n",
       " '#POCO_CD_POCO_009757',\n",
       " '#POCO_CD_POCO_010471',\n",
       " '#POCO_CD_POCO_010526',\n",
       " '#POCO_CD_POCO_010643',\n",
       " '#POCO_CD_POCO_010669',\n",
       " '#POCO_CD_POCO_010670',\n",
       " '#POCO_CD_POCO_010689',\n",
       " '#POCO_CD_POCO_010691',\n",
       " '#POCO_CD_POCO_010726',\n",
       " '#POCO_CD_POCO_010831',\n",
       " '#POCO_CD_POCO_011890',\n",
       " '#POCO_CD_POCO_011894',\n",
       " '#POCO_CD_POCO_011907',\n",
       " '#POCO_CD_POCO_011911',\n",
       " '#POCO_CD_POCO_012020',\n",
       " '#POCO_CD_POCO_012040',\n",
       " '#POCO_CD_POCO_012091',\n",
       " '#POCO_CD_POCO_012097',\n",
       " '#POCO_CD_POCO_012120',\n",
       " '#POCO_CD_POCO_012153',\n",
       " '#POCO_CD_POCO_012155',\n",
       " '#POCO_CD_POCO_012186',\n",
       " '#POCO_CD_POCO_012864',\n",
       " '#POCO_CD_POCO_012914',\n",
       " '#POCO_CD_POCO_012983',\n",
       " '#POCO_CD_POCO_012996',\n",
       " '#POCO_CD_POCO_015948',\n",
       " '#POCO_CD_POCO_016933',\n",
       " '#POCO_CD_POCO_017402',\n",
       " '#POCO_CD_POCO_017478',\n",
       " '#POCO_CD_POCO_018470',\n",
       " '#POCO_CD_POCO_020490',\n",
       " '#POCO_CD_POCO_020848',\n",
       " '#POCO_CD_POCO_021725',\n",
       " '#POCO_CD_POCO_021906',\n",
       " '#POCO_CD_POCO_021985',\n",
       " '#POCO_CD_POCO_022660',\n",
       " '#POCO_CD_POCO_022687',\n",
       " '#POCO_CD_POCO_022866',\n",
       " '#POCO_CD_POCO_022975',\n",
       " '#POCO_CD_POCO_023049',\n",
       " '#POCO_CD_POCO_023169',\n",
       " '#POCO_CD_POCO_023172',\n",
       " '#POCO_CD_POCO_023241',\n",
       " '#POCO_CD_POCO_023376',\n",
       " '#POCO_CD_POCO_023745',\n",
       " '#POCO_CD_POCO_023746',\n",
       " '#POCO_CD_POCO_024176',\n",
       " '#POCO_CD_POCO_024384',\n",
       " '#POCO_CD_POCO_024465',\n",
       " '#POCO_CD_POCO_024505',\n",
       " '#POCO_CD_POCO_024918',\n",
       " '#POCO_CD_POCO_025010',\n",
       " '#POCO_CD_POCO_025253',\n",
       " '#Paleocene',\n",
       " '#Paleogene',\n",
       " '#Paleoproterozoic',\n",
       " '#Paleozoic',\n",
       " '#Pennsylvanian',\n",
       " '#Permian',\n",
       " '#Permiano',\n",
       " '#Phanerozoic',\n",
       " '#Pleistocene',\n",
       " '#Pliensbachian',\n",
       " '#Pliocene',\n",
       " '#Priabonian',\n",
       " '#Proterozoic',\n",
       " '#Quaternary',\n",
       " '#Rupelian',\n",
       " '#Santonian',\n",
       " '#Silurian',\n",
       " '#Sinemurian',\n",
       " '#Thanetian',\n",
       " '#Tithonian',\n",
       " '#Toarcian',\n",
       " '#Triassic',\n",
       " '#Turonian',\n",
       " '#UpperCretaceous',\n",
       " '#UpperDevonian',\n",
       " '#UpperJurassic',\n",
       " '#UpperTriassic',\n",
       " '#Upper_Albian_Subage',\n",
       " '#Upper_Aptian_Subage',\n",
       " '#Upper_Campanian_Subage',\n",
       " '#Upper_Maastrichtian_Subage',\n",
       " '#Valanginian',\n",
       " '#Virgilian_Age',\n",
       " '#Ypresian',\n",
       " '#anhydrite',\n",
       " '#basalt',\n",
       " '#calciarenite',\n",
       " '#calcilutite',\n",
       " '#carnallite',\n",
       " '#clay',\n",
       " '#coal',\n",
       " '#conglomerate',\n",
       " '#diabase',\n",
       " '#diamictite',\n",
       " '#formacao_002',\n",
       " '#formacao_003',\n",
       " '#formacao_009',\n",
       " '#formacao_013',\n",
       " '#formacao_016',\n",
       " '#formacao_019',\n",
       " '#formacao_025',\n",
       " '#formacao_027',\n",
       " '#formacao_028',\n",
       " '#formacao_044',\n",
       " '#formacao_050',\n",
       " '#formacao_056',\n",
       " '#formacao_059',\n",
       " '#formacao_083',\n",
       " '#formacao_084',\n",
       " '#formacao_095',\n",
       " '#formacao_096',\n",
       " '#formacao_102',\n",
       " '#formacao_103',\n",
       " '#formacao_110',\n",
       " '#formacao_113',\n",
       " '#formacao_117',\n",
       " '#formacao_119',\n",
       " '#formacao_121',\n",
       " '#formacao_122',\n",
       " '#formacao_125',\n",
       " '#formacao_126',\n",
       " '#formacao_130',\n",
       " '#formacao_131',\n",
       " '#formacao_136',\n",
       " '#formacao_142',\n",
       " '#formacao_145',\n",
       " '#formacao_149',\n",
       " '#formacao_153',\n",
       " '#formacao_154',\n",
       " '#formacao_157',\n",
       " '#formacao_158',\n",
       " '#formacao_159',\n",
       " '#formacao_163',\n",
       " '#formacao_166',\n",
       " '#formacao_171',\n",
       " '#formacao_183',\n",
       " '#formacao_187',\n",
       " '#formacao_193',\n",
       " '#formacao_200',\n",
       " '#formacao_204',\n",
       " '#formacao_207',\n",
       " '#formacao_210',\n",
       " '#formacao_215',\n",
       " '#formacao_216',\n",
       " '#formacao_217',\n",
       " '#formacao_222',\n",
       " '#formacao_224',\n",
       " '#formacao_228',\n",
       " '#formacao_229',\n",
       " '#formacao_232',\n",
       " '#formacao_236',\n",
       " '#formacao_239',\n",
       " '#formacao_242',\n",
       " '#formacao_246',\n",
       " '#formacao_249',\n",
       " '#formacao_251',\n",
       " '#formacao_253',\n",
       " '#formacao_254',\n",
       " '#formacao_255',\n",
       " '#formacao_256',\n",
       " '#formacao_259',\n",
       " '#formacao_260',\n",
       " '#formacao_262',\n",
       " '#formacao_264',\n",
       " '#formacao_266',\n",
       " '#formacao_277',\n",
       " '#formacao_278',\n",
       " '#formacao_286',\n",
       " '#formacao_287',\n",
       " '#formacao_289',\n",
       " '#formacao_292',\n",
       " '#formacao_294',\n",
       " '#formacao_295',\n",
       " '#formacao_296',\n",
       " '#formacao_305',\n",
       " '#formacao_319',\n",
       " '#formacao_328',\n",
       " '#formacao_331',\n",
       " '#formacao_335',\n",
       " '#formacao_338',\n",
       " '#formacao_341',\n",
       " '#formacao_343',\n",
       " '#formacao_344',\n",
       " '#grupo_000',\n",
       " '#grupo_008',\n",
       " '#grupo_009',\n",
       " '#grupo_017',\n",
       " '#grupo_022',\n",
       " '#grupo_024',\n",
       " '#grupo_032',\n",
       " '#grupo_033',\n",
       " '#grupo_038',\n",
       " '#grupo_040',\n",
       " '#grupo_048',\n",
       " '#grupo_049',\n",
       " '#grupo_05',\n",
       " '#grupo_057',\n",
       " '#grupo_062',\n",
       " '#halite',\n",
       " '#igneous_rock',\n",
       " '#limestone',\n",
       " '#marlstone',\n",
       " '#membro_000',\n",
       " '#membro_002',\n",
       " '#membro_005',\n",
       " '#membro_007',\n",
       " '#membro_010',\n",
       " '#membro_013',\n",
       " '#membro_019',\n",
       " '#membro_020',\n",
       " '#membro_022',\n",
       " '#membro_025',\n",
       " '#membro_027',\n",
       " '#membro_032',\n",
       " '#membro_036',\n",
       " '#membro_042',\n",
       " '#membro_044',\n",
       " '#membro_050',\n",
       " '#membro_052',\n",
       " '#membro_060',\n",
       " '#membro_061',\n",
       " '#membro_070',\n",
       " '#membro_071',\n",
       " '#membro_075',\n",
       " '#membro_078',\n",
       " '#pyroclast',\n",
       " '#sand',\n",
       " '#sandstone',\n",
       " '#shale',\n",
       " '#siltstone',\n",
       " '#tachyhydrite']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uris.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[51,\n",
       " 140,\n",
       " 127,\n",
       " 26,\n",
       " 6,\n",
       " 40,\n",
       " 2,\n",
       " 12,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 35,\n",
       " 48,\n",
       " 4,\n",
       " 111,\n",
       " 1,\n",
       " 4,\n",
       " 11,\n",
       " 55,\n",
       " 8,\n",
       " 1,\n",
       " 9,\n",
       " 26,\n",
       " 27,\n",
       " 75,\n",
       " 28,\n",
       " 22,\n",
       " 4,\n",
       " 18,\n",
       " 2,\n",
       " 2,\n",
       " 42,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 6,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 13,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 8,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 6,\n",
       " 1,\n",
       " 1,\n",
       " 12,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 6,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 265,\n",
       " 138,\n",
       " 49,\n",
       " 47,\n",
       " 36,\n",
       " 19,\n",
       " 32,\n",
       " 92,\n",
       " 11,\n",
       " 16,\n",
       " 41,\n",
       " 22,\n",
       " 18,\n",
       " 32,\n",
       " 2,\n",
       " 3,\n",
       " 14,\n",
       " 53,\n",
       " 41,\n",
       " 31,\n",
       " 113,\n",
       " 3,\n",
       " 2,\n",
       " 22,\n",
       " 12,\n",
       " 2,\n",
       " 2,\n",
       " 7,\n",
       " 123,\n",
       " 2,\n",
       " 40,\n",
       " 3,\n",
       " 6,\n",
       " 12,\n",
       " 6,\n",
       " 8,\n",
       " 24,\n",
       " 26,\n",
       " 8,\n",
       " 2,\n",
       " 2,\n",
       " 21,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 6,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 48,\n",
       " 17,\n",
       " 5,\n",
       " 65,\n",
       " 5,\n",
       " 60,\n",
       " 1,\n",
       " 2,\n",
       " 56,\n",
       " 6,\n",
       " 2,\n",
       " 13,\n",
       " 15,\n",
       " 11,\n",
       " 14,\n",
       " 60,\n",
       " 6,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 8,\n",
       " 26,\n",
       " 40,\n",
       " 38,\n",
       " 8,\n",
       " 61,\n",
       " 4,\n",
       " 4,\n",
       " 22,\n",
       " 4,\n",
       " 12,\n",
       " 33,\n",
       " 4,\n",
       " 16,\n",
       " 5,\n",
       " 8,\n",
       " 1,\n",
       " 11,\n",
       " 1,\n",
       " 5,\n",
       " 3,\n",
       " 15,\n",
       " 3,\n",
       " 19,\n",
       " 1,\n",
       " 1,\n",
       " 17,\n",
       " 5,\n",
       " 6,\n",
       " 1,\n",
       " 3,\n",
       " 22,\n",
       " 8,\n",
       " 1,\n",
       " 4,\n",
       " 7,\n",
       " 1,\n",
       " 6,\n",
       " 1,\n",
       " 10,\n",
       " 1,\n",
       " 8,\n",
       " 14,\n",
       " 16,\n",
       " 13,\n",
       " 9,\n",
       " 2,\n",
       " 2,\n",
       " 6,\n",
       " 5,\n",
       " 1,\n",
       " 8,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 10,\n",
       " 12,\n",
       " 8,\n",
       " 7,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 6,\n",
       " 14,\n",
       " 6,\n",
       " 2,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 3,\n",
       " 10,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 8,\n",
       " 40,\n",
       " 16,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 15,\n",
       " 13,\n",
       " 4,\n",
       " 6,\n",
       " 23,\n",
       " 3,\n",
       " 20,\n",
       " 6,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 6,\n",
       " 6,\n",
       " 3,\n",
       " 2,\n",
       " 11,\n",
       " 22,\n",
       " 17,\n",
       " 7,\n",
       " 21,\n",
       " 4,\n",
       " 5,\n",
       " 13,\n",
       " 6,\n",
       " 1,\n",
       " 6,\n",
       " 3,\n",
       " 19,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 8,\n",
       " 2,\n",
       " 5,\n",
       " 35,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 9,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 5,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 31,\n",
       " 3,\n",
       " 5,\n",
       " 6,\n",
       " 1,\n",
       " 4,\n",
       " 6,\n",
       " 1,\n",
       " 6,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 11,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 198,\n",
       " 109,\n",
       " 19,\n",
       " 1]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numb_uris.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verificação de pares de entidades por tipo de relação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_filtred = pickle.load(open('df_relation.pkl', 'rb'))\n",
    "df_relations = pd.read_csv('df_relation.csv')\n",
    "df_grp = df_relations.groupby('Relation')\n",
    "relations_groups = df_grp.groups\n",
    "relations = list(relations_groups)\n",
    "lista_pares = []\n",
    "for relation in relations:\n",
    "    df_rel = df_grp.get_group(relation)\n",
    "    list_rel = []\n",
    "    for idx_rel in range(0,len(df_rel)):\n",
    "        par = df_rel.iloc[idx_rel]['Ent1'] + ' + ' + df_rel.iloc[idx_rel]['Ent2']\n",
    "        list_rel.append(par)\n",
    "    lista_pares.append(list_rel)\n",
    "print('Number of types of relations ->', len(lista_pares))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avaliar idx_pair de 0 ao tamanho apresentado acima para verificar os pares de entidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "pares, numb_pares = np.unique())[idx], return_counts = True)\n",
    "print('Relation -> ',relations[idx])\n",
    "print('Entities pair -> ',pares.tolist())\n",
    "print('Number of ocorrences -> ',numb_pares.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index_e</th>\n",
       "      <th>sentence</th>\n",
       "      <th>Ent1</th>\n",
       "      <th>Ent2</th>\n",
       "      <th>has_relation</th>\n",
       "      <th>relation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>[E1] Membro Mucuri [/E1], [E2] Eocretáceo [/E2...</td>\n",
       "      <td>UNIDADE_LITO</td>\n",
       "      <td>UNIDADE_CRONO</td>\n",
       "      <td>False</td>\n",
       "      <td>no_relation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>[E1] Membro Mucuri [/E1], Eocretáceo    [E2] B...</td>\n",
       "      <td>UNIDADE_LITO</td>\n",
       "      <td>BACIA</td>\n",
       "      <td>True</td>\n",
       "      <td>located_in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>[E2] Membro Mucuri [/E2], [E1] Eocretáceo [/E1...</td>\n",
       "      <td>UNIDADE_CRONO</td>\n",
       "      <td>UNIDADE_LITO</td>\n",
       "      <td>False</td>\n",
       "      <td>no_relation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>Membro Mucuri, [E1] Eocretáceo [/E1]    [E2] B...</td>\n",
       "      <td>UNIDADE_CRONO</td>\n",
       "      <td>BACIA</td>\n",
       "      <td>False</td>\n",
       "      <td>no_relation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>[E2] Membro Mucuri [/E2], Eocretáceo    [E1] B...</td>\n",
       "      <td>BACIA</td>\n",
       "      <td>UNIDADE_LITO</td>\n",
       "      <td>False</td>\n",
       "      <td>no_relation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>119</td>\n",
       "      <td>modelo proposto      autor, são discutidos con...</td>\n",
       "      <td>NÃOCONSOLID</td>\n",
       "      <td>BACIA</td>\n",
       "      <td>False</td>\n",
       "      <td>no_relation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>119</td>\n",
       "      <td>modelo proposto      autor, são discutidos con...</td>\n",
       "      <td>NÃOCONSOLID</td>\n",
       "      <td>UNIDADE_CRONO</td>\n",
       "      <td>False</td>\n",
       "      <td>no_relation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>119</td>\n",
       "      <td>modelo proposto      autor, são discutidos con...</td>\n",
       "      <td>NÃOCONSOLID</td>\n",
       "      <td>BACIA</td>\n",
       "      <td>False</td>\n",
       "      <td>no_relation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>119</td>\n",
       "      <td>modelo proposto      autor, são discutidos con...</td>\n",
       "      <td>BACIA</td>\n",
       "      <td>BACIA</td>\n",
       "      <td>False</td>\n",
       "      <td>no_relation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>119</td>\n",
       "      <td>modelo proposto      autor, são discutidos con...</td>\n",
       "      <td>BACIA</td>\n",
       "      <td>NÃOCONSOLID</td>\n",
       "      <td>False</td>\n",
       "      <td>no_relation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>118 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index_e                                           sentence           Ent1  \\\n",
       "0        6  [E1] Membro Mucuri [/E1], [E2] Eocretáceo [/E2...   UNIDADE_LITO   \n",
       "0        6  [E1] Membro Mucuri [/E1], Eocretáceo    [E2] B...   UNIDADE_LITO   \n",
       "0        6  [E2] Membro Mucuri [/E2], [E1] Eocretáceo [/E1...  UNIDADE_CRONO   \n",
       "0        6  Membro Mucuri, [E1] Eocretáceo [/E1]    [E2] B...  UNIDADE_CRONO   \n",
       "0        6  [E2] Membro Mucuri [/E2], Eocretáceo    [E1] B...          BACIA   \n",
       "..     ...                                                ...            ...   \n",
       "0      119  modelo proposto      autor, são discutidos con...    NÃOCONSOLID   \n",
       "0      119  modelo proposto      autor, são discutidos con...    NÃOCONSOLID   \n",
       "0      119  modelo proposto      autor, são discutidos con...    NÃOCONSOLID   \n",
       "0      119  modelo proposto      autor, são discutidos con...          BACIA   \n",
       "0      119  modelo proposto      autor, são discutidos con...          BACIA   \n",
       "\n",
       "             Ent2 has_relation     relation  \n",
       "0   UNIDADE_CRONO        False  no_relation  \n",
       "0           BACIA         True   located_in  \n",
       "0    UNIDADE_LITO        False  no_relation  \n",
       "0           BACIA        False  no_relation  \n",
       "0    UNIDADE_LITO        False  no_relation  \n",
       "..            ...          ...          ...  \n",
       "0           BACIA        False  no_relation  \n",
       "0   UNIDADE_CRONO        False  no_relation  \n",
       "0           BACIA        False  no_relation  \n",
       "0           BACIA        False  no_relation  \n",
       "0     NÃOCONSOLID        False  no_relation  \n",
       "\n",
       "[118 rows x 6 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
